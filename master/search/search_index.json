{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>EDS-NLP is a collaborative NLP framework that aims at extracting information from French clinical notes. At its core, it is a collection of components or pipes, either rule-based functions or deep learning modules. These components are organized into a novel efficient and modular pipeline system, built for hybrid and multitask models. We use spaCy to represent documents and their annotations, and Pytorch as a deep-learning backend for trainable components.</p> <p>EDS-NLP is versatile and can be used on any textual document. The rule-based components are fully compatible with spaCy's pipelines, and vice versa. This library is a product of collaborative effort, and we encourage further contributions to enhance its capabilities.</p> <p>Check out our interactive demo !</p>"},{"location":"#quick-start","title":"Quick start","text":""},{"location":"#installation","title":"Installation","text":"<p>You can install EDS-NLP via <code>pip</code>. We recommend pinning the library version in your projects, or use a strict package manager like Poetry.</p> <pre><code>pip install edsnlp==0.17.2\n</code></pre> <p>or if you want to use the trainable components (using pytorch)</p> <pre><code>pip install \"edsnlp[ml]==0.17.2\"\n</code></pre>"},{"location":"#a-first-pipeline","title":"A first pipeline","text":"<p>Once you've installed the library, let's begin with a very simple example that extracts mentions of COVID19 in a text, and detects whether they are negated.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")  # (1)\n\nterms = dict(\n    covid=[\"covid\", \"coronavirus\"],  # (2)\n)\n\n# Sentencizer component, needed for negation detection\nnlp.add_pipe(eds.sentences())  # (3)\n# Matcher component\nnlp.add_pipe(eds.matcher(terms=terms))  # (4)\n# Negation detection\nnlp.add_pipe(eds.negation())\n\n# Process your text in one call !\ndoc = nlp(\"Le patient n'est pas atteint de covid\")\n\ndoc.ents  # (5)\n# Out: (covid,)\n\ndoc.ents[0]._.negation  # (6)\n# Out: True\n</code></pre> <ol> <li>'eds' is the name of the language, which defines the tokenizer.</li> <li>This example terminology provides a very simple, and by no means exhaustive, list of synonyms for COVID19.</li> <li>Similarly to spaCy, pipes are added via the <code>nlp.add_pipe</code> method.</li> <li>See the matching tutorial for mode details.</li> <li>spaCy stores extracted entities in the <code>Doc.ents</code> attribute.</li> <li>The <code>eds.negation</code> component has adds a <code>negation</code> custom attribute.</li> </ol> <p>This example is complete, it should run as-is.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>To learn more about EDS-NLP, we have prepared a series of tutorials that should cover the main features of the library.</p>"},{"location":"#available-pipeline-components","title":"Available pipeline components","text":"CoreQualifiersMiscellaneousNERTrainable <p>See the Core components overview for more information.</p> Component Description <code>eds.normalizer</code> Non-destructive input text normalisation <code>eds.sentences</code> Better sentence boundary detection <code>eds.matcher</code> A simple yet powerful entity extractor <code>eds.terminology</code> A simple yet powerful terminology matcher <code>eds.contextual_matcher</code> A conditional entity extractor <code>eds.endlines</code> An unsupervised model to classify each end line <p>See the Qualifiers overview for more information.</p> Pipeline Description <code>eds.negation</code> Rule-based negation detection <code>eds.family</code> Rule-based family context detection <code>eds.hypothesis</code> Rule-based speculation detection <code>eds.reported_speech</code> Rule-based reported speech detection <code>eds.history</code> Rule-based medical history detection <p>See the Miscellaneous components overview for more information.</p> Component Description <code>eds.dates</code> Date extraction and normalisation <code>eds.consultation_dates</code> Identify consultation dates <code>eds.quantities</code> Quantity extraction and normalisation <code>eds.sections</code> Section detection <code>eds.reason</code> Rule-based hospitalisation reason detection <code>eds.tables</code> Tables detection <code>eds.split</code> Doc splitting <code>eds.explode</code> Explode entities between multiples copies of a document <p>See the NER overview for more information.</p> Component Description <code>eds.covid</code> A COVID mentions detector <code>eds.charlson</code> A Charlson score extractor <code>eds.sofa</code> A SOFA score extractor <code>eds.elston_ellis</code> An Elston &amp; Ellis code extractor <code>eds.emergency_priority</code> A priority score extractor <code>eds.emergency_ccmu</code> A CCMU score extractor <code>eds.emergency_gemsa</code> A GEMSA score extractor <code>eds.tnm</code> A TNM score extractor <code>eds.adicap</code> A ADICAP codes extractor <code>eds.drugs</code> A drug mentions extractor <code>eds.cim10</code> A CIM10 terminology matcher <code>eds.umls</code> An UMLS terminology matcher <code>eds.ckd</code> CKD extractor <code>eds.copd</code> COPD extractor <code>eds.cerebrovascular_accident</code> Cerebrovascular accident extractor <code>eds.congestive_heart_failure</code> Congestive heart failure extractor <code>eds.connective_tissue_disease</code> Connective tissue disease extractor <code>eds.dementia</code> Dementia extractor <code>eds.diabetes</code> Diabetes extractor <code>eds.hemiplegia</code> Hemiplegia extractor <code>eds.leukemia</code> Leukemia extractor <code>eds.liver_disease</code> Liver disease extractor <code>eds.lymphoma</code> Lymphoma extractor <code>eds.myocardial_infarction</code> Myocardial infarction extractor <code>eds.peptic_ulcer_disease</code> Peptic ulcer disease extractor <code>eds.peripheral_vascular_disease</code> Peripheral vascular disease extractor <code>eds.solid_tumor</code> Solid tumor extractor <code>eds.alcohol</code> Alcohol consumption extractor <code>eds.tobacco</code> Tobacco consumption extractor <p>See the Trainable components overview for more information.</p> Name Description <code>eds.transformer</code> Embed text with a transformer model <code>eds.text_cnn</code> Contextualize embeddings with a CNN <code>eds.span_pooler</code> A span embedding component that aggregates word embeddings <code>eds.ner_crf</code> A trainable component to extract entities <code>eds.extractive_qa</code> A trainable component for extractive question answering <code>eds.span_classifier</code> A trainable component for multi-class multi-label span classification <code>eds.span_linker</code> A trainable entity linker (i.e. to a list of concepts) <code>eds.biaffine_dep_parser</code> A trainable biaffine dependency parser"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>The performances of an extraction pipeline may depend on the population and documents that are considered.</p>"},{"location":"#contributing-to-eds-nlp","title":"Contributing to EDS-NLP","text":"<p>We welcome contributions ! Fork the project and propose a pull request. Take a look at the dedicated page for detail.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use EDS-NLP, please cite us as below.</p> <pre><code>@misc{edsnlp,\nauthor = {Wajsburt, Perceval and Petit-Jean, Thomas and Dura, Basile and Cohen, Ariel and Jean, Charline and Bey, Romain},\ndoi    = {10.5281/zenodo.6424993},\ntitle  = {EDS-NLP: efficient information extraction from French clinical notes},\nurl    = {https://aphp.github.io/edsnlp}\n}\n</code></pre>"},{"location":"tokenizers/","title":"Tokenizers","text":"<p>In addition to the standard spaCy <code>FrenchLanguage</code> (<code>fr</code>), EDS-NLP offers a new language better fit for French clinical documents: <code>EDSLanguage</code> (<code>eds</code>). Additionally, the <code>EDSLanguage</code> document creation should be around 5-6 times faster than the <code>fr</code> language. The main differences lie in the tokenization process.</p> <p>A comparison of the two tokenization methods is demonstrated below:</p> Example FrenchLanguage EDSLanguage <code>ACR5</code> [<code>ACR5</code>] [<code>ACR</code>, <code>5</code>] <code>26.5/</code> [<code>26.5/</code>] [<code>26.5</code>, <code>/</code>] <code>\\n \\n CONCLUSION</code> [<code>\\n \\n</code>, <code>CONCLUSION</code>] [<code>\\n</code>, <code>\\n</code>, <code>CONCLUSION</code>] <code>l'art\u00e8re</code> [<code>l'</code>, <code>art\u00e8re</code>] [<code>l'</code>, <code>art\u00e8re</code>] (same) <code>Dr. Pichon</code> [<code>Dr</code>, <code>.</code>, <code>Pichon</code>] [<code>Dr.</code>, <code>Pichon</code>] <code>B.H.HP.A.7.A</code> [<code>B.H.HP.A.7.A</code>] [<code>B.</code>, <code>H.</code>, <code>HP.</code>, <code>A</code>, <code>7</code>, <code>A</code>, <code>0</code>] <p>To instantiate one of the two languages, you can call the <code>spacy.blank</code> method.</p> EDSLanguageFrenchLanguage <pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\n</code></pre> <pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"fr\")\n</code></pre>"},{"location":"advanced-tutorials/","title":"Advanced use cases","text":"<p>In this section, we review a few advanced use cases:</p> <ul> <li>Adding pre-computed word vectors to spaCy</li> <li>Deploying your spaCy pipeline as an API</li> <li>Creating your own component</li> </ul>"},{"location":"advanced-tutorials/fastapi/","title":"Deploying as an API","text":"<p>In this section, we will see how you can deploy your pipeline as a REST API using the power of FastAPI.</p>"},{"location":"advanced-tutorials/fastapi/#the-nlp-pipeline","title":"The NLP pipeline","text":"<p>Let's create a simple NLP model, that can:</p> <ul> <li>match synonyms of COVID19</li> <li>check for negation, speculation and reported speech.</li> </ul> <p>You know the drill:</p> pipeline.py<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank('eds')\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n   eds.matcher(\n    regex=dict(\n        covid=[\n            \"covid\",\n            r\"covid[-\\s]?19\",\n            r\"sars[-\\s]?cov[-\\s]?2\",\n            r\"corona[-\\s]?virus\",\n        ],\n    ),\n    attr=\"LOWER\",\n   ),\n)\nnlp.add_pipe(eds.negation())\nnlp.add_pipe(eds.family())\nnlp.add_pipe(eds.hypothesis())\nnlp.add_pipe(eds.rspeech())\n</code></pre>"},{"location":"advanced-tutorials/fastapi/#creating-the-fastapi-app","title":"Creating the FastAPI app","text":"<p>FastAPI is a incredibly efficient framework, based on Python type hints from the ground up, with the help of Pydantic (another great library for building modern Python). We won't go into too much detail about FastAPI in this tutorial. For further information on how the framework operates, go to its excellent documentation!</p> <p>We'll need to create two things:</p> <ol> <li>A module containing the models for inputs and outputs.</li> <li>The script that defines the application itself.</li> </ol> models.py<pre><code>from typing import List\n\nfrom pydantic import BaseModel\n\n\nclass Entity(BaseModel):  # (1)\n\n    # OMOP-style attributes\n    start: int\n    end: int\n    label: str\n    lexical_variant: str\n    normalized_variant: str\n\n    # Qualifiers\n    negated: bool\n    hypothesis: bool\n    family: bool\n    reported_speech: bool\n\n\nclass Document(BaseModel):  # (2)\n    text: str\n    ents: List[Entity]\n</code></pre> <ol> <li>The <code>Entity</code> model contains attributes that define a matched entity, as well as variables that contain the output of the qualifier components.</li> <li>The <code>Document</code> model contains the input text, and a list of detected entities</li> </ol> <p>Having defined the output models and the pipeline, we can move on to creating the application itself:</p> app.py<pre><code>from typing import List\n\nfrom fastapi import FastAPI\n\nfrom pipeline import nlp\nfrom models import Entity, Document\n\n\napp = FastAPI(title=\"EDS-NLP\", version=edsnlp.__version__)\n\n\n@app.post(\"/covid\", response_model=List[Document])  # (1)\nasync def process(\n    notes: List[str],  # (2)\n):\n\n    documents = []\n\n    for doc in nlp.pipe(notes):\n        entities = []\n\n        for ent in doc.ents:\n            entity = Entity(\n                start=ent.start_char,\n                end=ent.end_char,\n                label=ent.label_,\n                lexical_variant=ent.text,\n                normalized_variant=ent._.normalized_variant,\n                negated=ent._.negation,\n                hypothesis=ent._.hypothesis,\n                family=ent._.family,\n                reported_speech=ent._.reported_speech,\n            )\n            entities.append(entity)\n\n        documents.append(\n            Document(\n                text=doc.text,\n                ents=entities,\n            )\n        )\n\n    return documents\n</code></pre> <ol> <li>By telling FastAPI what output format is expected, you get automatic data validation.</li> <li>In FastAPI, input and output schemas are defined through Python type hinting.    Here, we tell FastAPI to expect a list of strings in the <code>POST</code> request body.    As a bonus, you get data validation for free.</li> </ol>"},{"location":"advanced-tutorials/fastapi/#running-the-api","title":"Running the API","text":"<p>Our simple API is ready to launch! We'll just need to install FastAPI along with a ASGI server to run it. This can be done in one go:</p> <pre><code>$ pip install 'fastapi[uvicorn]'\n---&gt; 100%\ncolor:green Successfully installed fastapi\n</code></pre> <p>Launching the API is trivial:</p> <pre><code>$ uvicorn app:app --reload\n</code></pre> <p>Go to <code>localhost:8000/docs</code> to admire the automatically generated documentation!</p>"},{"location":"advanced-tutorials/fastapi/#using-the-api","title":"Using the API","text":"<p>You can try the API directly from the documentation. Otherwise, you may use the <code>requests</code> package:</p> <pre><code>import requests\n\nnotes = [\n    \"Le p\u00e8re du patient n'est pas atteint de la covid.\",\n    \"Probable coronavirus.\",\n]\n\nr = requests.post(\n    \"http://localhost:8000/covid\",\n    json=notes,\n)\n\nr.json()\n</code></pre> <p>You should get something like:</p> <pre><code>[\n{\n\"text\": \"Le p\u00e8re du patient n'est pas atteint de la covid.\",\n\"ents\": [\n{\n\"start\": 43,\n\"end\": 48,\n\"label\": \"covid\",\n\"lexical_variant\": \"covid\",\n\"normalized_variant\": \"covid\",\n\"negated\": true,\n\"hypothesis\": false,\n\"family\": true,\n\"reported_speech\": false\n}\n]\n},\n{\n\"text\": \"Probable coronavirus.\",\n\"ents\": [\n{\n\"start\": 9,\n\"end\": 20,\n\"label\": \"covid\",\n\"lexical_variant\": \"coronavirus\",\n\"normalized_variant\": \"coronavirus\",\n\"negated\": false,\n\"hypothesis\": true,\n\"family\": false,\n\"reported_speech\": false\n}\n]\n}\n]\n</code></pre>"},{"location":"advanced-tutorials/word-vectors/","title":"Word embeddings","text":"<p>The only ready-to-use components in EDS-NLP are rule-based components. However, that does not prohibit you from exploiting spaCy's machine learning capabilities! You can mix and match machine learning pipelines, trainable or not, with EDS-NLP rule-based components.</p> <p>In this tutorial, we will explore how you can use static word vectors trained with Gensim within spaCy.</p> <p>Training the word embedding, however, is outside the scope of this post. You'll find very well designed resources on the subject in Gensim's documenation.</p> <p>Using Transformer models</p> <p>spaCy v3 introduced support for Transformer models through their helper library <code>spacy-transformers</code> that interfaces with HuggingFace's <code>transformers</code> library.</p> <p>Using transformer models can significantly increase your model's performance.</p>"},{"location":"advanced-tutorials/word-vectors/#adding-pre-trained-word-vectors","title":"Adding pre-trained word vectors","text":"<p>spaCy provides a <code>init vectors</code> CLI utility that takes a Gensim-trained binary and transforms it to a spaCy-readable pipeline.</p> <p>Using it is straightforward :</p> <pre><code>$ spacy init vectors fr /path/to/vectors /path/to/pipeline\n---&gt; 100%\ncolor:green Conversion successful!\n</code></pre> <p>See the documentation for implementation details.</p>"},{"location":"concepts/inference/","title":"Inference","text":"<p>Once you have obtained a pipeline, either by composing rule-based components, training a model or loading a model from the disk, you can use it to make predictions on documents. This is referred to as inference. This page answers the following questions :</p> <p>How do we leverage computational resources run a model on many documents?</p> <p>How do we connect to various data sources to retrieve documents?</p> <p>Be sure to check out the Processing multiple texts tutorial for a practical example of how to use EDS-NLP to process large datasets.</p>"},{"location":"concepts/inference/#inference-on-a-single-document","title":"Inference on a single document","text":"<p>In EDS-NLP, computing the prediction on a single document is done by calling the pipeline on the document. The input can be either:</p> <ul> <li>a text string</li> <li>or a Doc object</li> </ul> <pre><code>from pathlib import Path\n\nnlp = ...\ntext = \"... my text ...\"\ndoc = nlp(text)\n</code></pre> <p>If you're lucky enough to have a GPU, you can use it to speed up inference by moving the model to the GPU before calling the pipeline.</p> <pre><code>nlp.to(\"cuda\")  # same semantics as pytorch\ndoc = nlp(text)\n</code></pre> <p>To leverage multiple GPUs when processing multiple documents, refer to the multiprocessing backend description below.</p>"},{"location":"concepts/inference/#edsnlp.core.stream.Stream","title":"Streams","text":"<p>When processing multiple documents, we can optimize the inference by parallelizing the computation on a single core, multiple cores and GPUs or even multiple machines.</p> <p>These optimizations are enabled by performing lazy inference : the operations (e.g., reading a document, converting it to a Doc, running the different pipes of a model or writing the result somewhere) are not executed immediately but are instead scheduled in a Stream object. It can then be executed by calling the <code>execute</code> method, iterating over it or calling a writing method (e.g., <code>to_pandas</code>). In fact, data connectors like <code>edsnlp.data.read_json</code> return a stream, as well as the <code>nlp.pipe</code> method.</p> <p>A stream contains :</p> <ul> <li>a <code>reader</code>: the source of the data (e.g., a file, a database, a list of strings, etc.)</li> <li>the list of operations to perform (<code>stream.ops</code>) that contain the function / pipe, keyword arguments and context for each operation</li> <li>an optional <code>writer</code>: the destination of the data (e.g., a file, a database, a list of strings, etc.)</li> <li>the execution <code>config</code>, containing the backend to use and its configuration such as the number of workers, the batch size, etc.</li> </ul> <p>All methods (<code>map()</code>, <code>map_batches()</code>, <code>map_gpu()</code>, <code>map_pipeline()</code>, <code>set_processing()</code>) of the stream are chainable, meaning that they return a new stream object (no in-place modification).</p> <p>For instance, the following code will load a model, read a folder of JSON files, apply the model to each document and write the result in a Parquet folder, using 4 CPUs and 2 GPUs.</p> <pre><code>import edsnlp\n\n# Load or create a model\nnlp = edsnlp.load(\"path/to/model\")\n\n# Read some data (this is lazy, no data will be read until the end of of this snippet)\ndata = edsnlp.data.read_json(\"path/to/json_folder\", converter=\"...\")\n\n# Apply each pipe of the model to our documents and split the data\n# into batches such that each contains at most 100 000 padded words\n# (padded_words = max doc size in batch * batch size)\ndata = data.map_pipeline(\n    nlp,\n    # optional arguments\n    batch_size=100_000,\n    batch_by=\"padded_words\",\n)\n# or equivalently : data = nlp.pipe(data, batch_size=100_000, batch_by=\"padded_words\")\n\n# Sort the documents in chunks of 1024 documents\ndata = data.map_batches(\n    lambda batch: sorted(batch, key=lambda doc: len(doc)),\n    batch_size=1024,\n)\n\ndata = data.map_batches(\n    # Apply a function to each batch of documents\n    lambda batch: [doc._.my_custom_attribute for doc in batch]\n)\n\n# Configure the execution\ndata = data.set_processing(\n    # 4 CPUs to parallelize rule-based pipes, IO and preprocessing\n    num_cpu_workers=4,\n    # 2 GPUs to accelerate deep-learning pipes\n    num_gpu_workers=2,\n    # Show the progress bar\n    show_progress=True,\n)\n\n# Write the result, this will execute the stream\ndata.write_parquet(\"path/to/output_folder\", converter=\"...\", write_in_worker=True)\n</code></pre> <p>Streams support a variety of operations, such as applying a function to each element of the stream, batching the elements, applying a model to the elements, etc. In each case, the operations will not be executed immediately but will be scheduled to be executed when iterating of the collection, or calling the <code>execute()</code>, <code>to_*()</code> or <code>write_*()</code> methods.</p>"},{"location":"concepts/inference/#edsnlp.core.stream.Stream.map","title":"<code>map()</code>","text":"<p>Maps a callable to the documents. It takes a callable as input and an optional dictionary of keyword arguments. The function will be applied to each element of the collection. If the callable is a generator function, each element will be yielded to the stream as is.</p> PARAMETER DESCRIPTION <code>pipe</code> <p>The callable to map to the documents.</p> <p> </p> <code>kwargs</code> <p>The keyword arguments to pass to the callable.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"concepts/inference/#edsnlp.core.stream.Stream.map_batches","title":"<code>map_batches()</code>","text":"<p>To apply an operation to a stream in batches, you can use the <code>map_batches()</code> method. It takes a callable as input, an optional dictionary of keyword arguments and batching arguments.</p> <p>Maps a callable to a batch of documents. The callable should take a list of inputs. The output of the callable will be flattened if it is a list or a generator, or yielded to the stream as is if it is a single output (tuple or any other type).</p> PARAMETER DESCRIPTION <code>pipe</code> <p>The callable to map to the documents.</p> <p> </p> <code>kwargs</code> <p>The keyword arguments to pass to the callable.</p> <p> DEFAULT: <code>{}</code> </p> <code>batch_size</code> <p>The batch size. Can also be a batching expression like \"32 docs\", \"1024 words\", \"dataset\", \"fragment\", etc.</p> <p> TYPE: <code>Optional[Union[int, float, str]]</code> DEFAULT: <code>None</code> </p> <code>batch_by</code> <p>Function to compute the batches. If set, it should take an iterable of documents and return an iterable of batches. You can also set it to \"docs\", \"words\" or \"padded_words\" to use predefined batching functions. Defaults to \"docs\".</p> <p> TYPE: <code>BatchBy</code> DEFAULT: <code>None</code> </p>"},{"location":"concepts/inference/#edsnlp.core.stream.Stream.map_pipeline","title":"<code>map_pipeline()</code>","text":"<p>Maps a pipeline to the documents, i.e. adds each component of the pipeline to the stream operations. This function is called under the hood by <code>nlp.pipe()</code></p> PARAMETER DESCRIPTION <code>model</code> <p>The pipeline to map to the documents.</p> <p> TYPE: <code>Pipeline</code> </p> <code>batch_size</code> <p>The batch size. Can also be a batching expression like \"32 docs\", \"1024 words\", \"dataset\", \"fragment\", etc.</p> <p> TYPE: <code>Optional[Union[int, float, str]]</code> DEFAULT: <code>None</code> </p> <code>batch_by</code> <p>Function to compute the batches. If set, it should take an iterable of documents and return an iterable of batches. You can also set it to \"docs\", \"words\" or \"padded_words\" to use predefined batching functions. Defaults to \"docs\".</p> <p> TYPE: <code>BatchBy</code> DEFAULT: <code>None</code> </p>"},{"location":"concepts/inference/#edsnlp.core.stream.Stream.map_gpu","title":"<code>map_gpu()</code>","text":"<p>Maps a deep learning operation to a batch of documents, on a GPU worker.</p> PARAMETER DESCRIPTION <code>prepare_batch</code> <p>A callable that takes a list of documents and a device and returns a batch of tensors (or anything that can be passed to the <code>forward</code> callable). This will be called on a CPU-bound worker, and may be parallelized.</p> <p> TYPE: <code>Callable[[List, Union[str, device]], Any]</code> </p> <code>forward</code> <p>A callable that takes the output of <code>prepare_batch</code> and returns the output of the deep learning operation. This will be called on a GPU-bound worker.</p> <p> TYPE: <code>Callable[[Any], Any]</code> </p> <code>postprocess</code> <p>An optional callable that takes the list of documents and the output of the deep learning operation, and returns the final output. This will be called on the same CPU-bound worker that called the <code>prepare_batch</code> function.</p> <p> TYPE: <code>Optional[Callable[[List, Any], Any]]</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>The batch size. Can also be a batching expression like \"32 docs\", \"1024 words\", \"dataset\", \"fragment\", etc.</p> <p> TYPE: <code>Optional[Union[int, float, str]]</code> DEFAULT: <code>None</code> </p> <code>batch_by</code> <p>Function to compute the batches. If set, it should take an iterable of documents and return an iterable of batches. You can also set it to \"docs\", \"words\" or \"padded_words\" to use predefined batching functions. Defaults to \"docs\".</p> <p> TYPE: <code>BatchBy</code> DEFAULT: <code>None</code> </p>"},{"location":"concepts/inference/#edsnlp.core.stream.Stream.loop","title":"<code>loop()</code>","text":"<p>Loops over the stream indefinitely.</p> <p>Note that we cycle over items produced by the reader, not the items produced by the stream operations. This means that the stream operations will be applied to the same items multiple times, and may produce different results if they are non-deterministic. This also mean that calling this function will have the same effect regardless of the operations applied to the stream before calling it, ie:</p> <pre><code>stream.loop().map(...)\n# is equivalent to\nstream.map(...).loop()\n</code></pre>"},{"location":"concepts/inference/#edsnlp.core.stream.Stream.shuffle","title":"<code>shuffle()</code>","text":"<p>Shuffles the stream by accumulating the documents into batches and shuffling the batches. We try to optimize and avoid the accumulation by shuffling items directly in the reader, but if some upstream operations are not elementwise or if the reader is not compatible with the batching mode, we have to accumulate the documents into batches and shuffle the batches.</p> <p>For instance, imagine a reading from list of 2 very large documents and applying an operation to split the documents into sentences. Shuffling only in the reader, then applying the split operation would not shuffle the sentences across documents and may lead to a lack of randomness when training a model. Think of this as having lumps after mixing your data. In our case, we detect that the split op is not elementwise and trigger the accumulation of sentences into batches after their generation before shuffling the batches.</p> PARAMETER DESCRIPTION <code>batch_size</code> <p>The batch size. Can also be a batching expression like \"32 docs\", \"1024 words\", \"dataset\", \"fragment\", etc.</p> <p> TYPE: <code>Optional[Union[int, float, str]]</code> DEFAULT: <code>None</code> </p> <code>batch_by</code> <p>Function to compute the batches. If set, it should take an iterable of documents and return an iterable of batches. You can also set it to \"docs\", \"words\" or \"padded_words\" to use predefined batching functions. Defaults to \"docs\".</p> <p> TYPE: <code>Optional[str, BatchFn]</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>shuffle_reader</code> <p>Whether to shuffle the reader. Defaults to True if the reader is compatible with the batch_by mode, False otherwise.</p> <p> TYPE: <code>Optional[Union[bool, str]]</code> DEFAULT: <code>None</code> </p>"},{"location":"concepts/inference/#edsnlp.core.stream.Stream.set_processing","title":"Configure the execution with <code>set_processing()</code>","text":"<p>You can configure how the operations performed in the stream are executed by calling its <code>set_processing(...)</code> method. The following options are available :</p> PARAMETER DESCRIPTION <code>batch_size</code> <p>The batch size. Can also be a batching expression like \"32 docs\", \"1024 words\", \"dataset\", \"fragment\", etc.</p> <p> TYPE: <code>Optional[Union[int, float, str]]</code> DEFAULT: <code>None</code> </p> <code>batch_by</code> <p>Function to compute the batches. If set, it should take an iterable of documents and return an iterable of batches. You can also set it to \"docs\", \"words\" or \"padded_words\" to use predefined batching functions. Defaults to \"docs\".</p> <p> TYPE: <code>BatchBy</code> DEFAULT: <code>None</code> </p> <code>num_cpu_workers</code> <p>Number of CPU workers. A CPU worker handles the non deep-learning components and the preprocessing, collating and postprocessing of deep-learning components. If no GPU workers are used, the CPU workers also handle the forward call of the deep-learning components.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>num_gpu_workers</code> <p>Number of GPU workers. A GPU worker handles the forward call of the deep-learning components. Only used with \"multiprocessing\" backend.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>disable_implicit_parallelism</code> <p>Whether to disable OpenMP and Huggingface tokenizers implicit parallelism in multiprocessing mode. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>backend</code> <p>The backend to use for parallel processing. If not set, the backend is automatically selected based on the input data and the number of workers.</p> <ul> <li>\"simple\" is the default backend and is used when <code>num_cpu_workers</code> is 1     and <code>num_gpu_workers</code> is 0.</li> <li>\"multiprocessing\" is used when <code>num_cpu_workers</code> is greater than 1 or     <code>num_gpu_workers</code> is greater than 0.</li> <li>\"spark\" is used when the input data is a Spark dataframe and the output     writer is a Spark writer.</li> </ul> <p> TYPE: <code>Optional[Literal['simple', 'multiprocessing', 'mp', 'spark']]</code> DEFAULT: <code>None</code> </p> <code>autocast</code> <p>Whether to use automatic mixed precision (AMP) for the forward pass of the deep-learning components. If True (by default), AMP will be used with the default settings. If False, AMP will not be used. If a dtype is provided, it will be passed to the <code>torch.autocast</code> context manager.</p> <p> TYPE: <code>Union[bool, Any]</code> DEFAULT: <code>None</code> </p> <code>show_progress</code> <p>Whether to show progress bars (only applicable with \"simple\" and \"multiprocessing\" backends).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>gpu_pipe_names</code> <p>List of pipe names to accelerate on a GPUWorker, defaults to all pipes that inherit from TorchComponent. Only used with \"multiprocessing\" backend. Inferred from the pipeline if not set.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>process_start_method</code> <p>Whether to use \"fork\" or \"spawn\" as the start method for the multiprocessing backend. The default is \"fork\" on Unix systems and \"spawn\" on Windows.</p> <ul> <li>\"fork\" is the default start method on Unix systems and is the fastest     start method, but it is not available on Windows, can cause issues     with CUDA and is not safe when using multiple threads.</li> <li>\"spawn\" is the default start method on Windows and is the safest start     method, but it is not available on Unix systems and is slower than     \"fork\".</li> </ul> <p> TYPE: <code>Optional[Literal['fork', 'spawn']]</code> DEFAULT: <code>None</code> </p> <code>gpu_worker_devices</code> <p>List of GPU devices to use for the GPU workers. Defaults to all available devices, one worker per device. Only used with \"multiprocessing\" backend.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>cpu_worker_devices</code> <p>List of GPU devices to use for the CPU workers. Used for debugging purposes.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>deterministic</code> <p>Whether to try and preserve the order of the documents in \"multiprocessing\" mode. If set to <code>False</code>, workers will process documents whenever they are available in a dynamic fashion, which may result in out-of-order but usually faster processing. If set to true, tasks will be distributed in a static, round-robin fashion to workers. Defaults to <code>True</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"concepts/inference/#backends","title":"Backends","text":"<p>The <code>backend</code> parameter of the <code>set_processing</code> supports the following values:</p>"},{"location":"concepts/inference/#edsnlp.processing.simple.execute_simple_backend","title":"<code>simple</code>","text":"<p>This is the default execution mode which batches the documents and processes each batch on the current process in a sequential manner.</p>"},{"location":"concepts/inference/#edsnlp.processing.multiprocessing.execute_multiprocessing_backend","title":"<code>multiprocessing</code>","text":"<p>If you have multiple CPU cores, and optionally multiple GPUs, we provide the <code>multiprocessing</code> backend that allows to run the inference on multiple processes.</p> <p>This accelerator dispatches the batches between multiple workers (data-parallelism), and distribute the computation of a given batch on one or two workers (model-parallelism):</p> <ul> <li>a <code>CPUWorker</code> which handles the non deep-learning components and the   preprocessing, collating and postprocessing of deep-learning components</li> <li>a <code>GPUWorker</code> which handles the forward call of the deep-learning components</li> </ul> <p>If no GPU is available, no <code>GPUWorker</code> is started, and the <code>CPUWorkers</code> handle the forward call of the deep-learning components as well.</p> <p>The advantage of dedicating a worker to the deep-learning components is that it allows to prepare multiple batches in parallel in multiple <code>CPUWorker</code>, and ensure that the <code>GPUWorker</code> never wait for a batch to be ready.</p> <p>The overall architecture described in the following figure, for 3 CPU workers and 2 GPU workers.</p> <p>Here is how a small pipeline with rule-based components and deep-learning components is distributed between the workers:</p>"},{"location":"concepts/inference/#edsnlp.processing.spark.execute_spark_backend","title":"<code>spark</code>","text":"<p>This execution mode uses Spark to parallelize the processing of the documents. The documents are first stored in a Spark DataFrame (if it was not already the case) and then processed in parallel using Spark.</p> <p>Beware, if the original reader was not a SparkReader (<code>edsnlp.data.from_spark</code>), the local docs \u2192 spark dataframe conversion might take some time, and the whole process might be slower than using the <code>multiprocessing</code> backend.</p>"},{"location":"concepts/inference/#batching","title":"Batching","text":"<p>Many operations rely on batching, either to be more efficient or because they require a fixed-size input. The <code>batch_size</code> and <code>batch_by</code> argument of the <code>map_batches()</code> method allows you to specify the size of the batches and what function to use to compute the size of the batches.</p> <pre><code># Accumulate in chunks of 1024 documents\nlengths = data.map_batches(len, batch_size=1024)\n\n# Accumulate in chunks of 100 000 words\nlengths = data.map_batches(len, batch_size=100_000, batch_by=\"words\")\n# or\nlengths = data.map_batches(len, batch_size=\"100_000 words\")\n</code></pre> <p>We also support special values for <code>batch_size</code> which use \"sentinels\" (i.e. markers inserted in the stream) to delimit the batches.</p> <pre><code># Accumulate every element of the input in a single batch\n# which is useful when looping over the data in training\nlengths = data.map_batches(len, batch_size=\"dataset\")\n\n# Accumulate in chunks of fragments, in the case of parquet datasets\nlengths = data.map_batches(len, batch_size=\"fragments\")\n</code></pre> <p>Note that these batch functions are only available under specific conditions:</p> <ul> <li>either <code>backend=\"simple\"</code> or <code>deterministic=True</code> (default) if <code>backend=\"multiprocessing\"</code>, otherwise elements might be processed out of order</li> <li>if every op before was elementwise (e.g. <code>map()</code>, <code>map_gpu()</code>, <code>map_pipeline()</code> and no generator function), or <code>sentinel_mode</code> was explicitly set to <code>\"split\"</code> in <code>map_batches()</code>, otherwise the sentinel are dropped by default when the user requires batching.</li> </ul>"},{"location":"concepts/pipeline/","title":"Pipeline","text":"<p>The goal of EDS-NLP is to provide a framework for processing textual documents.</p> <p>Processing textual documents, and clinical documents in particular, usually involves many steps such as tokenization, cleaning, named entity recognition, span classification, normalization, linking, etc. Organising these steps together, combining static and deep learning components, while remaining modular and efficient is a challenge. This is why EDS-NLP is built on top of a novel pipelining system.</p> <p>Deep learning frameworks</p> <p>Trainable components in EDS-NLP are built around the PyTorch framework. While you can use any technology in static components, we do not provide tools to train components built with other deep learning frameworks.</p>"},{"location":"concepts/pipeline/#compatibility-with-spacy-and-pytorch","title":"Compatibility with spaCy and PyTorch","text":"<p>While EDS-NLP is built on top of its own pipeline system, it is also designed to be compatible with the awesome spaCy framework. This means that you can use (non-trainable) EDS-NLP components in a spaCy pipeline, and vice-versa. Documents, objects that are passed through the pipeline, are in fact spaCy documents, and we borrow many of spaCy's method names and conventions to make the transition between the two libraries as smooth as possible.</p> <p>Trainable components, on the other hand, are built on top of the PyTorch framework. This means that you can use PyTorch components in an EDS-NLP pipeline and benefit from the latest advances in deep learning research. For more information on PyTorch components, refer to the Torch component page.</p>"},{"location":"concepts/pipeline/#creating-a-pipeline","title":"Creating a pipeline","text":"<p>A pipeline is composed of multiple pipes, i.e., callable processing blocks, like a function, that apply a transformation on a Doc object, such as adding annotations, and return the modified object.</p> <p>To create your first EDS-NLP pipeline, run the following code. We provide several ways to create a pipeline:</p> EDS-NLP APISpaCy-like APIFrom a YAML config fileFrom a INI config file <p>This is the recommended way to create a pipeline, as it allows auto-completion, type checking and introspection (you can click on the component or its arguments to see the documentation in most IDEs).</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.matcher(regex={\"smoker\": [\"fume\", \"clope\"]}))\nnlp.add_pipe(eds.negation())\n</code></pre> <p>Curried components</p> <p>Most components (like <code>eds.matcher</code>) require an <code>nlp</code> argument initialization. The above <code>eds.matcher(regex={\"smoker\": [\"fume\", \"clope\"]})</code> actually returns a \"curried\" component, that will be instantiated when added to the pipeline. To create the actual component directly and use it outside of a pipeline (not recommended), you can use <code>eds.matcher(nlp, regex={\"smoker\": [\"fume\", \"clope\"]})</code>, or use the result of the <code>nlp.add_pipe</code> call.</p> <p>Pipes can be dynamically added to the pipeline using the <code>add_pipe</code> method, with a string matching their factory name and an optional configuration dictionary.</p> <pre><code>import edsnlp  # or import spacy\n\nnlp = edsnlp.blank(\"eds\")  # or spacy.blank(\"eds\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.matcher\", config=dict(regex={\"smoker\": [\"fume\", \"clope\"]}))\nnlp.add_pipe(\"eds.negation\")\n</code></pre> <p>You can also create a pipeline from a configuration file. This is useful when you plan on changing the pipeline configuration often.</p> config.yml<pre><code>nlp:\n\"@core\": pipeline\nlang: eds\ncomponents:\nsentences:\n\"@factory\": eds.sentences\n\nmatcher:\n\"@factory\": eds.matcher\nregex:\nsmoker: [\"fume\", \"clope\"]\n\nnegation:\n\"@factory\": eds.negation\n</code></pre> <p>and then load the pipeline with:</p> <pre><code>import edsnlp\n\nnlp = edsnlp.load(\"config.yml\")\n</code></pre> <p>You can also create a pipeline from a configuration file. This is useful when you plan on changing the pipeline configuration often.</p> config.cfg<pre><code>[nlp]\n@core = \"pipeline\"\nlang = \"eds\"\npipeline = [\"sentences\", \"matcher\", \"negation\"]\n\n[components.sentences]\n@factory = \"eds.sentences\"\n\n[components.matcher]\n@factory = \"eds.matcher\"\nregex = {\"smoker\": [\"fume\", \"clope\"]}\n\n[components.negation]\n@factory = \"eds.negation\"\n</code></pre> <p>and then load the pipeline with:</p> <pre><code>import edsnlp\n\nnlp = edsnlp.load(\"config.cfg\")\n</code></pre> <p>This pipeline can then be run on one or more texts documents. As the pipeline process documents, components will be called in the order they were added to the pipeline.</p> <pre><code>from pathlib import Path\n\n# Processing one document\nnlp(\"Le patient ne fume pas\")\n\n# Processing multiple documents\nnlp.pipe([text1, text2])\n</code></pre> <p>For more information on how to use the pipeline, refer to the Inference page.</p>"},{"location":"concepts/pipeline/#hybrid-models","title":"Hybrid models","text":"<p>EDS-NLP was designed to facilitate the training and inference of hybrid models that arbitrarily chain static components or trained deep learning components. Static components are callable objects that take a Doc object as input, perform arbitrary transformations over the input, and return the modified object. Torch components, on the other hand, allow for deep learning operations to be performed on the Doc object and must be trained to be used.</p> <p></p>"},{"location":"concepts/pipeline/#saving-and-loading-a-pipeline","title":"Saving and loading a pipeline","text":"<p>Pipelines can be saved and loaded using the <code>save</code> and <code>load</code> methods. Following spaCy, the saved pipeline is not a pickled objet but a folder containing the config file, the weights and extra resources for each pipeline. Deep-learning parameters are saved with the <code>safetensors</code> library to avoid any security issue. This allows for easy inspection and modification of the pipeline, and avoids the execution of arbitrary code when loading a pipeline.</p> <pre><code>nlp.to_disk(\"path/to/your/model\")\nnlp = edsnlp.load(\"path/to/your/model\")\n</code></pre>"},{"location":"concepts/pipeline/#sharing-a-pipeline","title":"Sharing a pipeline","text":"<p>To share the pipeline and turn it into a pip installable package, you can use the <code>package</code> method, which will use or create a pyproject.toml file, fill it accordingly, and create a wheel file. At the moment, we only support the poetry package manager.</p> <pre><code>nlp.package(\n    name=\"your-package-name\",  # leave None to reuse name in pyproject.toml\n    version=\"0.0.1\",\n    root_dir=\"path/to/project/root\",  # optional, to retrieve an existing pyproject.toml file\n    # if you don't have a pyproject.toml, you can provide the metadata here instead\n    metadata=dict(\n        authors=\"Firstname Lastname &lt;your.email@domain.fr&gt;\",\n        description=\"A short description of your package\",\n    ),\n)\n</code></pre> <p>This will create a wheel file in the root_dir/dist folder, which you can share and install with pip.</p>"},{"location":"concepts/torch-component/","title":"Torch Component","text":"<p>Torch components allow for deep learning operations to be performed on the Doc object and must be trained to be used. Such pipes can be used to train a model to detect named entities, predict the label of a document or an attribute of a text span, and so on.</p> <p></p> Example of sharing and nesting components"},{"location":"concepts/torch-component/#anatomy-of-a-trainable-pipe","title":"Anatomy of a trainable pipe","text":"<p>Building and running deep learning models usually requires to <code>preprocess</code> the input sample into features, to batch or <code>collate</code> these features together to process multiple samples at once, running deep learning operations over these features (in Pytorch, this step is done in the <code>forward</code> method) and to <code>postprocess</code> the outputs of these operation to complete the original sample.</p> <p>In the trainable pipes of EDS-NLP, preprocessing and postprocessing are decoupled from the deep learning code but collocated with the forward method. This is achieved by splitting the class of a trainable component into four methods, which allows us to keep the development of new deep-learning components simple while ensuring efficient models both during training and inference.</p> Methods of a trainable component"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.preprocess","title":"<code>preprocess</code>","text":"<p>Preprocess the document to extract features that will be used by the neural network and its subcomponents on to perform its predictions.</p>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.preprocess--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>Document to preprocess</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>Dictionary (optionally nested) containing the features extracted from the document.</p>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.collate","title":"<code>collate</code>","text":"<p>Collate the batch of features into a single batch of tensors that can be used by the forward method of the component.</p>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.collate--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <p>Batch of features</p> <p> TYPE: <code>Dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>BatchInput</code> <p>Dictionary (optionally nested) containing the collated tensors</p>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.forward","title":"<code>forward</code>","text":"<p>Perform the forward pass of the neural network.</p>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.forward--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <p>Batch of tensors (nested dictionary) computed by the collate method</p> <p> TYPE: <code>BatchInput</code> </p> RETURNS DESCRIPTION <code>BatchOutput</code>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.postprocess","title":"<code>postprocess</code>","text":"<p>Update the documents with the predictions of the neural network. By default, this is a no-op.</p> <p>Additionally, there is a fifth method:</p>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.postprocess--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>List of documents to update</p> <p> TYPE: <code>Sequence[Doc]</code> </p> <code>results</code> <p>Batch of predictions, as returned by the forward method</p> <p> TYPE: <code>BatchOutput</code> </p> <code>inputs</code> <p>List of preprocessed features, as returned by the preprocess method</p> <p> TYPE: <code>List[Dict[str, Any]]</code> </p> RETURNS DESCRIPTION <code>Sequence[Doc]</code>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.post_init","title":"<code>post_init</code>","text":"<p>This method completes the attributes of the component, by looking at some documents. It is especially useful to build vocabularies or detect the labels of a classification task.</p>"},{"location":"concepts/torch-component/#edsnlp.core.torch_component.TorchComponent.post_init--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>gold_data</code> <p>The documents to use for initialization.</p> <p> TYPE: <code>Iterable[Doc]</code> </p> <code>exclude</code> <p>The names of components to exclude from initialization. This argument will be gradually updated  with the names of initialized components</p> <p> TYPE: <code>Set[str]</code> </p>"},{"location":"concepts/torch-component/#nesting-trainable-pipes","title":"Nesting trainable pipes","text":"<p>Like pytorch modules, you can compose trainable pipes together to build complex architectures. For instance, a trainable named entity recognition component may delegate some of its logic to an embedding component, which will only be responsible for converting words into a embeddings. Nesting pipes allows switching parts of the neural networks to test various architectures and keeping the modelling logic modular.</p> <p>Nested preprocessing</p> <p>This is where the idea of collocating <code>preprocess</code> and <code>collate</code> with <code>forward</code> really shines: much like pytorch modules, they can be called recursively on the subcomponents of a trainable pipe. This allows to extend the composition pattern to the preprocessing step to enable true modularity.</p>"},{"location":"concepts/torch-component/#sharing-subcomponents","title":"Sharing subcomponents","text":"<p>Sharing parts of a neural network while training on different tasks can be an effective way to improve the network efficiency. For instance, it is common to share an embedding layer between multiple tasks that require embedding the same inputs.</p> <p>In EDS-NLP, sharing a subcomponent is simply done by sharing the object between the multiple pipes. You can either refer to an existing subcomponent when configuring a new component in Python, or use the interpolation mechanism of our configuration system.</p> API-basedConfiguration-based <pre><code>nlp.add_pipe(\n    eds.ner_crf(\n        ...,\n        embedding=eds.transformer(\n            model=\"bert-base-uncased\",\n            window=128,\n            stride=96,\n        ),\n    ),\n    name=\"first\",\n)\nnlp.add_pipe(\n    some_other_task(\n        embedding=nlp.pipes.first.embedding,\n    ),\n    name=\"second\",\n)\n</code></pre> <pre><code>[components.first]\n@factory = \"eds.ner_crf\"\n...\n\n[components.first.embedding]\n@factory = \"eds.embeddings\"\n...\n\n[components.second]\n@factory = \"some_other_task\"\nembedding = ${components.first.embedding}\n</code></pre> <p>To avoid recomputing the <code>preprocess</code> / <code>forward</code> and <code>collate</code> in the multiple components that use it, we rely on a light cache system.</p> <p>During the training loop, when computing the loss for each component, the forward calls must be wrapped by the <code>pipeline.cache()</code> context to enable this caching mechanism between components.</p>"},{"location":"concepts/torch-component/#implementation-example","title":"Implementation example","text":"<p>Here is a draft of a trainable component:</p> <pre><code>from typing import Any, Dict, Iterable, Sequence, List, Set\n\nimport torch\nfrom tqdm import tqdm\n\nfrom edsnlp import Pipeline, registry\nfrom edsnlp.core.torch_component import TorchComponent\n\n\n@registry.factory.register(\"my-component\")\nclass MyComponent(TorchComponent):\n    def __init__(\n        self,  # A subcomponent\n        nlp: Pipeline,\n        name: str,\n        *,\n        embedding: TorchComponent,\n    ):\n        super().__init__(nlp=nlp, name=name)\n        self.embedding = embedding\n\n    def post_init(self, gold_data: Iterable[\"spacy.tokens.Doc\"], exclude: set):\n\"\"\"\n        This method completes the attributes of the component, by looking at some\n        documents. It is especially useful to build vocabularies or detect the labels\n        of a classification task.\n\n        Parameters\n        ----------\n        gold_data: Iterable[Doc]\n            The documents to use for initialization.\n        exclude: Set\n            The names of components to exclude from initialization.\n            This argument will be gradually updated  with the names of initialized\n            components\n        \"\"\"\n        super().post_init(gold_data, exclude)\n\n        # Initialize the component with the gold documents\n        with self.label_vocabulary.initialization():\n            for doc in tqdm(gold_data, desc=\"Initializing the component\"):\n                # Do something like learning a vocabulary over the initialization\n                # documents\n                ...\n\n        # And post_init the subcomponent\n        self.embedding.post_init(gold_data, exclude)\n\n        # Initialize any layer that might be missing from the module\n        self.classifier = torch.nn.Linear(...)\n\n    def preprocess(self, doc: \"spacy.tokens.Doc\", **kwargs) -&gt; Dict[str, Any]:\n\"\"\"\n        Preprocess the document to extract features that will be used by the\n        neural network and its subcomponents on to perform its predictions.\n\n        Parameters\n        ----------\n        doc: Doc\n            Document to preprocess\n\n        Returns\n        -------\n        Dict[str, Any]\n            Dictionary (optionally nested) containing the features extracted from\n            the document.\n        \"\"\"\n        return {\n            \"embedding\": self.embedding.preprocess(doc),\n            \"my-feature\": ...,\n        }\n\n    def collate(self, batch) -&gt; Dict:\n\"\"\"\n        Collate the batch of features into a single batch of tensors that can be\n        used by the forward method of the component.\n\n        Parameters\n        ----------\n        batch: Dict[str, Any]\n            Batch of features\n\n        Returns\n        -------\n        BatchInput\n            Dictionary (optionally nested) containing the collated tensors\n        \"\"\"\n        return {\n            \"embedding\": self.embedding.collate(batch[\"embedding\"]),\n            \"my-feature\": torch.as_tensor(batch[\"my-feature\"]),\n        }\n\n    def forward(self, batch: Dict) -&gt; Dict:\n\"\"\"\n        Perform the forward pass of the neural network.\n\n        Parameters\n        ----------\n        batch: BatchInput\n            Batch of tensors (nested dictionary) computed by the collate method\n\n        Returns\n        -------\n        BatchOutput\n            Dict of scores, losses, embeddings tensors, etc.\n        \"\"\"\n        # Call the embedding subcomponent\n        embeds = self.embedding(batch[\"embedding\"])\n\n        # Do something with the embedding tensors\n        output = ...\n\n        return output\n\n    def postprocess(\n        self,\n        docs: Sequence[\"spacy.tokens.Doc\"],\n        results: Dict,\n        inputs: List[Dict[str, Any]],\n    ) -&gt; Sequence[\"spacy.tokens.Doc\"]:\n\"\"\"\n        Update the documents with the predictions of the neural network.\n        By default, this is a no-op.\n\n        Parameters\n        ----------\n        docs: Sequence[Doc]\n            List of documents to update\n        results: BatchOutput\n            Batch of predictions, as returned by the forward method\n        inputs: BatchInput\n            List of preprocessed features, as returned by the preprocess method\n\n        Returns\n        -------\n        Sequence[Doc]\n        \"\"\"\n        ...\n        return docs\n</code></pre>"},{"location":"data/","title":"Data connectors","text":"<p>We provide various connectors to read and write data from and to different formats.</p> <p>Reading from a given path or object takes the following form:</p> <pre><code>import edsnlp\n\ndocs = edsnlp.data.read_{format}(  # or .from_{format} for objects\n    # Path to the file or directory\n    \"path/to/file\",\n    # How to convert JSON-like samples to Doc objects\n    converter=predefined schema or function,\n)\n</code></pre> <p>Writing to given path or object takes the following form:</p> <pre><code>import edsnlp\n\nedsnlp.data.write_{format}(  # or .to_{format} for objects\n    # Path to the file or directory\n    \"path/to/file\",\n    # Iterable of Doc objects\n    docs,\n    # How to convert Doc objects to JSON-like samples\n    converter=predefined schema or function,\n)\n</code></pre> <p>The overall process is illustrated in the following diagram:</p> <p></p> <p>At the moment, we support the following data sources:</p> Source Description JSON <code>.json</code> and <code>.jsonl</code> files Standoff &amp; BRAT <code>.ann</code> and <code>.txt</code> files Pandas Pandas DataFrame objects Polars Polars DataFrame objects Spark Spark DataFrame objects <p>and the following schemas:</p> Schema Snippet Custom <code>converter=custom_fn</code> OMOP <code>converter=\"omop\"</code> Standoff <code>converter=\"standoff\"</code> Ents <code>converter=\"ents\"</code> Markup <code>converter=\"markup\"</code>"},{"location":"data/conll/","title":"CoNLL","text":"TLDR <pre><code>import edsnlp\n\nstream = edsnlp.data.read_conll(path)\nstream = stream.map_pipeline(nlp)\n</code></pre> <p>You can easily integrate CoNLL formatted files into your project by using EDS-NLP's CoNLL reader.</p> <p>There are many CoNLL formats corresponding to different shared tasks, but one of the most common is the CoNLL-U format, which is used for dependency parsing. In CoNLL files, each line corresponds to a token and contains various columns with information about the token, such as its index, form, lemma, POS tag, and dependency relation.</p> <p>EDS-NLP lets you specify the name of the <code>columns</code> if they are different from the default CoNLL-U format. If the <code>columns</code> parameter is unset, the reader looks for a comment containing <code># global.columns</code> to infer the column names. Otherwise, the columns are</p> <pre><code>ID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC\n</code></pre> <p>A typical CoNLL file looks like this:</p> sample.conllu<pre><code>1   euh euh INTJ    _   _   5   discourse   _   SpaceAfter=No\n2   ,   ,   PUNCT   _   _   1   punct   _   _\n3   il  lui PRON    _   Gender=Masc|Number=Sing|Person=3|PronType=Prs   5   expl:subj   _   _\n...\n</code></pre>"},{"location":"data/conll/#edsnlp.data.conll.read_conll","title":"Reading CoNLL files","text":"<p>The ConllReader (or <code>edsnlp.data.read_conll</code>) reads a file or directory of CoNLL files and yields documents.</p> <p>The raw output (i.e., by setting <code>converter=None</code>) will be in the following form for a single doc:</p> <pre><code>{\n    \"words\": [\n        {\"ID\": \"1\", \"FORM\": ...},\n        ...\n    ],\n}\n</code></pre>"},{"location":"data/conll/#edsnlp.data.conll.read_conll--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.read_conll(\"path/to/conll/file/or/directory\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.read_conll</code> returns a Stream. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list :</p> <pre><code>docs = list(edsnlp.data.read_conll(\"path/to/conll/file/or/directory\"))\n</code></pre>"},{"location":"data/conll/#edsnlp.data.conll.read_conll--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path to the directory containing the CoNLL files (will recursively look for files in subdirectories).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>columns</code> <p>List of column names to use. If None, will try to extract to look for a <code>#global.columns</code> comment at the start of the file to extract the column names.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>shuffle</code> <p>Whether to shuffle the data. If \"dataset\", the whole dataset will be shuffled before starting iterating on it (at the start of every epoch if looping).</p> <p> TYPE: <code>Literal['dataset', False]</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>loop</code> <p>Whether to loop over the data indefinitely.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>nlp</code> <p>The pipeline object (optional and likely not needed, prefer to use the <code>tokenizer</code> directly argument instead).</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>tokenizer</code> <p>The tokenizer instance used to tokenize the documents. Likely not needed since by default it uses the current context tokenizer :</p> <ul> <li>the tokenizer of the next pipeline run by <code>.map_pipeline</code> in a   Stream.</li> <li>or the <code>eds</code> tokenizer by default.</li> </ul> <p> TYPE: <code>Optional[Tokenizer]</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects.</p> <p> TYPE: <code>Optional[AsList[Union[str, Callable]]]</code> DEFAULT: <code>['conll']</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p>"},{"location":"data/converters/","title":"Converters","text":"<p>Data can be read from and writen to various sources, like JSON/BRAT/CSV files or dataframes, which expect a key-value representation and not Doc object. For that purpose, we document here a set of converters that can be used to convert between these representations and Doc objects.</p> <p>Converters can be configured in the <code>from_*</code> (or <code>read_*</code> in the case of files) and <code>to_*</code> (or <code>write_*</code> in the case of files) methods, depending on the chosen <code>converter</code> argument, which can be:</p> <ul> <li>a function, in which case it will be interpreted as a custom converter</li> <li>a string, in which case it will be interpreted as the name of a pre-defined converter</li> </ul>"},{"location":"data/converters/#none","title":"No converter (<code>converter=None</code>)","text":"<p>Except in <code>read_standoff</code> and <code>write_standoff</code>, the default converter is <code>None</code>. When <code>converter=None</code>, readers output the raw content of the input data (most often dictionaries) and writers expect dictionaries. This can actually be useful is you plan to use Streams without converting to Doc objects, for instance to parallelizing the execution of a function on raw Json, Parquet files or simple lists.</p> <pre><code>import edsnlp.data\n\n\ndef complex_func(n):\n    return n * n\n\n\nstream = edsnlp.data.from_iterable(range(20))\nstream = stream.map(complex_func)\nstream = stream.set_processing(num_cpu_workers=2)\nres = list(stream)\n</code></pre>"},{"location":"data/converters/#custom","title":"Custom converter","text":"<p>You can always define your own converter functions to convert between your data and Doc objects.</p>"},{"location":"data/converters/#reading-from-a-custom-schema","title":"Reading from a custom schema","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\nfrom spacy.tokens import Doc\nfrom edsnlp.data.converters import get_current_tokenizer\nfrom typing import Dict\n\ndef convert_row_to_dict(row: Dict) -&gt; Doc:\n    # Tokenizer will be inferred from the pipeline\n    doc = get_current_tokenizer()(row[\"custom_content\"])\n    doc._.note_id = row[\"custom_id\"]\n    doc._.note_datetime = row[\"custom_datetime\"]\n    # ...\n    return doc\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.covid())\n\n# Any kind of reader (`edsnlp.data.read/from_...`) can be used here\ndocs = edsnlp.data.from_pandas(\n    # Path to the file or directory\n    dataframe,\n    # How to convert JSON-like samples to Doc objects\n    converter=convert_row_to_dict,\n)\ndocs = docs.map_pipeline(nlp)\n</code></pre>"},{"location":"data/converters/#writing-to-a-custom-schema","title":"Writing to a custom schema","text":"<pre><code>def convert_doc_to_row(doc: Doc) -&gt; Dict:\n    return {\n        \"custom_id\": doc._.id,\n        \"custom_content\": doc.text,\n        \"custom_datetime\": doc._.note_datetime,\n        # ...\n    }\n\n# Any kind of writer (`edsnlp.data.write/to_...`) can be used here\ndocs.write_parquet(\n    \"path/to/output_folder\",\n    # How to convert Doc objects to JSON-like samples\n    converter=convert_doc_to_row,\n)\n</code></pre> <p>One row per entity</p> <p>This function can also return a list of dicts, for instance one dict per detected entity, that will be treated as multiple rows in dataframe writers (e.g., <code>to_pandas</code>, <code>to_spark</code>, <code>write_parquet</code>).</p> <pre><code>def convert_ents_to_rows(doc: Doc) -&gt; List[Dict]:\n    return [\n        {\n            \"note_id\": doc._.id,\n            \"ent_text\": ent.text,\n            \"ent_label\": ent.label_,\n            \"custom_datetime\": doc._.note_datetime,\n            # ...\n        }\n        for ent in doc.ents\n    ]\n\n\ndocs.write_parquet(\n    \"path/to/output_folder\",\n    # How to convert entities of Doc objects to JSON-like samples\n    converter=convert_ents_to_rows,\n)\n</code></pre>"},{"location":"data/converters/#omop","title":"OMOP (<code>converter=\"omop\"</code>)","text":"<p>OMOP is a schema that is used in the medical domain. It is based on the OMOP Common Data Model. We are mainly interested in the <code>note</code> table, which contains the clinical notes, and deviate from the original schema by adding an optional <code>entities</code> column that can be computed from the <code>note_nlp</code> table.</p> <p>Therefore, a complete OMOP-style document would look like this:</p> <pre><code>{\n\"note_id\": 0,\n\"note_text\": \"Le patient ...\",\n\"entities\": [\n{\n\"note_nlp_id\": 0,\n\"start_char\": 3,\n\"end_char\": 10,\n\"lexical_variant\": \"patient\",\n\"note_nlp_source_value\": \"person\",\n\n# optional fields\n\"negated\": False,\n\"certainty\": \"probable\",\n...\n},\n...\n],\n\n# optional fields\n\"custom_doc_field\": \"...\"\n...\n}\n</code></pre>"},{"location":"data/converters/#edsnlp.data.converters.OmopDict2DocConverter","title":"Converting OMOP data to Doc objects","text":""},{"location":"data/converters/#edsnlp.data.converters.OmopDict2DocConverter--examples","title":"Examples","text":"<pre><code># Any kind of reader (`edsnlp.data.read/from_...`) can be used here\ndocs = edsnlp.data.from_pandas(\n    df,\n    converter=\"omop\",\n\n    # Optional parameters\n    tokenizer=tokenizer,\n    doc_attributes=[\"note_datetime\"],\n\n    # Parameters below should only matter if you plan to import entities\n    # from the dataframe. If the data doesn't contain pre-annotated\n    # entities, you can ignore these.\n    span_setter={\"ents\": True, \"*\": True},\n    span_attributes={\"negation\": \"negated\"},\n    default_attributes={\"negated\": False, \"temporality\": \"present\"},\n)\n</code></pre>"},{"location":"data/converters/#edsnlp.data.converters.OmopDict2DocConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object (optional and likely not needed, prefer to use the <code>tokenizer</code> directly argument instead).</p> <p> </p> <code>tokenizer</code> <p>The tokenizer instance used to tokenize the documents. Likely not needed since by default it uses the current context tokenizer :</p> <ul> <li>the tokenizer of the next pipeline run by <code>.map_pipeline</code> in a   Stream.</li> <li>or the <code>eds</code> tokenizer by default.</li> </ul> <p> TYPE: <code>Optional[Tokenizer]</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use when setting the spans in the documents. Defaults to setting the spans in the <code>ents</code> attribute, and creates a new span group for each JSON entity label.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, '*': True}</code> </p> <code>doc_attributes</code> <p>Mapping from JSON attributes to additional Span extensions (can be a list too). By default, all attributes are imported as Doc extensions with the same name.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{'note_datetime': 'note_datetime'}</code> </p> <code>span_attributes</code> <p>Mapping from JSON attributes to Span extensions (can be a list too). By default, all attributes are imported as Span extensions with the same name.</p> <p> TYPE: <code>Optional[AttributesMappingArg]</code> DEFAULT: <code>None</code> </p> <code>default_attributes</code> <p>How to set attributes on spans for which no attribute value was found in the input format. This is especially useful for negation, or frequent attributes values (e.g. \"negated\" is often False, \"temporal\" is often \"present\"), that annotators may not want to annotate every time.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p>"},{"location":"data/converters/#edsnlp.data.converters.OmopDoc2DictConverter","title":"Converting Doc objects to OMOP data","text":""},{"location":"data/converters/#edsnlp.data.converters.OmopDoc2DictConverter--examples","title":"Examples","text":"<pre><code># Any kind of writer (`edsnlp.data.write/to_...`) can be used here\ndf = edsnlp.data.to_pandas(\n    docs,\n    converter=\"omop\",\n\n    # Optional parameters\n    span_getter={\"ents\": True},\n    doc_attributes=[\"note_datetime\"],\n    span_attributes=[\"negation\", \"family\"],\n)\n# or docs.to_pandas(...) if it's already a\n# [Stream][edsnlp.core.stream.Stream]\n</code></pre>"},{"location":"data/converters/#edsnlp.data.converters.OmopDoc2DictConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use when getting the spans from the documents. Defaults to getting the spans in the <code>ents</code> attribute.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>doc_attributes</code> <p>Mapping from Doc extensions to JSON attributes (can be a list too). By default, no doc attribute is exported, except <code>note_id</code>.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p> <code>span_attributes</code> <p>Mapping from Span extensions to JSON attributes (can be a list too). By default, no attribute is exported.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p>"},{"location":"data/converters/#standoff","title":"Standoff (<code>converter=\"standoff\"</code>)","text":"<p>Standoff refers mostly to the BRAT standoff format, but doesn't indicate how the annotations should be stored in a JSON-like schema. We use the following schema:</p> <pre><code>{\n\"doc_id\": 0,\n\"text\": \"Le patient ...\",\n\"entities\": [\n{\n\"entity_id\": 0,\n\"label\": \"drug\",\n\"fragments\": [{\n\"start\": 0,\n\"end\": 10\n}],\n\"attributes\": {\n\"negated\": True,\n\"certainty\": \"probable\"\n}\n},\n...\n]\n}\n</code></pre>"},{"location":"data/converters/#edsnlp.data.converters.StandoffDict2DocConverter","title":"Converting Standoff data to Doc objects","text":"<p>Why does BRAT/Standoff need a converter ?</p> <p>You may wonder : why do I need a converter ? Since BRAT is already a NLP oriented format, it should be straightforward to convert it to a Doc object.</p> <p>Indeed, we do provide a default converter for the BRAT standoff format, but we also acknowledge that there may be more than one way to convert a standoff document to a Doc object. For instance, an annotated span may be used to represent a relation between two smaller included entities, or another entity scope, etc.</p> <p>In such cases, we recommend you use a custom converter as described here.</p>"},{"location":"data/converters/#edsnlp.data.converters.StandoffDict2DocConverter--examples","title":"Examples","text":"<pre><code># Any kind of reader (`edsnlp.data.read/from_...`) can be used here\ndocs = edsnlp.data.read_standoff(\n    \"path/to/standoff\",\n    converter=\"standoff\",  # set by default\n\n    # Optional parameters\n    tokenizer=tokenizer,\n    span_setter={\"ents\": True, \"*\": True},\n    span_attributes={\"negation\": \"negated\"},\n    keep_raw_attribute_values=False,\n    default_attributes={\"negated\": False, \"temporality\": \"present\"},\n)\n</code></pre>"},{"location":"data/converters/#edsnlp.data.converters.StandoffDict2DocConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object (optional and likely not needed, prefer to use the <code>tokenizer</code> directly argument instead).</p> <p> </p> <code>tokenizer</code> <p>The tokenizer instance used to tokenize the documents. Likely not needed since by default it uses the current context tokenizer :</p> <ul> <li>the tokenizer of the next pipeline run by <code>.map_pipeline</code> in a   Stream.</li> <li>or the <code>eds</code> tokenizer by default.</li> </ul> <p> TYPE: <code>Optional[Tokenizer]</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use when setting the spans in the documents. Defaults to setting the spans in the <code>ents</code> attribute, and creates a new span group for each JSON entity label.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, '*': True}</code> </p> <code>span_attributes</code> <p>Mapping from BRAT attributes to Span extensions (can be a list too). By default, all attributes are imported as Span extensions with the same name.</p> <p> TYPE: <code>Optional[AttributesMappingArg]</code> DEFAULT: <code>None</code> </p> <code>keep_raw_attribute_values</code> <p>Whether to keep the raw attribute values (as strings) or to convert them to Python objects (e.g. booleans).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default_attributes</code> <p>How to set attributes on spans for which no attribute value was found in the input format. This is especially useful for negation, or frequent attributes values (e.g. \"negated\" is often False, \"temporal\" is often \"present\"), that annotators may not want to annotate every time.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p> <code>notes_as_span_attribute</code> <p>If set, the AnnotatorNote annotations will be concatenated and stored in a span attribute with this name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>split_fragments</code> <p>Whether to split the fragments into separate spans or not. If set to False, the fragments will be concatenated into a single span.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"data/converters/#edsnlp.data.converters.StandoffDoc2DictConverter","title":"Converting Doc objects to Standoff data","text":""},{"location":"data/converters/#edsnlp.data.converters.StandoffDoc2DictConverter--examples","title":"Examples","text":"<pre><code># Any kind of writer (`edsnlp.data.read/from_...`) can be used here\nedsnlp.data.write_standoff(\n    docs,\n    converter=\"standoff\",  # set by default\n\n    # Optional parameters\n    span_getter={\"ents\": True},\n    span_attributes=[\"negation\"],\n)\n# or docs.to_standoff(...) if it's already a\n# [Stream][edsnlp.core.stream.Stream]\n</code></pre>"},{"location":"data/converters/#edsnlp.data.converters.StandoffDoc2DictConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use when getting the spans from the documents. Defaults to getting the spans in the <code>ents</code> attribute.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>{'ents': True}</code> </p> <code>span_attributes</code> <p>Mapping from Span extensions to JSON attributes (can be a list too). By default, no attribute is exported, except <code>note_id</code>.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p>"},{"location":"data/converters/#edsnlp.data.converters.EntsDoc2DictConverter","title":"Entities (<code>converter=\"ents\"</code>)","text":"<p>We also provide a simple one-way (export) converter to convert Doc into a list of dictionaries, one per entity, that can be used to write to a dataframe. The schema of each produced row is the following:</p> <pre><code>{\n\"note_id\": 0,\n\"start\": 3,\n\"end\": 10,\n\"label\": \"drug\",\n\"lexical_variant\": \"patient\",\n\n# Optional fields\n\"negated\": False,\n\"certainty\": \"probable\"\n...\n}\n</code></pre>"},{"location":"data/converters/#edsnlp.data.converters.EntsDoc2DictConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use when getting the spans from the documents. Defaults to getting the spans in the <code>ents</code> attribute.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>doc_attributes</code> <p>Mapping from Doc extensions to JSON attributes (can be a list too). By default, no doc attribute is exported, except <code>note_id</code>.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p> <code>span_attributes</code> <p>Mapping from Span extensions to JSON attributes (can be a list too). By default, no attribute is exported.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p>"},{"location":"data/converters/#edsnlp.data.converters.MarkupToDocConverter","title":"Markup (<code>converter=\"markup\"</code>)","text":"<p>This converter is used to convert markup data, such as Markdown or XML into documents. This can be particularly useful when you want to create annotated documents from scratch (e.g., for testing purposes).</p>"},{"location":"data/converters/#edsnlp.data.converters.MarkupToDocConverter--examples","title":"Examples","text":"<pre><code>import edsnlp\n\n# Any kind of reader (`edsnlp.data.read/from_...`) can be used here\n# If input items are dicts, the converter expects a \"text\" key/column.\ndocs = list(\n    edsnlp.data.from_iterable(\n        [\n            \"This [is](VERB negation=True) not a [test](NOUN).\",\n            \"This is another [test](NOUN).\",\n        ],\n        converter=\"markup\",\n        span_setter=\"entities\",\n    ),\n)\nprint(docs[0].spans[\"entities\"])\n# Out: [is, test]\n</code></pre> <p>You can also use it directly on a string:</p> <pre><code>from edsnlp.data.converters import MarkupToDocConverter\n\nconverter = MarkupToDocConverter(\n    span_setter={\"verb\": \"VERB\", \"noun\": \"NOUN\"},\n    preset=\"xml\",\n)\ndoc = converter(\"This &lt;VERB negation=True&gt;is&lt;/VERB&gt; not a &lt;NOUN&gt;test&lt;/NOUN&gt;.\")\nprint(doc.spans[\"verb\"])\n# Out: [is]\nprint(doc.spans[\"verb\"][0]._.negation)\n# Out: True\n</code></pre>"},{"location":"data/converters/#edsnlp.data.converters.MarkupToDocConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>preset</code> <p>The preset to use for the markup format. Defaults to \"md\" (Markdown-like syntax). Use \"xml\" for XML-like syntax.</p> <p> TYPE: <code>Literal['md', 'xml']</code> DEFAULT: <code>'md'</code> </p> <code>opener</code> <p>The regex pattern to match the opening tag of the markup. Defaults to the preset's opener.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>closer</code> <p>The regex pattern to match the closing tag of the markup. Defaults to the preset's closer.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>tokenizer</code> <p>The tokenizer instance used to tokenize the documents. Likely not needed since by default it uses the current context tokenizer :</p> <ul> <li>the tokenizer of the next pipeline run by <code>.map_pipeline</code> in a   Stream.</li> <li>or the <code>eds</code> tokenizer by default.</li> </ul> <p> TYPE: <code>Optional[Tokenizer]</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use when setting the spans in the documents. Defaults to setting the spans in the <code>ents</code> attribute and creates a new span group for each JSON entity label.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, '*': True}</code> </p> <code>span_attributes</code> <p>Mapping from markup attributes to Span extensions (can be a list too). By default, all attributes are imported as Span extensions with the same name.</p> <p> TYPE: <code>Optional[AttributesMappingArg]</code> DEFAULT: <code>None</code> </p> <code>keep_raw_attribute_values</code> <p>Whether to keep the raw attribute values (as strings) or to convert them to Python objects (e.g. booleans).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default_attributes</code> <p>How to set attributes on spans for which no attribute value was found in the input format. This is especially useful for negation, or frequent attributes values (e.g. \"negated\" is often False, \"temporal\" is often \"present\"), that annotators may not want to annotate every time.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p> <code>bool_attributes</code> <p>List of boolean attributes to set to False by default. This is useful for attributes that are often not annotated, but you want to have a default value for them.</p> <p> TYPE: <code>AsList[str]</code> DEFAULT: <code>[]</code> </p>"},{"location":"data/json/","title":"JSON","text":"TLDR <pre><code>import edsnlp\n\nstream = edsnlp.data.read_json(path, converter=\"omop\")\nstream = stream.map_pipeline(nlp)\nres = stream.to_json(path, converter=\"omop\")\n# or equivalently\nedsnlp.data.to_json(stream, path, converter=\"omop\")\n</code></pre> <p>We provide methods to read and write documents (raw or annotated) from and to json files.</p> <p>As an example, imagine that we have the following document that uses the OMOP schema</p> data.jsonl<pre><code>{ \"note_id\": 0, \"note_text\": \"Le patient ...\", \"note_datetime\": \"2021-10-23\", \"entities\": [...] }\n{ \"note_id\": 1, \"note_text\": \"Autre doc ...\", \"note_datetime\": \"2022-12-24\", \"entities\": [] }\n...\n</code></pre> <p>You could also have multiple <code>.json</code> files in a directory, the reader will read them all.</p>"},{"location":"data/json/#edsnlp.data.json.read_json","title":"Reading JSON files","text":"<p>The JsonReader (or <code>edsnlp.data.read_json</code>) reads a directory of JSON files and yields documents. At the moment, only entities and attributes are loaded.</p>"},{"location":"data/json/#edsnlp.data.json.read_json--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.read_json(\"path/to/json/dir\", converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.read_json</code> returns a Stream. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.read_json(\"path/to/json/dir\", converter=\"omop\")\n</code></pre>"},{"location":"data/json/#edsnlp.data.json.read_json--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path to the directory containing the JSON files (will recursively look for files in subdirectories).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>keep_ipynb_checkpoints</code> <p>Whether to keep the files have \".ipynb_checkpoints\" in their path.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>shuffle</code> <p>Whether to shuffle the data. If \"dataset\", the whole dataset will be shuffled before starting iterating on it (at the start of every epoch if looping).</p> <p> TYPE: <code>Literal['dataset', False]</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>loop</code> <p>Whether to loop over the data indefinitely.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converters to use to convert the JSON objects to Doc objects. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[AsList[Union[str, Callable]]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"data/json/#edsnlp.data.json.write_json","title":"Writing JSON files","text":"<p><code>edsnlp.data.write_json</code> writes a list of documents using the JSON format in a directory. If <code>lines</code> is false, each document will be stored in its own JSON file, named after the FILENAME field returned by the converter (commonly the <code>note_id</code> attribute of the documents), and subdirectories will be created if the name contains <code>/</code> characters.</p>"},{"location":"data/json/#edsnlp.data.json.write_json--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.write_json([doc], \"path/to/json/file\", converter=\"omop\", lines=True)\n# or to write a directory of JSON files, ensure that each doc has a doc._.note_id\n# attribute, since this will be used as a filename:\nedsnlp.data.write_json([doc], \"path/to/json/dir\", converter=\"omop\", lines=False)\n</code></pre> <p>Overwriting files</p> <p>By default, <code>write_json</code> will raise an error if the directory already exists and contains files with <code>.a*</code> or <code>.txt</code> suffixes. This is to avoid overwriting existing annotations. To allow overwriting existing files, use <code>overwrite=True</code>.</p>"},{"location":"data/json/#edsnlp.data.json.write_json--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a Stream).</p> <p> TYPE: <code>Union[Any, Stream]</code> </p> <code>path</code> <p>Path to either - a file if <code>lines</code> is true : this will write the documents as a JSONL file - a directory if <code>lines</code> is false: this will write one JSON file per document   using the FILENAME field returned by the converter (commonly the <code>note_id</code>   attribute of the documents) as the filename.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>lines</code> <p>Whether to write the documents as a JSONL file or as a directory of JSON files. By default, this is inferred from the path: if the path is a file, lines is assumed to be true, otherwise it is assumed to be false.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>overwrite</code> <p>Whether to overwrite existing directories.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>execute</code> <p>Whether to execute the writing operation immediately or to return a stream</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before writing them. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"data/pandas/","title":"Pandas","text":"TLDR <pre><code>import edsnlp\n\nstream = edsnlp.data.from_pandas(df, converter=\"omop\")\nstream = stream.map_pipeline(nlp)\nres = stream.to_pandas(converter=\"omop\")\n# or equivalently\nedsnlp.data.to_pandas(stream, converter=\"omop\")\n</code></pre> <p>We provide methods to read and write documents (raw or annotated) from and to Pandas DataFrames.</p> <p>As an example, imagine that we have the following OMOP dataframe (we'll name it <code>note_df</code>)</p> note_id note_text note_datetime 0 Le patient est admis pour une pneumopathie... 2021-10-23"},{"location":"data/pandas/#edsnlp.data.pandas.from_pandas","title":"Reading from a Pandas Dataframe","text":"<p>The PandasReader (or <code>edsnlp.data.from_pandas</code>) handles reading from a table and yields documents. At the moment, only entities and attributes are loaded. Relations and events are not supported.</p>"},{"location":"data/pandas/#edsnlp.data.pandas.from_pandas--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.from_pandas(df, nlp=nlp, converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.from_pandas</code> returns a Stream. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.from_pandas(df, converter=\"omop\"))\n</code></pre>"},{"location":"data/pandas/#edsnlp.data.pandas.from_pandas--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>Pandas object</p> <p> </p> <code>shuffle</code> <p>Whether to shuffle the data. If \"dataset\", the whole dataset will be shuffled before starting iterating on it (at the start of every epoch if looping).</p> <p> TYPE: <code>Literal['dataset', False]</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>loop</code> <p>Whether to loop over the data indefinitely.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converters to use to convert the rows of the DataFrame (represented as dicts) to Doc objects. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[AsList[Union[str, Callable]]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"data/pandas/#edsnlp.data.pandas.to_pandas","title":"Writing to a Pandas DataFrame","text":"<p><code>edsnlp.data.to_pandas</code> writes a list of documents as a pandas table.</p>"},{"location":"data/pandas/#edsnlp.data.pandas.to_pandas--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.to_pandas([doc], converter=\"omop\")\n</code></pre>"},{"location":"data/pandas/#edsnlp.data.pandas.to_pandas--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a Stream).</p> <p> TYPE: <code>Union[Any, Stream]</code> </p> <code>dtypes</code> <p>Dictionary of column names to dtypes. This is passed to <code>pd.DataFrame.astype</code>.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>execute</code> <p>Whether to execute the writing operation immediately or to return a stream</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before storing them in the dataframe. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"data/pandas/#importing-entities-from-a-pandas-dataframe","title":"Importing entities from a Pandas DataFrame","text":"<p>If you have a dataframe with entities (e.g., <code>note_nlp</code> in OMOP), you must join it with the dataframe containing the raw text (e.g., <code>note</code> in OMOP) to obtain a single dataframe with the entities next to the raw text. For instance, the second <code>note_nlp</code> dataframe that we will name <code>note_nlp_df</code>.</p> note_nlp_id note_id start_char end_char note_nlp_source_value lexical_variant 0 0 46 57 disease coronavirus 1 0 77 88 drug parac\u00e9tamol ... ... ... ... ... ... <pre><code>df = (\n    note_df\n    .set_index(\"note_id\")\n    .join(\n        note_nlp_df\n        .set_index('note_id')\n        .groupby(level=0)\n        .apply(pd.DataFrame.to_dict, orient='records')\n        .rename(\"entities\")\n    )\n).reset_index()\n</code></pre> note_id note_text note_datetime entities 0 Le patient... 2021-10-23 <code>[{\"note_nlp_id\": 0, \"start_char\": 46, ...]</code> ... ... ... ..."},{"location":"data/parquet/","title":"Parquet","text":"TLDR <pre><code>import edsnlp\n\nstream = edsnlp.data.read_parquet(path, converter=\"omop\")\nstream = stream.map_pipeline(nlp)\nres = stream.to_parquet(path, converter=\"omop\")\n# or equivalently\nedsnlp.data.to_parquet(stream, path, converter=\"omop\")\n</code></pre> <p>We provide methods to read and write documents (raw or annotated) from and to parquet files.</p> <p>As an example, imagine that we have the following document that uses the OMOP schema (parquet files are not actually stored as human-readable text, but this is for the sake of the example):</p> data.pq<pre><code>{ \"note_id\": 0, \"note_text\": \"Le patient ...\", \"note_datetime\": \"2021-10-23\", \"entities\": [...] }\n{ \"note_id\": 1, \"note_text\": \"Autre doc ...\", \"note_datetime\": \"2022-12-24\", \"entities\": [] }\n...\n</code></pre> <p>You could also have multiple parquet files in a directory, the reader will read them all.</p>"},{"location":"data/parquet/#edsnlp.data.parquet.read_parquet","title":"Reading Parquet files","text":"<p>The ParquetReader (or <code>edsnlp.data.read_parquet</code>) reads a directory of parquet files (or a single file) and yields documents.</p>"},{"location":"data/parquet/#edsnlp.data.parquet.read_parquet--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.read_parquet(\"path/to/parquet\", converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.read_parquet</code> returns a Stream. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.read_parquet(\"path/to/parquet\", converter=\"omop\"))\n</code></pre>"},{"location":"data/parquet/#edsnlp.data.parquet.read_parquet--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path to the directory containing the parquet files (will recursively look for files in subdirectories). Supports any filesystem supported by pyarrow.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>shuffle</code> <p>Whether to shuffle the data. If \"dataset\", the whole dataset will be shuffled before starting iterating on it (at the start of every epoch if looping). If \"fragment\", shuffling will occur between and inside the parquet files, but not across them.</p> <p>Dataset shuffling</p> <p>Shuffling the dataset can be expensive, especially for large datasets, since it requires reading the entire dataset into memory. If you have a large dataset, consider shuffling at the \"fragment\" level.</p> <p> TYPE: <code>Literal['dataset', 'fragment', False]</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>loop</code> <p>Whether to loop over the data indefinitely.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>work_unit</code> <p>Only affects the multiprocessing mode. If \"record\", every worker will start to read the same parquet file and yield each every num_workers-th record, starting at an offset each. For instance, if num_workers=2, the first worker will read the 1st, 3rd, 5th, ... records, while the second worker will read the 2nd, 4th, 6th, ... records of the first parquet file.</p> <p>If \"fragment\", each worker will read a different parquet file. For instance, the first worker will every record of the 1st parquet file, the second worker will read every record of the 2nd parquet file, and so on. This way, no record is \"wasted\" and every record loaded in memory is yielded.</p> <p> TYPE: <code>Literal['record', 'fragment']</code> DEFAULT: <code>'record'</code> </p> <p>converter: Optional[AsList[Union[str, Callable]]]     Converters to use to convert the parquet rows of the data source to Doc objects     These are documented on the Converters page. kwargs:     Additional keyword arguments to pass to the converter. These are documented on     the Converters page.</p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"data/parquet/#edsnlp.data.parquet.write_parquet","title":"Writing Parquet files","text":"<p><code>edsnlp.data.write_parquet</code> writes a list of documents as a parquet dataset.</p>"},{"location":"data/parquet/#edsnlp.data.parquet.write_parquet--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.write_parquet([doc], \"path/to/parquet\")\n</code></pre> <p>Overwriting files</p> <p>By default, <code>write_parquet</code> will raise an error if the directory already exists and contains parquet files. This is to avoid overwriting existing annotations. To allow overwriting existing files, use <code>overwrite=True</code>.</p>"},{"location":"data/parquet/#edsnlp.data.parquet.write_parquet--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a Stream).</p> <p> TYPE: <code>Union[Any, Stream]</code> </p> <code>path</code> <p>Path to the directory containing the parquet files (will recursively look for files in subdirectories). Supports any filesystem supported by pyarrow.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>batch_size</code> <p>The maximum number of documents to write in each parquet file.</p> <p> TYPE: <code>Optional[Union[int, str]]</code> DEFAULT: <code>None</code> </p> <code>batch_by</code> <p>The method to batch the documents. If \"docs\", the batch size is the number of documents. If \"fragment\", each batch corresponds to a parquet file fragment from the input data.</p> <p> TYPE: <code>BatchBy</code> DEFAULT: <code>None</code> </p> <code>write_in_worker</code> <p>In multiprocessing or spark mode, whether to batch and write the documents in the workers or in the main process.</p> <p>For instance, a worker may read the 1st, 3rd, 5th, ... documents, while another reads the 2nd, 4th, 6th, ... documents.</p> <p>If <code>write_in_worker</code> is False, <code>deterministic</code> is True (default) and no operation adds or remove document from the stream (e.g., no <code>map_batches</code>), the original order of the documents will be recovered in the main process, and batching there can produce fragments that respect the original order.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>overwrite</code> <p>Whether to overwrite existing directories.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>pyarrow_write_kwargs</code> <p>Additional keyword arguments to pass to the <code>pyarrow.parquet.write_to_dataset</code></p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>execute</code> <p>Whether to execute the writing operation immediately or to return a stream</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before writing them as Parquet rows. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"data/polars/","title":"Polars","text":"TLDR <pre><code>import edsnlp\n\nstream = edsnlp.data.from_polars(df, converter=\"omop\")\nstream = stream.map_pipeline(nlp)\nres = stream.to_polars(converter=\"omop\")\n# or equivalently\nedsnlp.data.to_polars(stream, converter=\"omop\")\n</code></pre> <p>We provide methods to read and write documents (raw or annotated) from and to Polars DataFrames.</p> <p>As an example, imagine that we have the following OMOP dataframe (we'll name it <code>note_df</code>)</p> note_id note_text note_datetime 0 Le patient est admis pour une pneumopathie... 2021-10-23"},{"location":"data/polars/#edsnlp.data.polars.from_polars","title":"Reading from a Polars Dataframe","text":"<p>The PolarsReader (or <code>edsnlp.data.from_polars</code>) handles reading from a table and yields documents. At the moment, only entities and attributes are loaded. Relations and events are not supported.</p>"},{"location":"data/polars/#edsnlp.data.polars.from_polars--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.from_polars(df, nlp=nlp, converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.from_polars</code> returns a Stream. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.from_polars(df, converter=\"omop\"))\n</code></pre>"},{"location":"data/polars/#edsnlp.data.polars.from_polars--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>Polars object</p> <p> TYPE: <code>Union[DataFrame, LazyFrame]</code> </p> <code>shuffle</code> <p>Whether to shuffle the data. If \"dataset\", the whole dataset will be shuffled at the beginning (of every epoch if looping).</p> <p> TYPE: <code>Literal['dataset', False]</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>loop</code> <p>Whether to loop over the data indefinitely.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converters to use to convert the rows of the DataFrame (represented as dicts) to Doc objects. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[AsList[Union[str, Callable]]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"data/polars/#edsnlp.data.polars.to_polars","title":"Writing to a Polars DataFrame","text":"<p><code>edsnlp.data.to_polars</code> writes a list of documents as a polars dataframe.</p>"},{"location":"data/polars/#edsnlp.data.polars.to_polars--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.to_polars([doc], converter=\"omop\")\n</code></pre>"},{"location":"data/polars/#edsnlp.data.polars.to_polars--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a Stream).</p> <p> TYPE: <code>Union[Any, Stream]</code> </p> <code>dtypes</code> <p>Dictionary of column names to dtypes. This is passed to the schema parameter of <code>pl.from_dicts</code>.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before storing them in the dataframe. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>execute</code> <p>Whether to execute the writing operation immediately or to return a stream</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"data/spark/","title":"Spark","text":"TLDR <pre><code>import edsnlp\n\nstream = edsnlp.data.from_spark(df, converter=\"omop\")\nstream = stream.map_pipeline(nlp)\nres = stream.to_spark(converter=\"omop\")\n# or equivalently\nedsnlp.data.to_spark(stream, converter=\"omop\")\n</code></pre> <p>We provide methods to read and write documents (raw or annotated) from and to Spark DataFrames.</p> <p>As an example, imagine that we have the following OMOP dataframe (we'll name it <code>note_df</code>)</p> note_id note_text note_datetime 0 Le patient est admis pour une pneumopathie... 2021-10-23"},{"location":"data/spark/#edsnlp.data.spark.from_spark","title":"Reading from a Spark Dataframe","text":"<p>The SparkReader (or <code>edsnlp.data.from_spark</code>) reads a pyspark (or koalas) DataFrame and yields documents. At the moment, only entities and span attributes are loaded.</p>"},{"location":"data/spark/#edsnlp.data.spark.from_spark--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.from_spark(note_df, converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.from_spark</code> returns a Stream To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.from_spark(note_df, converter=\"omop\"))\n</code></pre>"},{"location":"data/spark/#edsnlp.data.spark.from_spark--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The DataFrame to read.</p> <p> </p> <code>shuffle</code> <p>Whether to shuffle the data. If \"dataset\", the whole dataset will be shuffled before starting iterating on it (at the start of every epoch if looping).</p> <p> TYPE: <code>Literal['dataset', False]</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>loop</code> <p>Whether to loop over the data indefinitely.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converters to use to convert the rows of the DataFrame to Doc objects. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[AsList[Union[str, Callable]]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"data/spark/#edsnlp.data.spark.to_spark","title":"Writing to a Spark DataFrame","text":"<p><code>edsnlp.data.to_spark</code> converts a list of documents into a Spark DataFrame, usually one row per document, unless the converter returns a list in which case each entry of the resulting list will be stored in its own row.</p>"},{"location":"data/spark/#edsnlp.data.spark.to_spark--example","title":"Example","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.covid())\n\nnote_df = sql('''\n    select note_id, note_text from note\n    where note_text is not null\n    limit 500\n''')\n\ndocs = edsnlp.data.from_spark(note_df, converter=\"omop\")\n\ndocs = nlp.pipe(docs)\n\nres = edsnlp.data.to_spark(docs, converter=\"omop\")\n\nres.show()\n</code></pre> <p>Mac OS X</p> <p>If you are using Mac OS X, you may need to set the following environment variable (see this thread) to run pyspark:</p> <pre><code>import os\nos.environ[\"OBJC_DISABLE_INITIALIZE_FORK_SAFETY\"] = \"YES\"\n</code></pre>"},{"location":"data/spark/#edsnlp.data.spark.to_spark--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a Stream).</p> <p> TYPE: <code>Union[Any, Stream]</code> </p> <code>dtypes</code> <p>The schema to use for the DataFrame.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>show_dtypes</code> <p>Whether to print the inferred schema (only if <code>dtypes</code> is None).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>execute</code> <p>Whether to execute the writing operation immediately or to return a stream</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before storing them in the dataframe. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"data/spark/#importing-entities-from-a-spark-dataframe","title":"Importing entities from a Spark DataFrame","text":"<p>If you have a dataframe with entities (e.g., <code>note_nlp</code> in OMOP), you must join it with the dataframe containing the raw text (e.g., <code>note</code> in OMOP) to obtain a single dataframe with the entities next to the raw text. For instance, the second <code>note_nlp</code> dataframe that we will name <code>note_nlp</code>.</p> note_nlp_id note_id start_char end_char note_nlp_source_value lexical_variant 0 0 46 57 disease coronavirus 1 0 77 88 drug parac\u00e9tamol <pre><code>import pyspark.sql.functions as F\n\ndf = note_df.join(\n    note_nlp_df\n    .groupBy(\"note_id\")\n    .agg(\n        F.collect_list(\n            F.struct(\n                F.col(\"note_nlp_id\"),\n                F.col(\"start_char\"),\n                F.col(\"end_char\"),\n                F.col(\"note_nlp_source_value\")\n            )\n        ).alias(\"entities\")\n    ), \"note_id\", \"left\")\n</code></pre>"},{"location":"data/standoff/","title":"BRAT and Standoff","text":"TLDR <pre><code>import edsnlp\n\nstream = edsnlp.data.read_standoff(path)\nstream = stream.map_pipeline(nlp)\nres = stream.write_standoff(path)\n# or equivalently\nedsnlp.data.write_standoff(stream, path)\n</code></pre> <p>You can easily integrate BRAT into your project by using EDS-NLP's BRAT reader and writer.</p> <p>BRAT annotations are in the standoff format. Consider the following document:</p> doc.txt<pre><code>Le patient est admis pour une pneumopathie au coronavirus.\nOn lui prescrit du parac\u00e9tamol.\n</code></pre> <p>Brat annotations are stored in a separate file formatted as follows:</p> doc.ann<pre><code>T1  Patient 4 11    patient\nT2  Disease 31 58   pneumopathie au coronavirus\nT3  Drug 79 90  parac\u00e9tamol\n</code></pre>"},{"location":"data/standoff/#edsnlp.data.standoff.read_standoff","title":"Reading Standoff files","text":"<p>The BratReader (or <code>edsnlp.data.read_standoff</code>) reads a directory of BRAT files and yields documents. At the moment, only entities and attributes are loaded. Relations  and events are not supported.</p>"},{"location":"data/standoff/#edsnlp.data.standoff.read_standoff--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.read_standoff(\"path/to/brat/directory\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.read_standoff</code> returns a Stream. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list :</p> <pre><code>docs = list(edsnlp.data.read_standoff(\"path/to/brat/directory\"))\n</code></pre> <p>True/False attributes</p> <p>Boolean values are not supported by the BRAT editor, and are stored as empty (key: empty value) if true, and not stored otherwise. This means that False values will not be assigned to attributes by default, which can be problematic when deciding if an entity is negated or not : is the entity not negated, or has the negation attribute not been annotated ?</p> <p>To avoid this issue, you can use the <code>bool_attributes</code> argument to specify which attributes should be considered as boolean when reading a BRAT dataset. These attributes will be assigned a value of <code>True</code> if they are present, and <code>False</code> otherwise.</p> <pre><code>doc_iterator = edsnlp.data.read_standoff(\n    \"path/to/brat/directory\",\n    span_attributes=[\"negation\", \"family\"],\n    bool_attributes=[\"negation\"],  # Missing values will be set to False\n)\n</code></pre>"},{"location":"data/standoff/#edsnlp.data.standoff.read_standoff--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path to the directory containing the BRAT files (will recursively look for files in subdirectories).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>shuffle</code> <p>Whether to shuffle the data. If \"dataset\", the whole dataset will be shuffled before starting iterating on it (at the start of every epoch if looping).</p> <p> TYPE: <code>Literal['dataset', False]</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>loop</code> <p>Whether to loop over the data indefinitely.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>nlp</code> <p>The pipeline object (optional and likely not needed, prefer to use the <code>tokenizer</code> directly argument instead).</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>tokenizer</code> <p>The tokenizer instance used to tokenize the documents. Likely not needed since by default it uses the current context tokenizer :</p> <ul> <li>the tokenizer of the next pipeline run by <code>.map_pipeline</code> in a   Stream.</li> <li>or the <code>eds</code> tokenizer by default.</li> </ul> <p> TYPE: <code>Optional[Tokenizer]</code> </p> <code>span_setter</code> <p>The span setter to use when setting the spans in the documents. Defaults to setting the spans in the <code>ents</code> attribute, and creates a new span group for each JSON entity label.</p> <p> TYPE: <code>SpanSetterArg</code> </p> <code>span_attributes</code> <p>Mapping from BRAT attributes to Span extensions (can be a list too). By default, all attributes are imported as Span extensions with the same name.</p> <p> TYPE: <code>Optional[AttributesMappingArg]</code> </p> <code>keep_raw_attribute_values</code> <p>Whether to keep the raw attribute values (as strings) or to convert them to Python objects (e.g. booleans).</p> <p> TYPE: <code>bool</code> </p> <code>default_attributes</code> <p>How to set attributes on spans for which no attribute value was found in the input format. This is especially useful for negation, or frequent attributes values (e.g. \"negated\" is often False, \"temporal\" is often \"present\"), that annotators may not want to annotate every time.</p> <p> TYPE: <code>AttributesMappingArg</code> </p> <code>notes_as_span_attribute</code> <p>If set, the AnnotatorNote annotations will be concatenated and stored in a span attribute with this name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>split_fragments</code> <p>Whether to split the fragments into separate spans or not. If set to False, the fragments will be concatenated into a single span.</p> <p> TYPE: <code>bool</code> </p> <code>keep_ipynb_checkpoints</code> <p>Whether to keep the files that are in the <code>.ipynb_checkpoints</code> directory.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>keep_txt_only_docs</code> <p>Whether to keep the <code>.txt</code> files that do not have corresponding <code>.ann</code> files.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects.</p> <p> TYPE: <code>Optional[AsList[Union[str, Callable]]]</code> DEFAULT: <code>['standoff']</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"data/standoff/#edsnlp.data.standoff.write_standoff","title":"Writing Standoff files","text":"<p><code>edsnlp.data.write_standoff</code> writes a list of documents using the BRAT/Standoff format in a directory. The BRAT files will be named after the <code>note_id</code> attribute of the documents, and subdirectories will be created if the name contains <code>/</code> characters.</p>"},{"location":"data/standoff/#edsnlp.data.standoff.write_standoff--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.write_standoff([doc], \"path/to/brat/directory\")\n</code></pre> <p>Overwriting files</p> <p>By default, <code>write_standoff</code> will raise an error if the directory already exists and contains files with <code>.a*</code> or <code>.txt</code> suffixes. This is to avoid overwriting existing annotations. To allow overwriting existing files, use <code>overwrite=True</code>.</p>"},{"location":"data/standoff/#edsnlp.data.standoff.write_standoff--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a Stream).</p> <p> TYPE: <code>Union[Any, Stream]</code> </p> <code>path</code> <p>Path to the directory containing the BRAT files (will recursively look for files in subdirectories).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>span_getter</code> <p>The span getter to use when listing the spans that will be exported as BRAT entities. Defaults to getting the spans in the <code>ents</code> attribute.</p> <p> </p> <code>span_attributes</code> <p>Mapping from BRAT attributes to Span extension. By default, no attribute will be exported.</p> <p> </p> <code>overwrite</code> <p>Whether to overwrite existing directories.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>execute</code> <p>Whether to execute the writing operation immediately or to return a stream</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects. Defaults to the \"standoff\" format converter.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>'standoff'</code> </p>"},{"location":"metrics/","title":"Metrics","text":"<p>EDS-NLP provides several metrics to evaluate the performance of its components. These metrics can be used to assess the quality of entity recognition, negation detection, and other tasks.</p> <p>At the moment, we support the following metrics:</p> Metric Description <code>eds.ner_exact</code> NER metric with exact match at the span level <code>eds.ner_token</code> NER metric with token-level match <code>eds.ner_overlap</code> NER metric with overlap match at the span level <code>eds.span_attribute</code> Span multi-label multi-class classification metric"},{"location":"metrics/ner/","title":"NER Metrics","text":"<p>We provide several metrics to evaluate the performance of Named Entity Recognition (NER) components. Let's look at an example and see how they differ. We'll use the following two documents: a reference document (ref) and a document with predicted entities (pred).</p>"},{"location":"metrics/ner/#shared-example","title":"Shared example","text":"<p>pred</p> <p>ref</p> <p>La patiente a une fi\u00e8vre aig\u00fce</p> <p>La patiente a une fi\u00e8vre aig\u00fce.</p> <p>Let's create matching documents in EDS-NLP using the following code snippet:</p> <pre><code>from edsnlp.data.converters import MarkupToDocConverter\n\nconv = MarkupToDocConverter(preset=\"md\", span_setter=\"entities\")\n\npred = conv(\"[La](PER) [patiente](PER) a une [fi\u00e8vre aigu\u00eb](DIS).\")\nref = conv(\"La [patiente](PER) a [une fi\u00e8vre](DIS) aigu\u00eb.\")\n</code></pre>"},{"location":"metrics/ner/#summary-of-metrics","title":"Summary of metrics","text":"<p>The table below shows the different scores depending on the metric used.</p> Metric Precision Recall F1 Span-level exact 0.33 0.5 0.40 Token-level 0.50 0.67 0.57 Span-level overlap 0.67 1.0 0.80"},{"location":"metrics/ner/#edsnlp.metrics.ner.NerExactMetric","title":"Span-level NER metric with exact match","text":"<p>The <code>eds.ner_exact</code> metric scores the extracted entities (that may be overlapping or nested) by looking in the spans returned by a given SpanGetter object and comparing predicted spans to gold spans for exact boundary and label matches.</p> <p>Let's view these elements as collections of (span \u2192 label) and count how many of the predicted spans match the gold spans exactly (and vice versa):</p> <p>pred</p> <p>ref</p> <p>La patiente fi\u00e8vre aigu\u00eb</p> <p>patiente une fi\u00e8vre </p> <p>Precision, Recall and F1 (micro-average and per\u2010label) are computed as follows:</p> <ul> <li>Precision: <code>p = |matched items of pred| / |pred|</code></li> <li>Recall: <code>r = |matched items of ref| / |ref|</code></li> <li>F1: <code>f = 2 / (1/p + 1/f)</code></li> </ul>"},{"location":"metrics/ner/#edsnlp.metrics.ner.NerExactMetric--examples","title":"Examples","text":"<pre><code>from edsnlp.metrics.ner import NerExactMetric\n\nmetric = NerExactMetric(span_getter=conv.span_setter, micro_key=\"micro\")\nmetric([ref], [pred])\n# Out: {\n#   'micro': {'f': 0.4, 'p': 0.33, 'r': 0.5, 'tp': 1, 'support': 2, 'positives': 3},\n#   'PER': {'f': 0.67, 'p': 0.5, 'r': 1, 'tp': 1, 'support': 1, 'positives': 2},\n#   'DIS': {'f': 0.0, 'p': 0.0, 'r': 0.0, 'tp': 0, 'support': 1, 'positives': 1},\n# }\n</code></pre>"},{"location":"metrics/ner/#edsnlp.metrics.ner.NerExactMetric--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use to extract the spans from the document</p> <p> TYPE: <code>SpanGetterArg</code> </p> <code>micro_key</code> <p>The key to use to store the micro-averaged results for spans of all types</p> <p> TYPE: <code>str</code> DEFAULT: <code>'micro'</code> </p> <code>filter_expr</code> <p>The filter expression to use to filter the documents. Evaluated with <code>doc</code> as the variable.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p>"},{"location":"metrics/ner/#edsnlp.metrics.ner.NerOverlapMetric","title":"Span-level NER metric with approximate match","text":"<p>The <code>eds.ner_overlap</code> metric scores the extracted entities that may be overlapping or nested by looking in the spans returned by a given SpanGetter object and counting a prediction as correct if it overlaps by at least the given Dice\u2010coefficient threshold with a gold span of the same label.</p> <p>This metric is useful for evaluating NER systems where the exact boundaries do not matter too much, but the presence of the entity at the same spot is important. For instance, you may not want to penalize a system that forgets determiners if the rest of the entity is correctly identified.</p> <p>Let's view these elements as sets of (span \u2192 label) and count how many of the predicted spans match the gold spans by at least the given Dice coefficient (and vice versa):</p> <p>pred</p> <p>ref</p> <p>La patiente fi\u00e8vre aigu\u00eb</p> <p>patiente une fi\u00e8vre </p> <p>Precision, Recall and F1 (micro-average and per\u2010label) are computed as follows:</p> <ul> <li>Precision: <code>p = |matched items of pred| / |pred|</code></li> <li>Recall: <code>r = |matched items of ref| / |ref|</code></li> <li>F1: <code>f = 2 / (1/p + 1/f)</code></li> </ul> <p>Overlap threshold</p> <p>The threshold is the minimum Dice coefficient to consider two spans as overlapping. Setting it to 1.0 will yield the same results as the <code>eds.ner_exact</code> metric, while setting it to a near-zero value (e.g., like 1e-14) will match any two spans that share at least one token.</p>"},{"location":"metrics/ner/#edsnlp.metrics.ner.NerOverlapMetric--examples","title":"Examples","text":"<pre><code>from edsnlp.metrics.ner import NerOverlapMetric\n\nmetric = NerOverlapMetric(\n    span_getter=conv.span_setter, micro_key=\"micro\", threshold=0.5\n)\nmetric([ref], [pred])\n# Out: {\n#   'micro': {'f': 0.8, 'p': 0.67, 'r': 1.0, 'tp': 2, 'support': 2, 'positives': 3},\n#   'PER': {'f': 0.67, 'p': 0.5, 'r': 1.0, 'tp': 1, 'support': 1, 'positives': 2},\n#   'DIS': {'f': 1.0, 'p': 1.0, 'r': 1.0, 'tp': 1, 'support': 1, 'positives': 1}\n# }\n</code></pre>"},{"location":"metrics/ner/#edsnlp.metrics.ner.NerOverlapMetric--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use to extract the spans from the document</p> <p> TYPE: <code>SpanGetterArg</code> </p> <code>micro_key</code> <p>The key to use to store the micro-averaged results for spans of all types</p> <p> TYPE: <code>str</code> DEFAULT: <code>'micro'</code> </p> <code>filter_expr</code> <p>The filter expression to use to filter the documents</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>threshold</code> <p>The threshold on the Dice coefficient to consider two spans as overlapping</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p>"},{"location":"metrics/ner/#edsnlp.metrics.ner.NerTokenMetric","title":"Token-level NER metric","text":"<p>The <code>eds.ner_token</code> metric scores the extracted entities that may be overlapping or nested by looking in <code>doc.ents</code>, and <code>doc.spans</code>, and comparing the predicted and gold entities at the token level.</p> <p>Assuming we use the <code>eds</code> (or <code>fr</code> or <code>en</code>) tokenizer, in the above example, there are 3 annotated tokens in the reference, and 4 annotated tokens in the prediction. Let's view these elements as sets of (token, label) and count how many of the predicted tokens match the gold tokens exactly (and vice versa):</p> <p>pred</p> <p>ref</p> <p>La patiente fi\u00e8vre aigu\u00eb</p> <p>patiente une fi\u00e8vre </p> <p>Precision, Recall and F1 (micro-average and per\u2010label) are computed as follows:</p> <ul> <li>Precision: <code>p = |matched items of pred| / |pred|</code></li> <li>Recall: <code>r = |matched items of ref| / |ref|</code></li> <li>F1: <code>f = 2 / (1/p + 1/f)</code></li> </ul>"},{"location":"metrics/ner/#edsnlp.metrics.ner.NerTokenMetric--examples","title":"Examples","text":"<pre><code>from edsnlp.metrics.ner import NerTokenMetric\n\nmetric = NerTokenMetric(span_getter=conv.span_setter, micro_key=\"micro\")\nmetric([ref], [pred])\n# Out: {\n#   'micro': {'f': 0.57, 'p': 0.5, 'r': 0.67, 'tp': 2, 'support': 3, 'positives': 4},\n#   'PER': {'f': 0.67, 'p': 0.5, 'r': 1, 'tp': 1, 'support': 1, 'positives': 2},\n#   'DIS': {'f': 0.5, 'p': 0.5, 'r': 0.5, 'tp': 1, 'support': 2, 'positives': 2}\n# }\n</code></pre>"},{"location":"metrics/ner/#edsnlp.metrics.ner.NerTokenMetric--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use to extract the spans from the document</p> <p> TYPE: <code>SpanGetterArg</code> </p> <code>micro_key</code> <p>The key to use to store the micro-averaged results for spans of all types</p> <p> TYPE: <code>str</code> DEFAULT: <code>'micro'</code> </p> <code>filter_expr</code> <p>The filter expression to use to filter the documents. Will be evaluated with <code>doc</code> as the variable name, so you can use <code>doc.ents</code>, <code>doc.spans</code>, etc.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p>"},{"location":"metrics/span-attribute/","title":"Span Attribute Classification Metrics","text":"<p>Several NLP tasks consist in classifying existing spans of text into multiple classes, such as the detection of negation, hypothesis or span linking. We provide a metric to evaluate the performance of such tasks.</p> <p>Let's look at an example. We'll use the following two documents: a reference document (ref) and a document with predicted entities (pred).</p> <p>pred</p> <p>ref</p> <p>Le patient n'est pas fi\u00e8vreux, son p\u00e8re a du diab\u00e8te. Pas d'\u00e9volution du cancer.</p> <p>Le patient n'est pas fi\u00e8vreux, son p\u00e8re a du diab\u00e8te. Pas d'\u00e9volution du cancer.</p> <p>We can quickly create matching documents in EDS-NLP using the following code snippet:</p> <pre><code>from edsnlp.data.converters import MarkupToDocConverter\n\nconv = MarkupToDocConverter(preset=\"md\", span_setter=\"entities\")\n# Create a document with predicted attributes and a reference document\npred = conv(\n    \"Le patient n'est pas [fi\u00e8vreux](SYMP neg=true), \"\n    \"son p\u00e8re a [du diab\u00e8te](DIS neg=false carrier=PATIENT). \"\n    \"Pas d'\u00e9volution du [cancer](DIS neg=true carrier=PATIENT).\"\n)\nref = conv(\n    \"Le patient n'est pas [fi\u00e8vreux](SYMP neg=true), \"\n    \"son p\u00e8re a [du diab\u00e8te](DIS neg=false carrier=FATHER). \"\n    \"Pas d'\u00e9volution du [cancer](DIS neg=false carrier=PATIENT).\"\n)\n</code></pre> <p>The <code>eds.span_attribute</code> metric evaluates span\u2010level attribute classification by comparing predicted and gold attribute values on the same set of spans. For each attribute you specify, it computes Precision, Recall, F1, number of true positives (tp), number of gold instances (support), number of predicted instances (positives), and the Average Precision (ap). A micro\u2010average over all attributes is also provided under <code>micro_key</code>.</p> <pre><code>from edsnlp.metrics.span_attribute import SpanAttributeMetric\n\nmetric = SpanAttributeMetric(\n    span_getter=conv.span_setter,\n    # Evaluated attributes\n    attributes={\n        \"neg\": True,  # 'neg' on every entity\n        \"carrier\": [\"DIS\"],  # 'carrier' only on 'DIS' entities\n    },\n    # Ignore these default values when counting matches\n    default_values={\n        \"neg\": False,\n    },\n    micro_key=\"micro\",\n)\n</code></pre> <p>Let's enumerate (span -&gt; attr = value) items in our documents. Only the items with matching span boundaries, attribute name, and value are counted as a true positives. For instance, with the predicted and reference spans of the example above:</p> <p>pred</p> <p>ref</p> <p>fi\u00e8vreux \u2192 neg = True du diab\u00e8te \u2192 neg = False du diab\u00e8te \u2192 carrier = PATIENT cancer \u2192 neg = True cancer \u2192 carrier = PATIENT</p> <p>fi\u00e8vreux \u2192 neg = True du diab\u00e8te \u2192 neg = False du diab\u00e8te \u2192 carrier = FATHER cancer \u2192 neg = False cancer \u2192 carrier = PATIENT</p> <p>Default values</p> <p>Note that there we don't count \"neg=False\" items, shown in grey in the table. In EDS-NLP, this is done by setting <code>defaults_values={\"neg\": False}</code> when creating the metric. This is quite common in classification tasks, where one of the values is both the most common and the \"default\" (hence the name of the parameter). Counting these values would likely skew the micro-average metrics towards the default value.</p> <p>Precision, Recall and F1 (micro-average and per\u2010label) are computed as follows:</p> <ul> <li>Precision: <code>p = |matched items of pred| / |pred|</code></li> <li>Recall: <code>r = |matched items of ref| / |ref|</code></li> <li>F1: <code>f = 2 / (1/p + 1/f)</code></li> </ul> <p>This yields the following metrics:</p> <pre><code>metric([ref], [pred])\n# Out: {\n#   'micro': {'f': 0.57, 'p': 0.5, 'r': 0.67, 'tp': 2, 'support': 3, 'positives': 4, 'ap': 0.17},\n#   'neg': {'f': 0.67, 'p': 0.5, 'r': 1, 'tp': 1, 'support': 1, 'positives': 2, 'ap': 0.0},\n#   'carrier': {'f': 0.5, 'p': 0.5, 'r': 0.5, 'tp': 1, 'support': 2, 'positives': 2, 'ap': 0.25},\n# }\n</code></pre>"},{"location":"metrics/span-attribute/#edsnlp.metrics.span_attribute.SpanAttributeMetric--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to extract spans from each <code>Doc</code>.</p> <p> TYPE: <code>SpanGetterArg</code> </p> <code>attributes</code> <p>Map each attribute name to <code>True</code> (evaluate on all spans) or a sequence of labels restricting which spans to test.</p> <p> TYPE: <code>Mapping[str, Union[bool, Sequence[str]]]</code> DEFAULT: <code>None</code> </p> <code>default_values</code> <p>Attribute values to omit from micro\u2010average counts (e.g., common negative or default labels).</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>include_falsy</code> <p>If <code>False</code>, ignore falsy values (e.g., <code>False</code>, <code>None</code>, <code>''</code>) in predictions or gold when computing metrics; if <code>True</code>, count them.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>micro_key</code> <p>Key under which to store the micro\u2010averaged results across all attributes.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'micro'</code> </p> <code>filter_expr</code> <p>A Python expression (using <code>doc</code>) to filter which examples are scored.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dict[str, Dict[str, float]]</code> <p>A dictionary mapping each attribute name (and the <code>micro_key</code>) to its metrics:</p> <ul> <li> <p><code>label</code> or micro_key :</p> <ul> <li><code>p</code> : precision</li> <li><code>r</code> : recall</li> <li><code>f</code> : F1 score</li> <li><code>tp</code> : true positive count</li> <li><code>support</code> : number of gold instances</li> <li><code>positives</code> : number of predicted instances</li> <li><code>ap</code> : average precision</li> </ul> </li> </ul>"},{"location":"pipes/","title":"Pipes overview","text":"<p>EDS-NLP provides easy-to-use pipeline components (aka pipes).</p>"},{"location":"pipes/#available-components","title":"Available components","text":"CoreQualifiersMiscellaneousNERTrainable <p>See the Core components overview for more information.</p> Component Description <code>eds.normalizer</code> Non-destructive input text normalisation <code>eds.sentences</code> Better sentence boundary detection <code>eds.matcher</code> A simple yet powerful entity extractor <code>eds.terminology</code> A simple yet powerful terminology matcher <code>eds.contextual_matcher</code> A conditional entity extractor <code>eds.endlines</code> An unsupervised model to classify each end line <p>See the Qualifiers overview for more information.</p> Pipeline Description <code>eds.negation</code> Rule-based negation detection <code>eds.family</code> Rule-based family context detection <code>eds.hypothesis</code> Rule-based speculation detection <code>eds.reported_speech</code> Rule-based reported speech detection <code>eds.history</code> Rule-based medical history detection <p>See the Miscellaneous components overview for more information.</p> Component Description <code>eds.dates</code> Date extraction and normalisation <code>eds.consultation_dates</code> Identify consultation dates <code>eds.quantities</code> Quantity extraction and normalisation <code>eds.sections</code> Section detection <code>eds.reason</code> Rule-based hospitalisation reason detection <code>eds.tables</code> Tables detection <code>eds.split</code> Doc splitting <code>eds.explode</code> Explode entities between multiples copies of a document <p>See the NER overview for more information.</p> Component Description <code>eds.covid</code> A COVID mentions detector <code>eds.charlson</code> A Charlson score extractor <code>eds.sofa</code> A SOFA score extractor <code>eds.elston_ellis</code> An Elston &amp; Ellis code extractor <code>eds.emergency_priority</code> A priority score extractor <code>eds.emergency_ccmu</code> A CCMU score extractor <code>eds.emergency_gemsa</code> A GEMSA score extractor <code>eds.tnm</code> A TNM score extractor <code>eds.adicap</code> A ADICAP codes extractor <code>eds.drugs</code> A drug mentions extractor <code>eds.cim10</code> A CIM10 terminology matcher <code>eds.umls</code> An UMLS terminology matcher <code>eds.ckd</code> CKD extractor <code>eds.copd</code> COPD extractor <code>eds.cerebrovascular_accident</code> Cerebrovascular accident extractor <code>eds.congestive_heart_failure</code> Congestive heart failure extractor <code>eds.connective_tissue_disease</code> Connective tissue disease extractor <code>eds.dementia</code> Dementia extractor <code>eds.diabetes</code> Diabetes extractor <code>eds.hemiplegia</code> Hemiplegia extractor <code>eds.leukemia</code> Leukemia extractor <code>eds.liver_disease</code> Liver disease extractor <code>eds.lymphoma</code> Lymphoma extractor <code>eds.myocardial_infarction</code> Myocardial infarction extractor <code>eds.peptic_ulcer_disease</code> Peptic ulcer disease extractor <code>eds.peripheral_vascular_disease</code> Peripheral vascular disease extractor <code>eds.solid_tumor</code> Solid tumor extractor <code>eds.alcohol</code> Alcohol consumption extractor <code>eds.tobacco</code> Tobacco consumption extractor <p>See the Trainable components overview for more information.</p> Name Description <code>eds.transformer</code> Embed text with a transformer model <code>eds.text_cnn</code> Contextualize embeddings with a CNN <code>eds.span_pooler</code> A span embedding component that aggregates word embeddings <code>eds.ner_crf</code> A trainable component to extract entities <code>eds.extractive_qa</code> A trainable component for extractive question answering <code>eds.span_classifier</code> A trainable component for multi-class multi-label span classification <code>eds.span_linker</code> A trainable entity linker (i.e. to a list of concepts) <code>eds.biaffine_dep_parser</code> A trainable biaffine dependency parser <p>You can add them to your pipeline by simply calling <code>add_pipe</code>, for instance:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.tnm())\n</code></pre>"},{"location":"pipes/#basic-architecture","title":"Basic architecture","text":"<p>Most components provided by EDS-NLP aim to qualify pre-extracted entities. To wit, the basic usage of the library:</p> <ol> <li>Implement a normaliser (see <code>eds.normalizer</code>)</li> <li>Add an entity recognition component (eg the simple but powerful <code>eds.matcher</code> component)</li> <li>Add zero or more entity qualification components, such as <code>eds.negation</code>, <code>eds.family</code> or <code>eds.hypothesis</code>. These qualifiers typically help detect false-positives.</li> </ol>"},{"location":"pipes/#extraction-components","title":"Extraction components","text":"<p>Extraction components (matchers, the date detector or NER components, for instance) keep their results to the <code>doc.ents</code> and <code>doc.spans</code> attributes directly.</p> <p>By default, some components do not write their output to <code>doc.ents</code>, such as the <code>eds.sections</code> matcher. This is mainly due to the fact that, since <code>doc.ents</code> cannot contain overlapping entities, we filter spans and keep the largest one by default. Since sections usually cover large spans of text, storing them in ents would remove every other overlapping entities.</p>"},{"location":"pipes/#entity-tagging","title":"Entity tagging","text":"<p>Moreover, most components declare extensions, on the <code>Doc</code>, <code>Span</code> and/or <code>Token</code> objects.</p> <p>These extensions are especially useful for qualifier components, but can also be used by other components to persist relevant information. For instance, the <code>eds.dates</code> component declares a <code>span._.date</code> extension to store a normalised version of each detected date.</p>"},{"location":"pipes/architecture/","title":"Basic Architecture","text":"<p>Most pipes provided by EDS-NLP aim to qualify pre-extracted entities. To wit, the basic usage of the library:</p> <ol> <li>Implement a normaliser (see <code>eds.normalizer</code>)</li> <li>Add an entity recognition component (eg the simple but powerful <code>eds.matcher</code>)</li> <li>Add zero or more entity qualification components, such as <code>eds.negation</code>, <code>eds.family</code> or <code>eds.hypothesis</code>. These qualifiers typically help detect false-positives.</li> </ol>"},{"location":"pipes/architecture/#scope","title":"Scope","text":"<p>Since the basic usage of EDS-NLP components is to qualify entities, most pipes can function in two modes:</p> <ol> <li>Annotation of the extracted entities (this is the default). To increase throughput, only pre-extracted entities (found in <code>doc.ents</code>) are processed.</li> <li>Full-text, token-wise annotation. This mode is activated by setting the <code>on_ents_only</code> parameter to <code>False</code>.</li> </ol> <p>The possibility to do full-text annotation implies that one could use the pipes the other way around, eg detecting all negations once and for all in an ETL phase, and reusing the results consequently. However, this is not the intended use of the library, which aims to help researchers downstream as a standalone application.</p>"},{"location":"pipes/architecture/#result-persistence","title":"Result persistence","text":"<p>Depending on their purpose (entity extraction, qualification, etc), EDS-NLP pipes write their results to <code>Doc.ents</code>, <code>Doc.spans</code> or in a custom attribute.</p>"},{"location":"pipes/architecture/#extraction-pipes","title":"Extraction pipes","text":"<p>Extraction pipes (matchers, the date detector or NER pipes, for instance) keep their results to the <code>Doc.ents</code> attribute directly.</p> <p>Note that spaCy prohibits overlapping entities within the <code>Doc.ents</code> attribute. To circumvent this limitation, we filter spans, and keep all discarded entities within the <code>discarded</code> key of the <code>Doc.spans</code> attribute.</p> <p>Some pipes write their output to the <code>Doc.spans</code> dictionary. We enforce the following doctrine:</p> <ul> <li>Should the pipe extract entities that are directly informative (typically the output of the <code>eds.matcher</code> component), said entities are stashed in the <code>Doc.ents</code> attribute.</li> <li>On the other hand, should the entity be useful to another pipe, but less so in itself (eg the output of the <code>eds.sections</code> or <code>eds.dates</code> component), it will be stashed in a specific key within the <code>Doc.spans</code> attribute.</li> </ul>"},{"location":"pipes/architecture/#entity-tagging","title":"Entity tagging","text":"<p>Moreover, most pipes declare spaCy extensions, on the <code>Doc</code>, <code>Span</code> and/or <code>Token</code> objects.</p> <p>These extensions are especially useful for qualifier pipes, but can also be used by other pipes to persist relevant information. For instance, the <code>eds.dates</code> pipeline component:</p> <ol> <li>Populates <code>Doc.spans[\"dates\"]</code></li> <li>For each detected item, keeps the normalised date in <code>Span._.date</code></li> </ol> <ol></ol>"},{"location":"pipes/core/","title":"Core Components","text":"<p>This section deals with \"core\" functionalities offered by EDS-NLP:</p> <ul> <li>Generic matchers against regular expressions and list of terms</li> <li>Text cleaning</li> <li>Sentence boundaries detection</li> </ul>"},{"location":"pipes/core/#available-components","title":"Available components","text":"Component Description <code>eds.normalizer</code> Non-destructive input text normalisation <code>eds.sentences</code> Better sentence boundary detection <code>eds.matcher</code> A simple yet powerful entity extractor <code>eds.terminology</code> A simple yet powerful terminology matcher <code>eds.contextual_matcher</code> A conditional entity extractor <code>eds.endlines</code> An unsupervised model to classify each end line"},{"location":"pipes/core/contextual-matcher/","title":"Contextual Matcher","text":"<p>EDS-NLP provides simple pattern matchers like <code>eds.matcher</code> to extract regular expressions, specific phrases, or perform lexical similarity matching on documents. However, certain use cases require examining the context around matched entities to filter out irrelevant matches or enrich them with additional information. For example, to extract mentions of malignant cancers, we need to exclude matches that have \u201cbenin\u201d mentioned nearby : <code>eds.contextual_matcher</code> was built to address such needs.</p>"},{"location":"pipes/core/contextual-matcher/#example","title":"Example","text":"<p>The following example demonstrates how to configure and use <code>eds.contextual_matcher</code> to extract mentions of solid cancers and lymphomas, while filtering out irrelevant mentions (e.g., benign tumors) and enriching entities with contextual information such as stage or metastasis status.</p> <p>Let's dive in with the full code example:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(\n    eds.contextual_matcher(\n        patterns=[\n            dict(\n                terms=[\"cancer\", \"tumeur\"],  # (1)!\n                regex=[r\"adeno(carcinom|[\\s-]?k)\", \"neoplas\", \"melanom\"],  # (2)!\n                regex_attr=\"NORM\",  # (3)!\n                exclude=dict(\n                    regex=\"benign|benin\",  # (4)!\n                    window=3,  # (5)!\n                ),\n                assign=[\n                    dict(\n                        name=\"stage\",  # (6)!\n                        regex=\"stade (I{1,3}V?|[1234])\",  # (7)!\n                        window=\"words[-10:10]\",  # (8)!\n                        replace_entity=False,  # (9)!\n                        reduce_mode=None,  # (10)!\n                    ),\n                    dict(\n                        name=\"metastase\",  # (11)!\n                        regex=\"(metasta)\",  # (12)!\n                        window=10,  # (13)!\n                        replace_entity=False,  # (14)!\n                        reduce_mode=\"keep_last\",  # (15)!\n                    ),\n                ],\n                source=\"Cancer solide\",  # (16)!\n            ),\n            dict(\n                regex=[\"lymphom\", \"lymphangio\"],  # (17)!\n                regex_attr=\"NORM\",  # (18)!\n                exclude=dict(\n                    regex=[\"hodgkin\"],  # (19)!\n                    window=3,  # (20)!\n                ),\n                source=\"Lymphome\",  # (21)!\n            ),\n        ],\n        label=\"cancer\",\n    ),\n)\n</code></pre> <ol> <li>Exact match terms (faster than regex, but less flexible)</li> <li>Regex for flexible matching</li> <li>Apply regex on normalized text</li> <li>Regex to exclude benign mentions</li> <li>Window size for exclusion check</li> <li>Extract cancer stage</li> <li>Stage regex pattern</li> <li>Window range for stage extraction. Visit the documentation of ContextWindow for more information about this syntax.</li> <li>Do not use these matches as replacement for the anchor (default behavior)</li> <li>Keep all matches</li> <li>Detect metastasis</li> <li>Regex for metastasis detection</li> <li>Window size for detection</li> <li>Keep main entity</li> <li>Keep furthest extraction</li> <li>Optional source label for solid tumor. This can be useful to know which pattern matched the entity.</li> <li>Regex patterns for lymphoma</li> <li>Apply regex on normalized text</li> <li>Exclude Hodgkin lymphoma</li> <li>Window size for exclusion</li> <li>Optional source label for lymphoma. This can be useful to know which pattern matched the entity.</li> </ol> <p>Let's explore some examples using this pipeline:</p> Simple matchExclusion ruleExtracting additional infos <pre><code>txt = \"Le patient a eu un cancer il y a 5 ans\"\ndoc = nlp(txt)\nent = doc.ents[0]\n\nent.label_\n# Out: cancer\n\nent._.source\n# Out: Cancer solide\n\nent.text, ent.start, ent.end\n# Out: ('cancer', 5, 6)\n</code></pre> <p>Check exclusion with a benign mention:</p> <pre><code>txt = \"Le patient a eu un cancer relativement b\u00e9nin il y a 5 ans\"\ndoc = nlp(txt)\n\ndoc.ents\n# Out: ()\n</code></pre> <p>Additional information extracted via <code>assign</code> configurations is available in the <code>assigned</code> attribute:</p> <pre><code>txt = \"Le patient a eu un cancer de stade 3.\"\ndoc = nlp(txt)\n\ndoc.ents[0]._.assigned  # (1)!\n# Out: {'stage': ['3']}\n</code></pre> <ol> <li>We get a list for 'stage' because <code>reduce_mode</code> is set to <code>None</code> (default). If you want to keep only the first or last match, set <code>reduce_mode=\"keep_first\"</code> or <code>reduce_mode=\"keep_last\"</code>.</li> </ol>"},{"location":"pipes/core/contextual-matcher/#better-control-over-the-final-extracted-entities","title":"Better control over the final extracted entities","text":"<p>Three main parameters refine how entities are extracted:</p>"},{"location":"pipes/core/contextual-matcher/#include_assigned","title":"<code>include_assigned</code>","text":"<p>Following the previous example, if you want extracted entities to include the cancer stage or metastasis status (if found), set <code>include_assigned=True</code> in the pipe configuration.</p> <p>For instance, from the sentence \"Le patient a un cancer au stade 3\":</p> <ul> <li>If <code>include_assigned=False</code>, the extracted entity is \"cancer\"</li> <li>If <code>include_assigned=True</code>, the extracted entity is \"cancer au stade 3\"</li> </ul>"},{"location":"pipes/core/contextual-matcher/#reduce_mode","title":"<code>reduce_mode</code>","text":"<p>Sometimes, an assignment matches multiple times. For example, in the sentence \"Le patient a un cancer au stade 3 et au stade 4\", both \"stade 3\" and \"stade 4\" match the <code>stage</code> key. Depending on your use case:</p> <ul> <li><code>reduce_mode=None</code> (default): Keeps all matched extractions in a list</li> <li><code>reduce_mode=\"keep_first\"</code>: Keeps only the extraction closest to the main matched entity (\"stade 3\" in this case)</li> <li><code>reduce_mode=\"keep_last\"</code>: Keeps only the furthest extraction</li> </ul>"},{"location":"pipes/core/contextual-matcher/#replace_entity","title":"<code>replace_entity</code>","text":"<p>This parameter can be set to <code>True</code> for only one assign key per dictionary. If set to <code>True</code>, the matched assignment replaces the main entity.</p> <p>Example using \"Le patient a un cancer au stade 3\":</p> <ul> <li>With <code>replace_entity=True</code> for the <code>stage</code> key, the entity extracted is \"stade 3\"</li> <li>With <code>replace_entity=False</code>, the entity extracted remains \"cancer\"</li> </ul> <p>Note: With <code>replace_entity=True</code>, if the corresponding assign key matches nothing, the entity is discarded.</p> <p>The primary configuration is provided in the <code>patterns</code> key as either a pattern dictionary or a list of pattern dictionaries.</p>"},{"location":"pipes/core/contextual-matcher/#edsnlp.pipes.core.contextual_matcher.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>patterns</code> The patterns to match PARAMETER DESCRIPTION <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities in the doc.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> </p> <code>terms</code> <p>A single term or a list of terms (for exact matches)</p> <p> TYPE: <code>Union[RegexFlag, int]</code> </p> <code>exclude</code> One or more exclusion patterns PARAMETER DESCRIPTION <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> TYPE: <code>RegexFlag</code> </p> <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>window</code> <p>Context window to search for patterns around the anchor. Defaults to \"sent\" ( i.e. the sentence of the anchor span).</p> <p> TYPE: <code>Optional[ContextWindow]</code> </p> <p> TYPE: <code>AsList[SingleExcludeModel]</code> </p> <code>include</code> One or more inclusion patterns PARAMETER DESCRIPTION <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> TYPE: <code>RegexFlag</code> </p> <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>window</code> <p>Context window to search for patterns around the anchor. Defaults to \"sent\" ( i.e. the sentence of the anchor span).</p> <p> TYPE: <code>Optional[ContextWindow]</code> </p> <p> TYPE: <code>AsList[SingleIncludeModel]</code> </p> <code>assign</code> One or more assignment patterns PARAMETER DESCRIPTION <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities in the doc.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> TYPE: <code>RegexFlag</code> </p> <code>window</code> <p>Context window to search for patterns around the anchor. Defaults to \"sent\" ( i.e. the sentence of the anchor span).</p> <p> TYPE: <code>Optional[ContextWindow]</code> </p> <code>replace_entity</code> <p>If set to <code>True</code>, the match from the corresponding assign key will be used as entity, instead of the main match. See this paragraph</p> <p> TYPE: <code>Optional[bool]</code> </p> <code>reduce_mode</code> <p>Set how multiple assign matches are handled. See the documentation of the <code>reduce_mode</code> parameter</p> <p> TYPE: <code>Optional[Flags]</code> </p> <code>required</code> <p>If set to <code>True</code>, the assign key must match for the extraction to be kept. If it does not match, the extraction is discarded.</p> <p> TYPE: <code>Optional[str]</code> </p> <p> TYPE: <code>AsList[SingleAssignModel]</code> </p> <code>source</code> <p>A label describing the pattern</p> <p> TYPE: <code>str</code> </p> <p> TYPE: <code>FullConfig</code> </p> <code>assign_as_span</code> <p>Whether to store eventual extractions defined via the <code>assign</code> key as Spans or as string</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>alignment_mode</code> <p>Overwrite alignment mode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>expand</code> </p> <code>regex_flags</code> <p>RegExp flags to use when matching, filtering and assigning (See here)</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>include_assigned</code> <p>Whether to include (eventual) assign matches to the final entity</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>label_name</code> <p>Deprecated, use <code>label</code> instead. The label to assign to the matched entities</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>The label to assign to the matched entities</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p>"},{"location":"pipes/core/contextual-matcher/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.contextual_matcher</code> pipeline component was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/core/endlines/","title":"Endlines","text":"<p>The <code>eds.endlines</code> component classifies newline characters as actual end of lines or mere spaces. In the latter case, the token is removed from the normalised document.</p> <p>Behind the scenes, it uses a <code>endlinesmodel</code> instance, which is an unsupervised algorithm based on the work of Zweigenbaum et al., 2016.</p> <p>Installation</p> <p>To use this component, you need to install the <code>scikit-learn</code> library.</p> <ol><li><p><p>Zweigenbaum P., Grouin C. and Lavergne T., 2016. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (End-of-line classification with no supervision). https://aclanthology.org/2016.jeptalnrecital-poster.7</p></p></li></ol>"},{"location":"pipes/core/endlines/#edsnlp.pipes.core.endlines.factory.create_component--training","title":"Training","text":"<pre><code>import edsnlp\nfrom edsnlp.pipes.core.endlines.model import EndLinesModel\n\nnlp = edsnlp.blank(\"eds\")\n\ntexts = [\n\"\"\"\nLe patient est arriv\u00e9 hier soir.\nIl est accompagn\u00e9 par son fils\n\nANTECEDENTS\nIl a fait une TS en 2010\nFumeur, il est arret\u00e9 il a 5 mois\nChirurgie de coeur en 2011\nCONCLUSION\nIl doit prendre\nle medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin\ndans 1 mois.\nDIAGNOSTIC :\nIl aime le fromage...\n\nAntecedents Familiaux:\n- 1. P\u00e8re avec diabete\n\"\"\",\n\"\"\"\nJ'aime le\nfromage...\n\"\"\",\n]\n\ndocs = list(nlp.pipe(texts))\n\n# Train and predict an EndLinesModel\nendlines = EndLinesModel(nlp=nlp)\n\ndf = endlines.fit_and_predict(docs)\ndf.head()\n\nPATH = \"/tmp/path_to_save\"\nendlines.save(PATH)\n</code></pre>"},{"location":"pipes/core/endlines/#edsnlp.pipes.core.endlines.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\nfrom spacy.tokens import Span\nfrom spacy import displacy\n\nnlp = edsnlp.blank(\"eds\")\n\nPATH = \"/tmp/path_to_save\"\nnlp.add_pipe(eds.endlines(model_path=PATH))\n\ndocs = list(nlp.pipe(texts))\n\ndoc_exemple = docs[1]\n\ndoc_exemple.ents = tuple(\n    Span(doc_exemple, token.i, token.i + 1, \"excluded\")\n    for token in doc_exemple\n    if token.tag_ == \"EXCLUDED\"\n)\n\ndisplacy.render(doc_exemple, style=\"ent\", options={\"colors\": {\"space\": \"red\"}})\n</code></pre>"},{"location":"pipes/core/endlines/#edsnlp.pipes.core.endlines.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.endlines</code> pipe declares one extension, on both <code>Span</code> and <code>Token</code> objects. The <code>end_line</code> attribute is a boolean, set to <code>True</code> if the pipe predicts that the new line is an end line character. Otherwise, it is set to <code>False</code> if the new line is classified as a space.</p> <p>The pipe also sets the <code>excluded</code> custom attribute on newlines that are classified as spaces. It lets downstream matchers skip excluded tokens (see normalisation) for more detail.</p>"},{"location":"pipes/core/endlines/#edsnlp.pipes.core.endlines.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'endlines'</code> </p> <code>model_path</code> <p>Path to trained model. If None, it will use a default model</p> <p> TYPE: <code>Optional[Union[str, EndLinesModel]]</code> DEFAULT: <code>None</code> </p>"},{"location":"pipes/core/endlines/#edsnlp.pipes.core.endlines.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.endlines</code> pipe was developed by AP-HP's Data Science team based on the work of Zweigenbaum et al., 2016.</p>"},{"location":"pipes/core/matcher/","title":"Matcher","text":"<p>EDS-NLP simplifies the matching process by exposing a <code>eds.matcher</code> component that can match on terms or regular expressions.</p>"},{"location":"pipes/core/matcher/#edsnlp.pipes.core.matcher.factory.create_component--examples","title":"Examples","text":"<p>Let us redefine the pipeline :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],  # (1)\n    patient=\"patient\",  # (2)\n)\n\nregex = dict(\n    covid=r\"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\",  # (3)\n)\n\nnlp.add_pipe(\n    eds.matcher(\n        terms=terms,\n        regex=regex,\n        attr=\"LOWER\",\n        term_matcher=\"exact\",\n        term_matcher_config={},\n    ),\n)\n</code></pre> <ol> <li>Every key in the <code>terms</code> dictionary is mapped to a concept.</li> <li>The <code>eds.matcher</code> pipeline expects a list of expressions, or a single expression.</li> <li>We can also define regular expression patterns.</li> </ol> <p>This snippet is complete, and should run as is.</p> <p>Patterns, be they <code>terms</code> or <code>regex</code>, are defined as dictionaries where keys become  the label of the extracted entities. Dictionary values are either a single  expression or a list of expressions that match the concept.</p>"},{"location":"pipes/core/matcher/#edsnlp.pipes.core.matcher.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'matcher'</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p>You won't be able to match on newlines if this is enabled and the \"spaces\"/\"newline\" option of <code>eds.normalizer</code> is enabled (by default).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>exact</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>span_setter</code> <p>How to set the spans in the doc.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p>"},{"location":"pipes/core/matcher/#edsnlp.pipes.core.matcher.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.matcher</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/core/normalizer/","title":"Normalisation","text":"<p>The normalisation scheme used by EDS-NLP adheres to the non-destructive doctrine. In other words,</p> <pre><code>nlp(text).text == text\n</code></pre> <p>is always true.</p> <p>To achieve this, the input text is never modified. Instead, our normalisation strategy focuses on two axes:</p> <ol> <li>Only the <code>NORM</code> and <code>tag_</code> attributes are modified by the <code>normalizer</code> pipeline component ;</li> <li>Pipes (e.g., <code>pollution</code>) can mark tokens as excluded by setting the extension <code>Token.tag_</code> to <code>EXCLUDED</code> or as space by setting the extension <code>Token.tag_</code> to <code>SPACE</code>.    It enables downstream matchers to skip excluded tokens.</li> </ol> <p>The normaliser can act on the input text in five dimensions :</p> <ol> <li>Move the text to lowercase.</li> <li>Remove accents. We use a deterministic approach to avoid modifying the character-length of the text, which helps for RegEx matching.</li> <li>Normalize apostrophes and quotation marks, which are often coded using special characters.</li> <li>Detect spaces and new lines and mark them as such (to be skipped later)</li> <li>Detect tokens in pollutions patterns and mark them as such (to be skipped later)</li> </ol> <p>Note</p> <p>We recommend you also add an end-of-line classifier to remove excess new line characters (introduced by the PDF layout).</p> <p>We provide a <code>endlines</code> pipeline component, which requires training an unsupervised model. Refer to the dedicated page for more information.</p>"},{"location":"pipes/core/normalizer/#usage","title":"Usage","text":"<p>The normalisation is handled by the single <code>eds.normalizer</code> pipeline component. The following code snippet is complete, and should run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom edsnlp.matchers.utils import get_text\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\n\n# Notice the special character used for the apostrophe and the quotes\ntext = \"Le patient est admis \u00e0 l'h\u00f4pital le 23 ao\u00fbt 2021 pour une douleur \u02baaffreuse\u201d \u00e0 l`estomac.\"\n\ndoc = nlp(text)\n\nget_text(doc, attr=\"NORM\", ignore_excluded=False)\n# Out: le patient est admis a l'hopital le 23 aout 2021 pour une douleur \"affreuse\" a l'estomac.\n</code></pre>"},{"location":"pipes/core/normalizer/#utilities","title":"Utilities","text":"<p>To simplify the use of the normalisation output, we provide the <code>get_text</code> utility function. It computes the textual representation for a <code>Span</code> or <code>Doc</code> object.</p> <p>Moreover, every span exposes a <code>normalized_variant</code> extension getter, which computes the normalised representation of an entity on the fly.</p>"},{"location":"pipes/core/normalizer/#configuration","title":"Configuration","text":"<p>The pipeline component can be configured using the following parameters :</p> <p>Normalisation pipeline. Modifies the <code>NORM</code> attribute, acting on five dimensions :</p> <ul> <li><code>lowercase</code>: using the default <code>NORM</code></li> <li><code>accents</code>: deterministic and fixed-length normalisation of accents.</li> <li><code>quotes</code>: deterministic and fixed-length normalisation of quotation marks.</li> <li><code>spaces</code>: \"removal\" of spaces tokens (via the tag_ attribute).</li> <li><code>pollution</code>: \"removal\" of pollutions (via the tag_ attribute).</li> </ul> <p>options: only_parameters: true</p>"},{"location":"pipes/core/normalizer/#edsnlp.pipes.core.normalizer.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'normalizer'</code> </p> <code>lowercase</code> <p>Whether to remove case.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>accents</code> <p><code>Accents</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>quotes</code> <p><code>Quotes</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>spaces</code> <p><code>Spaces</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>pollution</code> <p>Optional <code>Pollution</code> configuration object.</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> Source code in <code>edsnlp/pipes/core/normalizer/factory.py</code> <pre><code>@registry.factory.register(\n    \"eds.normalizer\",\n    assigns=[\"token.norm\", \"token.tag\"],\n    deprecated=[\"normalizer\"],\n)\ndef create_component(\n    nlp: PipelineProtocol,\n    name: str = \"normalizer\",\n    *,\n    accents: Union[bool, Dict[str, Any]] = True,\n    lowercase: Union[bool, Dict[str, Any]] = True,\n    quotes: Union[bool, Dict[str, Any]] = True,\n    spaces: Union[bool, Dict[str, Any]] = True,\n    pollution: Union[bool, Dict[str, Any]] = True,\n) -&gt; Normalizer:\n\"\"\"\n    Normalisation pipeline. Modifies the `NORM` attribute,\n    acting on five dimensions :\n\n    - `lowercase`: using the default `NORM`\n    - `accents`: deterministic and fixed-length normalisation of accents.\n    - `quotes`: deterministic and fixed-length normalisation of quotation marks.\n    - `spaces`: \"removal\" of spaces tokens (via the tag_ attribute).\n    - `pollution`: \"removal\" of pollutions (via the tag_ attribute).\n\n    Parameters\n    ----------\n    nlp: PipelineProtocol\n        The pipeline object.\n    name : str\n        The component name.\n    lowercase : bool\n        Whether to remove case.\n    accents : Union[bool, Dict[str, Any]]\n        `Accents` configuration object\n    quotes : Union[bool, Dict[str, Any]]\n        `Quotes` configuration object\n    spaces : Union[bool, Dict[str, Any]]\n        `Spaces` configuration object\n    pollution : Union[bool, Dict[str, Any]]\n        Optional `Pollution` configuration object.\n    \"\"\"\n\n    if accents:\n        accents = AccentsConverter(\n            nlp=nlp,\n            name=\"eds.accents\",\n            **(accents if accents is not True else {}),\n        )\n\n    if quotes:\n        quotes = QuotesConverter(\n            nlp=nlp,\n            name=\"eds.quotes\",\n            **(quotes if quotes is not True else {}),\n        )\n\n    if spaces:\n        spaces = SpacesTagger(\n            nlp=nlp,\n            name=\"eds.spaces\",\n            **(spaces if spaces is not True else {}),\n        )\n\n    if pollution:\n        config = dict(default_enabled_pollution)\n        if isinstance(pollution, dict):\n            pollution = (\n                pollution if \"pollution\" not in pollution else pollution[\"pollution\"]\n            )\n            config.update(pollution)\n        pollution = PollutionTagger(\n            nlp=nlp,\n            name=\"eds.pollution\",\n            pollution=config,\n        )\n\n    normalizer = Normalizer(\n        nlp=nlp,\n        name=name,\n        lowercase=lowercase,\n        accents=accents or None,\n        quotes=quotes or None,\n        pollution=pollution or None,\n        spaces=spaces or None,\n    )\n\n    return normalizer\n</code></pre>"},{"location":"pipes/core/normalizer/#pipes","title":"Pipes","text":"<p>Let's review each subcomponent.</p>"},{"location":"pipes/core/normalizer/#lowercase","title":"Lowercase","text":"<p>The <code>eds.lowercase</code> pipeline component transforms every token to lowercase. It is not configurable.</p> <p>Consider the following example :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom edsnlp.matchers.utils import get_text\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.normalizer(\n        lowercase=True,\n        accents=False,\n        quotes=False,\n        spaces=False,\n        pollution=False,\n    ),\n)\n\ntext = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\"\n\ndoc = nlp(text)\n\nget_text(doc, attr=\"NORM\", ignore_excluded=False)\n# Out: pneumopathie \u00e0 nbnbwbwbnbwbnbnbnbwbw 'coronavirus'\n</code></pre>"},{"location":"pipes/core/normalizer/#accents","title":"Accents","text":"<p>The <code>eds.accents</code> pipeline component removes accents. To avoid edge cases, the component uses a specified list of accentuated characters and their unaccented representation, making it more predictable than using a library such as <code>unidecode</code>.</p> <p>Consider the following example :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom edsnlp.matchers.utils import get_text\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.normalizer(\n        lowercase=False,\n        accents=True,\n        quotes=False,\n        spaces=False,\n        pollution=False,\n    ),\n)\n\ntext = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\"\n\ndoc = nlp(text)\n\nget_text(doc, attr=\"NORM\", ignore_excluded=False)\n# Out: Pneumopathie a NBNbWbWbNbWbNBNbNbWbW `coronavirus'\n</code></pre>"},{"location":"pipes/core/normalizer/#apostrophes-and-quotation-marks","title":"Apostrophes and quotation marks","text":"<p>Apostrophes and quotation marks can be encoded using unpredictable special characters. The <code>eds.quotes</code> component transforms every such special character to <code>'</code> and <code>\"</code>, respectively.</p> <p>Consider the following example :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom edsnlp.matchers.utils import get_text\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.normalizer(\n        lowercase=False,\n        accents=False,\n        quotes=True,\n        spaces=False,\n        pollution=False,\n    ),\n)\n\ntext = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\"\n\ndoc = nlp(text)\n\nget_text(doc, attr=\"NORM\", ignore_excluded=False)\n# Out: Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW 'coronavirus'\n</code></pre>"},{"location":"pipes/core/normalizer/#spaces","title":"Spaces","text":"<p>This is not truly a normalisation component, but this allows us to detect spaces tokens ahead of the other components and encode it as using the <code>tag_</code> attribute for fast matching.</p> <p>Tip</p> <p>This component and its <code>spaces</code> option should be enabled if you ever set   <code>ignore_space_tokens</code> parameter token to True in a downstream component.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.normalizer(\n        lowercase=False,\n        accents=False,\n        quotes=False,\n        spaces=True,\n        pollution=False,\n    ),\n)\n\ndoc = nlp(\"Phrase    avec des espaces \\n et un retour \u00e0 la ligne\")\n[t.tag_ for t in doc]\n# Out: ['', 'SPACE', '', '', '', 'SPACE', '', '', '', '', '', '']\n</code></pre>"},{"location":"pipes/core/normalizer/#pollution","title":"Pollution","text":"<p>The pollution pipeline component uses a set of regular expressions to detect pollutions (irrelevant non-medical text that hinders text processing). Corresponding tokens are marked as excluded (by setting <code>Token._.excluded</code> to <code>True</code>), enabling the use of the phrase matcher.</p> <p>Consider the following example :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom edsnlp.matchers.utils import get_text\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.normalizer(\n        lowercase=False,\n        accents=True,\n        quotes=False,\n        spaces=False,\n        pollution=True,\n    ),\n)\n\ntext = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\"\n\ndoc = nlp(text)\n\nget_text(doc, attr=\"NORM\", ignore_excluded=False)\n# Out: Pneumopathie a NBNbWbWbNbWbNBNbNbWbW `coronavirus'\n\nget_text(doc, attr=\"TEXT\", ignore_excluded=True)\n# Out: Pneumopathie \u00e0 `coronavirus'\n</code></pre> <p>This example above shows that the normalisation scheme works on two axes: non-destructive text modification and exclusion of tokens. The two are independent: a matcher can use the <code>NORM</code> attribute but keep excluded tokens, and conversely, match on <code>TEXT</code> while ignoring excluded tokens.</p> <p></p>"},{"location":"pipes/core/normalizer/#types-of-pollution","title":"Types of pollution","text":"<p>Pollution can come in various forms in clinical texts. We provide a small set of possible pollutions patterns that can be enabled or disabled as needed.</p> <p>For instance, if we consider biology tables as pollution, we only need to instantiate the <code>normalizer</code> pipe as follows:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.normalizer(\n        pollution=dict(biology=True),\n    ),\n)\n</code></pre> Type Description Example Included by default <code>information</code> Footnote present in a lot of notes, providing information to the patient about the use of its data \"L'AP-HP collecte vos donn\u00e9es administratives \u00e0 des fins ...\" <code>True</code> <code>bars</code> Barcodes wrongly parsed as text \"...NBNbWbWbNbWbNBNbNbWbW...\" <code>True</code> <code>biology</code> Parsed biology results table. It often contains disease names that often leads to false positives with NER pipes. \"...\u00a6UI/L \u00a620 \u00a6 \u00a6 \u00a620-70 Polyarthrite rhumato\u00efde Facteur rhumatoide \u00a6UI/mL \u00a6 \u00a6&lt;10 \u00a6 \u00a6 \u00a6 \u00a60-14...\" <code>False</code> <code>doctors</code> List of doctor names and specialities, often found in left-side note margins. Also source of potential false positives. \"... Dr ABC - Diab\u00e8te/Endocrino ...\" <code>True</code> <code>web</code> Webpages URL and email adresses. Also source of potential false positives. \"... www.vascularites.fr ...\" <code>True</code> <code>coding</code> Subsection containing ICD-10 codes along with their description. Also source of potential false positives. \"... (2) E112 + Oeil (2) E113 + Neuro (2) E114 D\u00e9mence (2) F03 MA (2) F001+G301 DCL G22+G301 Vasc (2) ...\" <code>False</code> <code>footer</code> Footer of new page \"2/2Pat : NOM Prenom le 2020/01/01 IPP 12345678 Intitul\u00e9 RCP : Urologie HMN le \" <code>True</code>"},{"location":"pipes/core/normalizer/#custom-pollution","title":"Custom pollution","text":"<p>If you want to exclude specific patterns, you can provide them as a RegEx (or a list of Regexes). For instance, to consider text between \"AAA\" and \"ZZZ\" as pollution you might use:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.normalizer(\n        pollution=dict(custom_pollution=r\"AAA.*ZZZ\"),\n    ),\n)\n</code></pre>"},{"location":"pipes/core/normalizer/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.normalizer</code> pipeline component was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/core/sentences/","title":"Sentences","text":"<p>The <code>eds.sentences</code> matcher provides an alternative to spaCy's default <code>sentencizer</code>, aiming to overcome some of its limitations.</p> <p>Indeed, the <code>sentencizer</code> merely looks at period characters to detect the end of a sentence, a strategy that often fails in a clinical note settings. Our <code>eds.sentences</code> component also classifies end-of-lines as sentence boundaries if the subsequent token begins with an uppercase character, leading to slightly better performances.</p> <p>Moreover, the <code>eds.sentences</code> component use the output of the <code>eds.normalizer</code> and <code>eds.endlines</code> output by default when these components are added to the pipeline.</p>"},{"location":"pipes/core/sentences/#edsnlp.pipes.core.sentences.factory.create_component--examples","title":"Examples","text":"EDS-NLPspaCy sentencizer <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())  # same as nlp.add_pipe(\"eds.sentences\")\n\ntext = \"\"\"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\nIl lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\"\n\"\"\"\n\ndoc = nlp(text)\n\nfor sentence in doc.sents:\n    print(\"&lt;s&gt;\", sentence, \"&lt;/s&gt;\")\n# Out: &lt;s&gt; Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\n# Out:  &lt;\\s&gt;\n# Out: &lt;s&gt; Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. &lt;\\s&gt;\n</code></pre> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\"sentencizer\")\n\ntext = \"\"\"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\"\nIl lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\n\"\"\"\n\ndoc = nlp(text)\n\nfor sentence in doc.sents:\n    print(\"&lt;s&gt;\", sentence, \"&lt;/s&gt;\")\n# Out: &lt;s&gt; Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\n# Out: Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. &lt;\\s&gt;\n</code></pre> <p>Notice how EDS-NLP's implementation is more robust to ill-defined sentence endings.</p>"},{"location":"pipes/core/sentences/#edsnlp.pipes.core.sentences.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The EDS-NLP pipeline</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'sentences'</code> </p> <code>punct_chars</code> <p>Punctuation characters.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>use_endlines</code> <p>Whether to use endlines prediction.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>check_capitalized</code> <p>Whether to check for capitalized words after newlines or full stops.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>min_newline_count</code> <p>The minimum number of newlines to consider a newline-triggered sentence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p>"},{"location":"pipes/core/sentences/#edsnlp.pipes.core.sentences.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.sentences</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/core/terminology/","title":"Terminology","text":"<p>EDS-NLP simplifies the terminology matching process by exposing a <code>eds.terminology</code> pipeline that can match on terms or regular expressions.</p> <p>The terminology matcher is very similar to the generic matcher, although the use case differs slightly. The generic matcher is designed to extract any entity, while the terminology matcher is specifically tailored towards high volume terminologies.</p> <p>There are some key differences:</p> <ol> <li>It labels every matched entity to the same value, provided to the pipeline</li> <li>The keys provided in the <code>regex</code> and <code>terms</code> dictionaries are used as the    <code>kb_id_</code> of the entity, which handles fine-grained labelling</li> </ol> <p>For instance, a terminology matcher could detect every drug mention under the top-level label <code>drug</code>, and link each individual mention to a given drug through its <code>kb_id_</code> attribute.</p> <ol></ol>"},{"location":"pipes/core/terminology/#edsnlp.pipes.core.terminology.factory.create_component--examples","title":"Examples","text":"<p>Let us redefine the pipeline :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],  # (1)\n    flu=[\"grippe saisonni\u00e8re\"],  # (2)\n)\n\nregex = dict(\n    covid=r\"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\",  # (3)\n)\n\nnlp.add_pipe(\n    eds.terminology(\n        label=\"disease\",\n        terms=terms,\n        regex=regex,\n        attr=\"LOWER\",\n    ),\n)\n</code></pre> <ol> <li>Every key in the <code>terms</code> dictionary is mapped to a concept.</li> <li>The <code>eds.matcher</code> pipeline expects a list of expressions, or a single expression.</li> <li>We can also define regular expression patterns.</li> </ol> <p>This snippet is complete, and should run as is.</p>"},{"location":"pipes/core/terminology/#edsnlp.pipes.core.terminology.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>exact</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <p>Patterns, be they <code>terms</code> or <code>regex</code>, are defined as dictionaries where keys become the <code>kb_id_</code> of the extracted entities. Dictionary values are either a single expression or a list of expressions that match the concept (see example).</p>"},{"location":"pipes/core/terminology/#edsnlp.pipes.core.terminology.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.terminology</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/misc/","title":"Miscellaneous","text":"<p>This section regroups components that extract information that can be used by other components, but have little medical value in itself.</p> <p>For instance, the date detection and normalisation pipeline falls in this category.</p>"},{"location":"pipes/misc/#available-components","title":"Available components","text":"Component Description <code>eds.dates</code> Date extraction and normalisation <code>eds.consultation_dates</code> Identify consultation dates <code>eds.quantities</code> Quantity extraction and normalisation <code>eds.sections</code> Section detection <code>eds.reason</code> Rule-based hospitalisation reason detection <code>eds.tables</code> Tables detection <code>eds.split</code> Doc splitting <code>eds.explode</code> Explode entities between multiples copies of a document"},{"location":"pipes/misc/consultation-dates/","title":"Consultation dates","text":"<p>The <code>eds.consultation-dates</code> matcher consists of two main parts:</p> <ul> <li>A matcher which finds mentions of consultation events (more details below)</li> <li>A date parser (see the corresponding pipe) that links a date to those events</li> </ul>"},{"location":"pipes/misc/consultation-dates/#edsnlp.pipes.misc.consultation_dates.factory.create_component--examples","title":"Examples","text":"<p>Note</p> <p>The matcher has been built to run on consultation notes (<code>CR-CONS</code> at APHP), so please filter accordingly before proceeding.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        lowercase=True,\n        accents=True,\n        quotes=True,\n        pollution=False,\n    ),\n)\nnlp.add_pipe(eds.consultation_dates())\n\ntext = \"\"\"\nXXX\nObjet : Compte-Rendu de Consultation du 03/10/2018.\nXXX\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.spans[\"consultation_dates\"]\n# Out: [Consultation du 03/10/2018]\n\ndoc.spans[\"consultation_dates\"][0]._.consultation_date.to_datetime()\n# Out: 2018-10-03 00:00:00\n</code></pre>"},{"location":"pipes/misc/consultation-dates/#edsnlp.pipes.misc.consultation_dates.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.consultation_dates</code> pipeline declares one extension on the <code>Span</code> object: the <code>consultation_date</code> attribute, which is a Python <code>datetime</code> object.</p>"},{"location":"pipes/misc/consultation-dates/#edsnlp.pipes.misc.consultation_dates.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Language pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>consultation_mention</code> <p>List of RegEx for consultation mentions.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains terms directly referring to consultations, such as \"Consultation du...\" or \"Compte rendu du...\". This list is the only one enabled by default since it is fairly precise and not error-prone.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>True</code> </p> <code>town_mention</code> <p>List of RegEx for all AP-HP hospitals' towns mentions.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains the towns of each AP-HP's hospital. Its goal is to fetch dates mentioned as \"Paris, le 13 d\u00e9cembre 2015\". It has a high recall but poor precision, since those dates can often be dates of letter redaction instead of consultation dates.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>False</code> </p> <code>document_date_mention</code> <p>List of RegEx for document date.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains expressions mentioning the date of creation/edition of a document, such as \"Date du rapport: 13/12/2015\" or \"Sign\u00e9 le 13/12/2015\". Like <code>town_mention</code> patterns, it has a high recall but is prone to errors since document date and consultation date aren't necessary similar.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>False</code> </p>"},{"location":"pipes/misc/consultation-dates/#edsnlp.pipes.misc.consultation_dates.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.consultation_dates</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/misc/dates/","title":"Dates","text":"<p>The <code>eds.dates</code> matcher detects and normalize dates within a medical document. We use simple regular expressions to extract date mentions.</p>"},{"location":"pipes/misc/dates/#edsnlp.pipes.misc.dates.factory.create_component--scope","title":"Scope","text":"<p>The <code>eds.dates</code> pipeline finds absolute (eg <code>23/08/2021</code>) and relative (eg <code>hier</code>, <code>la semaine derni\u00e8re</code>) dates alike. It also handles mentions of duration.</p> Type Example <code>absolute</code> <code>3 mai</code>, <code>03/05/2020</code> <code>relative</code> <code>hier</code>, <code>la semaine derni\u00e8re</code> <code>duration</code> <code>pendant quatre jours</code> <p>See the tutorial for a presentation of a full pipeline featuring the <code>eds.dates</code> component.</p>"},{"location":"pipes/misc/dates/#edsnlp.pipes.misc.dates.factory.create_component--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\nimport datetime\nimport pytz\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.dates())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac. \"\n    \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a un an pendant une semaine. \"\n    \"Il a \u00e9t\u00e9 diagnostiqu\u00e9 en mai 1995.\"\n)\n\ndoc = nlp(text)\n\ndates = doc.spans[\"dates\"]\ndates\n# Out: [23 ao\u00fbt 2021, il y a un an, mai 1995]\n\ndates[0]._.date.to_datetime()\n# Out: 2021-08-23 00:00:00\n\ndates[1]._.date.to_datetime()\n# Out: None\n\nnote_datetime = datetime.datetime(2021, 8, 27, tzinfo=pytz.timezone(\"Europe/Paris\"))\ndoc._.note_datetime = note_datetime\n\ndates[1]._.date.to_datetime()\n# Out: 2020-08-27 00:00:00+00:09\n\ndate_2_output = dates[2]._.date.to_datetime(\n    note_datetime=note_datetime,\n    infer_from_context=True,\n    tz=\"Europe/Paris\",\n    default_day=15,\n)\ndate_2_output\n# Out: 1995-05-15 00:00:00+02:00\n\ndoc.spans[\"durations\"]\n# Out: [pendant une semaine]\n</code></pre> <p>Example on a collection of documents stored in the OMOP schema :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\n# with cols \"note_id\", \"note_text\" and optionally \"note_datetime\"\nmy_omop_df = ...\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.dates(as_ents=True))\ndocs = edsnlp.data.from_pandas(my_omop_df)\ndocs = docs.map_pipeline(nlp)\ndocs = docs.to_pandas(\n    converter=\"ents\",\n    span_attributes=[\"date.datetime\"],\n)\nprint(docs)\n# note_id  start  end label lexical_variant span_type datetime\n# ...\n</code></pre>"},{"location":"pipes/misc/dates/#edsnlp.pipes.misc.dates.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.dates</code> pipeline declares two extensions on the <code>Span</code> object:</p> <ul> <li>the <code>span._.date</code> attribute of a date contains a parsed version of the date.</li> <li>the <code>span._.duration</code> attribute of a duration contains a parsed version of the   duration.</li> </ul> <p>As with other components, you can use the <code>span._.value</code> attribute to get either the parsed date or the duration depending on the span.</p>"},{"location":"pipes/misc/dates/#edsnlp.pipes.misc.dates.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the pipeline component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'dates'</code> </p> <code>absolute</code> <p>List of regular expressions for absolute dates.</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>relative</code> <p>List of regular expressions for relative dates (eg <code>hier</code>, <code>la semaine prochaine</code>).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>duration</code> <p>List of regular expressions for durations (eg <code>pendant trois mois</code>).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>false_positive</code> <p>List of regular expressions for false positive (eg phone numbers, etc).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Where to look for dates in the doc. By default, look in the whole doc. You can combine this with the <code>merge_mode</code> argument for interesting results.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>merge_mode</code> <p>How to merge matched dates with the spans from <code>span_getter</code>, if given:</p> <ul> <li><code>intersect</code>: return only the matches that fall in the <code>span_getter</code> spans</li> <li><code>align</code>: if a date overlaps a span from <code>span_getter</code> (e.g. a date extracted   by a machine learning model), return the <code>span_getter</code> span instead, and   assign all the parsed information (<code>._.date</code> / <code>._.duration</code>) to it. Otherwise   don't return the date.</li> </ul> <p> TYPE: <code>Literal['intersect', 'align']</code> DEFAULT: <code>intersect</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> and <code>merge_mode</code> instead. Whether to look on dates in the whole document or in specific sentences:</p> <ul> <li>If <code>True</code>: Only look in the sentences of each entity in doc.ents</li> <li>If False: Look in the whole document</li> <li>If given a string <code>key</code> or list of string: Only look in the sentences of   each entity in <code>doc.spans[key]</code></li> </ul> <p> TYPE: <code>Union[bool, str, Iterable[str]]</code> DEFAULT: <code>None</code> </p> <code>detect_periods</code> <p>Whether to detect periods (experimental)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>detect_time</code> <p>Whether to detect time inside dates</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>period_proximity_threshold</code> <p>Max number of words between two dates to extract a period.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>as_ents</code> <p>Deprecated, use span_setter instead. Whether to treat dates as entities</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>spaCy attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>LOWER</code> </p> <code>date_label</code> <p>Label to use for dates</p> <p> TYPE: <code>str</code> DEFAULT: <code>date</code> </p> <code>duration_label</code> <p>Label to use for durations</p> <p> TYPE: <code>str</code> DEFAULT: <code>duration</code> </p> <code>period_label</code> <p>Label to use for periods</p> <p> TYPE: <code>str</code> DEFAULT: <code>period</code> </p> <code>span_setter</code> <p>How to set matches in the doc.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'dates': ['date'], 'durations': ['duration'], ...</code> </p> <code>explain</code> <p>Whether to keep track of regex cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"pipes/misc/dates/#edsnlp.pipes.misc.dates.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.dates</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/misc/explode/","title":"Explode","text":"<p>Explode a Doc into multiple distinct Doc objects, one per span retrieved through the <code>span_getter</code> : each span becomes alone in its own Doc. Note that entities that are not selected by the <code>span_getter</code> will be lost in the new docs.</p> <p>Not for pipelines</p> <p>This component is not meant to be used in a pipeline, but rather as a preprocessing step when dealing with a stream of documents as in the example below.</p> <p>Difference with <code>eds.split</code></p> <p>While <code>eds.split</code> breaks a document into smaller chunks based on length or regex rules, <code>eds.explode</code> creates a separate document for each selected span. This means <code>eds.split</code> is typically used for segmenting text for context size or processing constraints, whereas <code>eds.explode</code> is designed for span-level tasks that require span-level mixing, like training span classifiers, ensuring that each span is isolated in its own document while preserving the original context.</p>"},{"location":"pipes/misc/explode/#edsnlp.pipes.misc.explode.explode.Explode--examples","title":"Examples","text":"<pre><code>import edsnlp.pipes as eds\nfrom edsnlp.data.converters import MarkupToDocConverter\n\nconverter = MarkupToDocConverter(\n    preset=\"xml\",\n    # Put xml annotated spans in distinct doc.spans[label] groups\n    span_setter={\"*\": True},\n)\ndoc = converter(\n    \"Le &lt;person&gt;patient&lt;/person&gt; a mal au &lt;body_part&gt;bras&lt;/body_part&gt;, \u00e0 la \"\n    \"&lt;body_part&gt;jambe&lt;/body_part&gt; et au &lt;body_part&gt;torse&lt;/body_part&gt;\"\n)\n\nexploder = eds.explode(span_getter=[\"body_part\"])\nprint(doc.text, \"-&gt;\", doc.spans)\n# Out: Le patient a mal au bras, \u00e0 la jambe et au torse -&gt; {'person': [patient], 'body_part': [bras, jambe, torse]}\n\nfor new_doc in exploder(doc):\n    print(new_doc.text, \"-&gt;\", new_doc.spans)\n# Out: Le patient a mal au bras, \u00e0 la jambe et au torse -&gt; {'person': [], 'body_part': [bras]}\n# Out: Le patient a mal au bras, \u00e0 la jambe et au torse -&gt; {'person': [], 'body_part': [jambe]}\n# Out: Le patient a mal au bras, \u00e0 la jambe et au torse -&gt; {'person': [], 'body_part': [torse]}\n</code></pre>"},{"location":"pipes/misc/explode/#edsnlp.pipes.misc.explode.explode.Explode--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use to retrieve spans from the Doc. Default is <code>{\"ents\": True}</code> which retrieves all entities in <code>doc.ents</code>.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>filter_expr</code> <p>An optional filter expression to filter the produced documents. The callable expects a single argument, the new Doc, and should return a boolean.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p>"},{"location":"pipes/misc/quantities/","title":"Quantities","text":"<p>The <code>eds.quantities</code> matcher detects and normalizes numerical quantities within a medical document.</p> <p>Warning</p> <p>The <code>quantities</code> pipeline is still in active development and has not been rigorously validated. If you come across a quantity expression that goes undetected, please file an issue !</p>"},{"location":"pipes/misc/quantities/#edsnlp.pipes.misc.quantities.factory.create_component--pipe-definition","title":"Pipe definition","text":"<pre><code>text = \"\"\"Poids : 65. Taille : 1.75\n          On mesure ... \u00e0 3mmol/l ; pression : 100mPa-110mPa.\n          Acte r\u00e9alis\u00e9 par ... \u00e0 12h13\"\"\"\n</code></pre> All quantitiesCustom quantitiesPredefined quantities <pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.tables\")\nnlp.add_pipe(\n    \"eds.quantities\",\n    config=dict(\n        quantities=\"all\", extract_ranges=True, use_tables=True  # (3)  # (1)\n    ),  # (2)\n)\nnlp(text).spans[\"quantities\"]\n# Out: [65, 1.75, 3mmol/l, 100mPa-110mPa, 12h13]\n</code></pre> <ol> <li>100-110mg, 2 \u00e0 4 jours ...</li> <li>If True <code>eds.tables</code> must be called</li> <li>All units from Availability will be detected</li> </ol> <pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.tables\")\nnlp.add_pipe(\n    \"eds.quantities\",\n    config=dict(\n        quantities={\n            \"concentration\": {\"unit\": \"mol_per_l\"},\n            \"pressure\": {\"unit\": \"Pa\"},\n        },  # (3)\n        extract_ranges=True,  # (1)\n        use_tables=True,\n    ),  # (2)\n)\nnlp(text).spans[\"quantities\"]\n# Out: [3mmol/l, 100mPa-110mPa]\n</code></pre> <ol> <li>100-110mg, 2 \u00e0 4 jours ...</li> <li>If True <code>eds.tables</code> must be called</li> <li>Which units are available ? See Availability.    More on customization ? See Customization</li> </ol> <pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.tables\")\nnlp.add_pipe(\n    \"eds.quantities\",\n    config=dict(\n        quantities=[\"weight\", \"size\"],  # (3)\n        extract_ranges=True,  # (1)\n        use_tables=True,\n    ),  # (2)\n)\nnlp(text).spans[\"quantities\"]\n# Out: [65, 1.75]\n</code></pre> <ol> <li>100-110mg, 2 \u00e0 4 jours ...</li> <li>If True <code>eds.tables</code> must be called</li> <li>Which quantities are available ? See Availability</li> </ol>"},{"location":"pipes/misc/quantities/#edsnlp.pipes.misc.quantities.factory.create_component--scope","title":"Scope","text":"<p>The <code>eds.quantities</code> matcher can extract simple (e.g. <code>3cm</code>) quantities. It can also detect elliptic enumerations (eg <code>32, 33 et 34kg</code>) of quantities of the same type and split the quantities accordingly.</p> <p>The normalized value can then be accessed via the <code>span._.{measure_name}</code> attribute, for instance <code>span._.size</code> or <code>span._.weight</code> and be converted on the fly to a desired unit. Like for other components, the <code>span._.value</code> extension can also be used to access the normalized value for any quantity span.</p> <p>See Availability section for details on which units are handled</p>"},{"location":"pipes/misc/quantities/#edsnlp.pipes.misc.quantities.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.quantities(\n        quantities=[\"size\", \"weight\", \"bmi\"],\n        extract_ranges=True,\n    ),\n)\n\ntext = \"\"\"\nLe patient est admis hier, fait 1m78 pour 76kg.\nLes deux nodules b\u00e9nins sont larges de 1,2 et 2.4mm.\nBMI: 24.\n\nLe nodule fait entre 1 et 1.5 cm\n\"\"\"\n\ndoc = nlp(text)\n\nquantities = doc.spans[\"quantities\"]\n\nquantities\n# Out: [1m78, 76kg, 1,2, 2.4mm, 24, entre 1 et 1.5 cm]\n\nquantities[0]\n# Out: 1m78\n\nstr(quantities[0]._.size), str(quantities[0]._.value)\n# Out: ('1.78 m', '1.78 m')\n\nquantities[0]._.value.cm\n# Out: 178.0\n\nquantities[2]\n# Out: 1,2\n\nstr(quantities[2]._.value)\n# Out: '1.2 mm'\n\nstr(quantities[2]._.value.mm)\n# Out: 1.2\n\nquantities[4]\n# Out: 24\n\nstr(quantities[4]._.value)\n# Out: '24 kg_per_m2'\n\nstr(quantities[4]._.value.kg_per_m2)\n# Out: 24\n\nstr(quantities[5]._.value)\n# Out: 1-1.5 cm\n</code></pre> <p>To extract all sizes in centimeters, and average range quantities, you can use the following snippet:</p> <pre><code>sizes = [\n    sum(item.cm for item in m._.value) / len(m._.value)\n    for m in doc.spans[\"quantities\"]\n    if m.label_ == \"size\"\n]\nsizes\n# Out: [178.0, 0.12, 0.24, 1.25]\n</code></pre> <p>To extract the quantities from many texts, you can use the following snippet:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.quantities(quantities=\"weight\", extract_ranges=True, as_ents=True),\n)\ntexts = [\"Le patient mesure 40000,0 g (aussi not\u00e9 40 kg)\"]\ndocs = edsnlp.data.from_iterable(texts)\ndocs = docs.map_pipeline(nlp)\ndocs.to_pandas(\n    converter=\"ents\",\n    span_attributes=[\"value.unit\", \"value.kg\"],\n)\n#   note_id  start  end   label lexical_variant span_type original_unit    kg\n# 0    None     18   27  weight       40000,0 g      ents             g  40.0\n# 1    None     40   45  weight           40 kg      ents            kg  40.0\n</code></pre>"},{"location":"pipes/misc/quantities/#edsnlp.pipes.misc.quantities.factory.create_component--available-units-and-quantities","title":"Available units and quantities","text":"<p>Feel free to propose any missing raw unit or predefined quantity.</p> <p>Raw units and their derivations (g, mg, mgr ...) and their compositions (g/ml, cac/j ...) can be detected.</p> <p>Available raw units :</p> <p><code>g, m, m2, m3, mol, ui, Pa, %, log, mmHg, s/min/h/d/w/m/y, arc-second, \u00b0, \u00b0C, cac, goutte, l, x10*4, x10*5</code></p> <p>Available predefined quantities :</p> quantity_name Example <code>size</code> <code>1m50</code>, <code>1.50m</code>... <code>weight</code> <code>1kg</code>, <code>Poids : 65</code>... <code>bmi</code> <code>BMI: 24</code>, <code>24 kg.m-2</code> <code>volume</code> <code>2 cac</code>, <code>8ml</code>... <p>See the patterns for exhaustive definition.</p>"},{"location":"pipes/misc/quantities/#edsnlp.pipes.misc.quantities.factory.create_component--customization","title":"Customization","text":"<p>You can declare custom quantities by altering the patterns:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.quantities(\n        quantities={\n            \"my_custom_surface_quantity\": {\n                # This quantity unit is homogenous to square meters\n                \"unit\": \"m2\",\n                # Handle cases like \"surface: 1.8\" (implied m2),\n                # vs \"surface: 50\" (implied cm2)\n                \"unitless_patterns\": [\n                    {\n                        \"terms\": [\"surface\", \"aire\"],\n                        \"ranges\": [\n                            {\"unit\": \"m2\", \"min\": 0, \"max\": 9},\n                            {\"unit\": \"cm2\", \"min\": 10, \"max\": 100},\n                        ],\n                    }\n                ],\n            },\n        }\n    ),\n)\n</code></pre>"},{"location":"pipes/misc/quantities/#edsnlp.pipes.misc.quantities.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.quantities</code> pipeline declares its extensions dynamically, depending on the <code>quantities</code> parameter: each quantity gets its own extension, and is assigned to a different span group.</p>"},{"location":"pipes/misc/quantities/#edsnlp.pipes.misc.quantities.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'quantities'</code> </p> <code>quantities</code> <p>A mapping from measure names to MsrConfig Each measure's configuration has the following shape: <pre><code>{\n  # the unit (e.g. \"kg\"),\n  \"unit\": str,\n  \"unitless_patterns\": {\n    # preceding trigger terms\n    \"terms\": List[str],\n    # unitless ranges -&gt; unit patterns\n    \"ranges\": List[\n      {\"min\": int, \"max\": int, \"unit\": str},\n      {\"min\": int, \"unit\": str},\n      ...,\n    ],\n    ...\n  }\n}\n</code></pre> Set <code>quantities=\"all\"</code> to extract all raw quantities from units_config file.</p> <p> TYPE: <code>Union[str, List[Union[str, MsrConfig]], Dict[str, MsrConfig]]</code> DEFAULT: <code>['weight', 'size', 'bmi', 'volume']</code> </p> <code>number_terms</code> <p>A mapping of numbers to their lexical variants</p> <p> TYPE: <code>Dict[str, List[str]]</code> DEFAULT: <code>{'0.125': ['\u215b'], '0.16666666': ['\u2159'], '0.2': ['...</code> </p> <code>stopwords</code> <p>A list of stopwords that do not matter when placed between a unitless trigger and a number</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>['par', 'sur', 'de', 'a', ',', 'et', '-', '\u00e0']</code> </p> <code>unit_divisors</code> <p>A list of terms used to divide two units (like: m / s)</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>['/', 'par']</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>ignore_excluded</code> <p>Whether to exclude pollution patterns when matching in the text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>compose_units</code> <p>Whether to compose units (like \"m/s\" or \"m.s-1\")</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>extract_ranges</code> <p>Whether to extract ranges (like \"entre 1 et 2 cm\")</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>range_patterns</code> <p>A list of \"{FROM} xx {TO} yy\" patterns to match range quantities</p> <p> TYPE: <code>List[Tuple[Optional[str], Optional[str]]]</code> DEFAULT: <code>[('De', '\u00e0'), ('De', 'a'), ('de', '\u00e0'), ('de', ...</code> </p> <code>after_snippet_limit</code> <p>Maximum word distance after to link a part of a quantity after its number</p> <p> TYPE: <code>int</code> DEFAULT: <code>6</code> </p> <code>before_snippet_limit</code> <p>Maximum word distance after to link a part of a quantity before its number</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>span_setter</code> <p>How to set the spans in the document. By default, each quantity will be assigned to its own span group (using either the \"name\" field of the config, or the key if you passed a dict), and to the \"quantities\" group.</p> <p> TYPE: <code>Optional[SpanSetterArg]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Where to look for quantities in the doc. By default, look in the whole doc. You can combine this with the <code>merge_mode</code> argument for interesting results.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>merge_mode</code> <p>How to merge matches with the spans from <code>span_getter</code>, if given:</p> <ul> <li><code>intersect</code>: return only the matches that fall in the <code>span_getter</code> spans</li> <li><code>align</code>: if a match overlaps a span from <code>span_getter</code> (e.g. a match   extracted by a machine learning model), return the <code>span_getter</code> span   instead, and assign all the parsed information (<code>._.date</code> / <code>._.duration</code>)   to it. Otherwise, don't return the date.</li> </ul> <p> TYPE: <code>Literal['intersect', 'align']</code> DEFAULT: <code>intersect</code> </p>"},{"location":"pipes/misc/quantities/#edsnlp.pipes.misc.quantities.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.quantities</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/misc/reason/","title":"Reasons","text":"<p>The <code>eds.reason</code> matcher uses a rule-based algorithm to detect spans that relate to the reason of the hospitalisation. It was designed at AP-HP's EDS.</p>"},{"location":"pipes/misc/reason/#edsnlp.pipes.misc.reason.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and looks for spans of hospitalisation reasons. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018\nMOTIF D'HOSPITALISATION\nMonsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978,\na \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nANT\u00c9C\u00c9DENTS\nAnt\u00e9c\u00e9dents m\u00e9dicaux :\nPremier \u00e9pisode d'asthme en mai 2018.\"\"\"\n\nnlp = edsnlp.blank(\"eds\")\n\n# Extraction of entities\nnlp.add_pipe(\n    eds.matcher(\n        terms=dict(\n            respiratoire=[\n                \"asthmatique\",\n                \"asthme\",\n                \"toux\",\n            ]\n        )\n    ),\n)\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.reason(use_sections=True))\ndoc = nlp(text)\n\nreason = doc.spans[\"reasons\"][0]\nreason\n# Out: hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nreason._.is_reason\n# Out: True\n\nentities = reason._.ents_reason\nentities\n# Out: [asthme]\n\nentities[0].label_\n# Out: 'respiratoire'\n\nent = entities[0]\nent._.is_reason\n# Out: True\n</code></pre>"},{"location":"pipes/misc/reason/#edsnlp.pipes.misc.reason.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.reason</code> pipeline adds the key <code>reasons</code> to <code>doc.spans</code> and declares one extension, on the <code>Span</code> objects called <code>ents_reason</code>.</p> <p>The <code>ents_reason</code> extension is a list of named entities that overlap the <code>Span</code>, typically entities found in upstream components like <code>matcher</code>.</p> <p>It also declares the boolean extension <code>is_reason</code>. This extension is set to True for the Reason Spans but also for the entities that overlap the reason span.</p>"},{"location":"pipes/misc/reason/#edsnlp.pipes.misc.reason.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> </p> <code>reasons</code> <p>Reason patterns</p> <p> TYPE: <code>Dict[str, Union[List[str], str]]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>Default token attribute to use to build the text to match on.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>use_sections</code> <p>Whether or not use the <code>sections</code> matcher to improve results.</p> <p> TYPE: <code>(bool)</code> DEFAULT: <code>False</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"pipes/misc/reason/#edsnlp.pipes.misc.reason.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.reason</code> matcher was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/misc/sections/","title":"Sections","text":"<p>The <code>eds.sections</code> component extracts section titles from clinical documents. A \"section\" is then defined as the span of text between two titles.</p> <p>Here is the list of sections that are currently targeted :</p> <ul> <li><code>allergies</code></li> <li><code>ant\u00e9c\u00e9dents</code></li> <li><code>ant\u00e9c\u00e9dents familiaux</code></li> <li><code>traitements entr\u00e9e</code></li> <li><code>conclusion</code></li> <li><code>conclusion entr\u00e9e</code></li> <li><code>habitus</code></li> <li><code>correspondants</code></li> <li><code>diagnostic</code></li> <li><code>donn\u00e9es biom\u00e9triques entr\u00e9e</code></li> <li><code>examens</code></li> <li><code>examens compl\u00e9mentaires</code></li> <li><code>facteurs de risques</code></li> <li><code>histoire de la maladie</code></li> <li><code>actes</code></li> <li><code>motif</code></li> <li><code>prescriptions</code></li> <li><code>traitements sortie</code></li> <li><code>evolution</code></li> <li><code>modalites sortie</code></li> <li><code>vaccinations</code></li> <li><code>introduction</code></li> </ul> <p>Remarks :</p> <ul> <li>section <code>introduction</code> corresponds to the span of text between the header   \"COMPTE RENDU D'HOSPITALISATION\" (usually denoting the beginning of the document)   and the title of the following detected section</li> <li>this matcher works well for hospitalization summaries (CRH), but not necessarily   for all types of documents (in particular for emergency or scan summaries   CR-IMAGERIE)</li> </ul> <p>Experimental</p> <p>Should you rely on <code>eds.sections</code> for critical downstream tasks, make sure to validate the results to make sure that the component works in your case.</p>"},{"location":"pipes/misc/sections/#edsnlp.pipes.misc.sections.factory.create_component--examples","title":"Examples","text":"<p>The following snippet detects section titles. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sections())\n\ntext = \"\"\"\nCRU du 10/09/2021\nMotif :\nPatient admis pour suspicion de COVID\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.spans[\"section_titles\"]\n# Out: [Motif]\n</code></pre>"},{"location":"pipes/misc/sections/#edsnlp.pipes.misc.sections.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.sections</code> matcher adds two fields to the <code>doc.spans</code> attribute :</p> <ol> <li>The <code>section_titles</code> key contains the list of all section titles extracted using    the list declared in the <code>terms.py</code> module.</li> <li>The <code>sections</code> key contains a list of sections, ie spans of text between two    section titles (or the last title and the end of the document).</li> </ol> <p>If the document has entities before calling this matcher an attribute <code>section</code> is added to each entity.</p>"},{"location":"pipes/misc/sections/#edsnlp.pipes.misc.sections.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>sections</code> <p>Dictionary of terms to look for.</p> <p> TYPE: <code>Dict[str, List[str]]</code> DEFAULT: <code>{'allergies': ['allergies'], 'ant\u00e9c\u00e9dents': ['a...</code> </p> <code>attr</code> <p>Default attribute to match on.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>add_patterns</code> <p>Whether add update patterns to match start / end of lines</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"pipes/misc/sections/#edsnlp.pipes.misc.sections.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.sections</code> matcher was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/misc/split/","title":"Split","text":"<p>The <code>eds.split</code> component splits a document into multiple documents based on a regex pattern or a maximum length.</p> <p>Not for pipelines</p> <p>This component is not meant to be used in a pipeline, but rather as a preprocessing step when dealing with a stream of documents as in the example below.</p>"},{"location":"pipes/misc/split/#edsnlp.pipes.misc.split.split.Split--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\n# Create the stream\nstream = edsnlp.data.from_iterable(\n    [\"Sentence 1\\n\\nThis is another longer sentence more than 5 words\"]\n)\n\n# Convert texts into docs\nstream = stream.map_pipeline(edsnlp.blank(\"eds\"))\n\n# Apply the split component\nstream = stream.map(eds.split(max_length=5, regex=\"\\n{2,}\"))\n\nprint(\" | \".join(doc.text.strip() for doc in stream))\n# Out: Sentence 1 | This is another longer sentence | more than 5 words\n</code></pre>"},{"location":"pipes/misc/split/#edsnlp.pipes.misc.split.split.Split--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>max_length</code> <p>The maximum length of the produced documents. If 0, the document will not be split based on length.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>regex</code> <p>The regex pattern to split the document on</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'\\n{2,}'</code> </p> <code>filter_expr</code> <p>An optional filter expression to filter the produced documents. The callable expects a single <code>doc</code> argument, the new Doc, and should return a boolean. For example, to filter out documents with less than 5 tokens, you can use <code>filter_expr=\"len(doc) &gt;= 5\"</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>randomize</code> <p>The randomization factor to split the documents, to avoid producing documents that are all <code>max_length</code> tokens long (0 means all documents will have the maximum possible length while 1 will produce documents with a length varying between 0 and <code>max_length</code> uniformly)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p>"},{"location":"pipes/misc/tables/","title":"Tables","text":"<p>The <code>eds.tables</code> matcher detects tables in a documents.</p>"},{"location":"pipes/misc/tables/#edsnlp.pipes.misc.tables.factory.create_component--examples","title":"Examples","text":"<p><pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.tables())\n\ntext = \"\"\"\nSERVICE\nMEDECINE INTENSIVE \u2013\nREANIMATION\nR\u00e9animation / Surveillance Continue\nM\u00e9dicale\n\nCOMPTE RENDU D'HOSPITALISATION du 05/06/2020 au 10/06/2020\nMadame DUPONT Marie, n\u00e9e le 16/05/1900, \u00e2g\u00e9e de 20 ans, a \u00e9t\u00e9 hospitalis\u00e9e en\nr\u00e9animation du 05/06/1920 au 10/06/1920 pour intoxication m\u00e9dicamenteuse volontaire.\n\nExamens compl\u00e9mentaires\nH\u00e9matologie\nNum\u00e9ration\nLeucocytes \u00a6x10*9/L \u00a64.97 \u00a64.09-11\nH\u00e9maties \u00a6x10*12/L\u00a64.68 \u00a64.53-5.79\nH\u00e9moglobine \u00a6g/dL \u00a614.8 \u00a613.4-16.7\nH\u00e9matocrite \u00a6% \u00a644.2 \u00a639.2-48.6\nVGM \u00a6fL \u00a694.4 + \u00a679.6-94\nTCMH \u00a6pg \u00a631.6 \u00a627.3-32.8\nCCMH \u00a6g/dL \u00a633.5 \u00a632.4-36.3\nPlaquettes \u00a6x10*9/L \u00a6191 \u00a6172-398\nVMP \u00a6fL \u00a611.5 + \u00a67.4-10.8\n\nSur le plan neurologique : Devant la persistance d'une confusion \u00e0 distance de\nl'intoxication au\n...\n\n2/2Pat : &lt;NOM&gt; &lt;Prenom&gt;|F |&lt;date&gt; | &lt;ipp&gt; |Intitul\u00e9 RCP\n\"\"\"\n\ndoc = nlp(text)\n\n# A table span\ntable = doc.spans[\"tables\"][0]\n\n# Leucocytes \u00a6x10*9/L \u00a64.97 \u00a64.09-11\n# H\u00e9maties \u00a6x10*12/L\u00a64.68 \u00a64.53-5.79\n# H\u00e9moglobine \u00a6g/dL \u00a614.8 \u00a613.4-16.7\n# H\u00e9matocrite \u00a6% \u00a644.2 \u00a639.2-48.6\n# VGM \u00a6fL \u00a694.4 + \u00a679.6-94\n# TCMH \u00a6pg \u00a631.6 \u00a627.3-32.8\n# CCMH \u00a6g/dL \u00a633.5 \u00a632.4-36.3\n# Plaquettes \u00a6x10*9/L \u00a6191 \u00a6172-398\n# VMP \u00a6fL \u00a611.5 + \u00a67.4-10.8\n\n# Convert span to Pandas table\ndf = table._.to_pd_table(\n    as_spans=False,  # set True to set the table cells as spans instead of strings\n    header=False,  # set True to use the first row as header\n    index=False,  # set True to use the first column as index\n)\ntype(df)\n# Out: &lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> The pandas DataFrame:</p> 0 1 2 3 0 Leucocytes x10*9/L 4.97 4.09-11 1 H\u00e9maties x10*12/L 4.68 4.53-5.79 2 H\u00e9moglobine g/dL 14.8 13.4-16.7 3 H\u00e9matocrite % 44.2 39.2-48.6 4 VGM fL 94.4 + 79.6-94 5 TCMH pg 31.6 27.3-32.8 6 CCMH g/dL 33.5 32.4-36.3 7 Plaquettes x10*9/L 191 172-398 8 VMP fL 11.5 + 7.4-10.8"},{"location":"pipes/misc/tables/#edsnlp.pipes.misc.tables.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.tables</code> pipeline declares the <code>span._.to_pd_table()</code> Span extension. This function returns a parsed pandas version of the table.</p>"},{"location":"pipes/misc/tables/#edsnlp.pipes.misc.tables.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the component.</p> <p> </p> <code>tables_pattern</code> <p>The regex pattern to identify tables. The key of dictionary should be <code>tables</code></p> <p> TYPE: <code>Optional[Dict[str, str]]</code> DEFAULT: <code>None</code> </p> <code>sep_pattern</code> <p>The regex pattern to identify the separator pattern. Used when calling <code>to_pd_table</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>min_rows</code> <p>Only tables with more then <code>min_rows</code> lines will be detected.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>2</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'. We can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"pipes/misc/tables/#edsnlp.pipes.misc.tables.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.tables</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/ner/","title":"Named Entity Recognition Components","text":"<p>We provide several Named Entity Recognition (NER) components. Named Entity Recognition is the task of identifying short relevant spans of text, named entities, and classifying them into pre-defined categories. In the case of clinical documents, these entities can be scores, disorders, behaviors, codes, dates, quantities, etc.</p>"},{"location":"pipes/ner/#edsnlp.pipes.base.SpanSetterArg","title":"Span setters: where are stored extracted entities ?","text":"<p>A component assigns entities to a document by adding them to the <code>doc.ents</code> or <code>doc.spans[group]</code> attributes. <code>doc.ents</code> only supports non overlapping entities, therefore, if two entities overlap, the longest one will be kept. <code>doc.spans[group]</code> on the other hand, can contain overlapping entities. To control where entities are added, you can use the <code>span_setter</code> argument in any of these component.</p> <p>Valid values for the <code>span_setter</code> argument of a component can be :</p> <ul> <li>a (doc, matches) -&gt; None callable</li> <li>a span group name</li> <li>a list of span group names</li> <li>a dict of group name to True or list of labels</li> </ul> <p>The group name <code>\"ents\"</code> is a special case, and will add the matches to <code>doc.ents</code></p>"},{"location":"pipes/ner/#edsnlp.pipes.base.SpanSetterArg--examples","title":"Examples","text":"<ul> <li><code>span_setter=[\"ents\", \"ckd\"]</code> will add the matches to both <code>doc.ents</code> and <code>doc.spans[\"ckd\"]</code>. It is equivalent to <code>{\"ents\": True, \"ckd\": True}</code>.</li> <li><code>span_setter={\"ents\": [\"foo\", \"bar\"]}</code> will add the matches with label \"foo\" and \"bar\" to <code>doc.ents</code>.</li> <li><code>span_setter=\"ents\"</code> will add all matches only to <code>doc.ents</code>.</li> <li><code>span_setter=\"ckd\"</code> will add all matches only to <code>doc.spans[\"ckd\"]</code>.</li> </ul>"},{"location":"pipes/ner/#available-components","title":"Available components","text":"Component Description <code>eds.covid</code> A COVID mentions detector <code>eds.charlson</code> A Charlson score extractor <code>eds.sofa</code> A SOFA score extractor <code>eds.elston_ellis</code> An Elston &amp; Ellis code extractor <code>eds.emergency_priority</code> A priority score extractor <code>eds.emergency_ccmu</code> A CCMU score extractor <code>eds.emergency_gemsa</code> A GEMSA score extractor <code>eds.tnm</code> A TNM score extractor <code>eds.adicap</code> A ADICAP codes extractor <code>eds.drugs</code> A drug mentions extractor <code>eds.cim10</code> A CIM10 terminology matcher <code>eds.umls</code> An UMLS terminology matcher <code>eds.ckd</code> CKD extractor <code>eds.copd</code> COPD extractor <code>eds.cerebrovascular_accident</code> Cerebrovascular accident extractor <code>eds.congestive_heart_failure</code> Congestive heart failure extractor <code>eds.connective_tissue_disease</code> Connective tissue disease extractor <code>eds.dementia</code> Dementia extractor <code>eds.diabetes</code> Diabetes extractor <code>eds.hemiplegia</code> Hemiplegia extractor <code>eds.leukemia</code> Leukemia extractor <code>eds.liver_disease</code> Liver disease extractor <code>eds.lymphoma</code> Lymphoma extractor <code>eds.myocardial_infarction</code> Myocardial infarction extractor <code>eds.peptic_ulcer_disease</code> Peptic ulcer disease extractor <code>eds.peripheral_vascular_disease</code> Peripheral vascular disease extractor <code>eds.solid_tumor</code> Solid tumor extractor <code>eds.alcohol</code> Alcohol consumption extractor <code>eds.tobacco</code> Tobacco consumption extractor"},{"location":"pipes/ner/adicap/","title":"Adicap","text":"<p>The <code>eds.adicap</code> pipeline component matches the ADICAP codes. It was developped to run on anapathology reports.</p> <p>Document type</p> <p>It was developped to work on anapathology reports. We recommend also to use the <code>eds</code> language (<code>edsnlp.blank(\"eds\")</code>)</p> <p>The compulsory characters of the ADICAP code are identified and decoded. These characters represent the following attributes:</p> Field [en] Field [fr] Attribute Sampling mode Mode de prelevement sampling_mode Technic Type de technique technic Organ and regions Appareils, organes et r\u00e9gions organ Pathology Pathologie g\u00e9n\u00e9rale pathology Pathology type Type de la pathologie pathology_type Behaviour type Type de comportement behaviour_type <p>The pathology field takes 4 different values corresponding to the 4 possible interpretations of the ADICAP code, which are : \"PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE\", \"PATHOLOGIE TUMORALE\", \"PATHOLOGIE PARTICULIERE DES ORGANES\" and \"CYTOPATHOLOGIE\".</p> <p>Depending on the pathology value the behaviour type meaning changes, when the pathology is tumoral then it describes the malignancy of the tumor.</p> <p>For further details about the ADICAP code follow this link.</p> <ol><li><p><p>sant\u00e9 A., 2019. Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions. http://esante.gouv.fr/terminologie-adicap</p></p></li></ol>"},{"location":"pipes/ner/adicap/#edsnlp.pipes.ner.adicap.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.adicap())\n\ntext = \"\"\"\nCOMPTE RENDU D\u2019EXAMEN\n\nAnt\u00e9riorit\u00e9(s) :  NEANT\n\n\nRenseignements cliniques :\nContexte d'exploration d'un carcinome canalaire infiltrant du quadrant sup\u00e9ro-\nexterne du sein droit. La l\u00e9sion biopsi\u00e9e ce jour est situ\u00e9e \u00e0 5,5 cm de la l\u00e9sion\ndu quadrant sup\u00e9ro-externe, \u00e0 l'union des quadrants inf\u00e9rieurs.\n\n\nMacrobiopsie 10G sur une zone de prise de contraste focale \u00e0 l'union des quadrants\ninf\u00e9rieurs du sein droit, mesurant 4 mm, class\u00e9e ACR4\n\n14 fragments ont \u00e9t\u00e9 communiqu\u00e9s fix\u00e9s en formol (lame n\u00b0 1a et lame n\u00b0 1b) . Il\nn'y a pas eu d'\u00e9chantillon congel\u00e9. Ces fragments ont \u00e9t\u00e9 inclus en paraffine en\ntotalit\u00e9 et coup\u00e9s sur plusieurs niveaux.\nHistologiquement, il s'agit d'un parenchyme mammaire fibroadipeux parfois\nl\u00e9g\u00e8rement dystrophique avec quelques petits kystes. Il n'y a pas d'hyperplasie\n\u00e9pith\u00e9liale, pas d'atypie, pas de prolif\u00e9ration tumorale. On note quelques\nsuffusions h\u00e9morragiques focales.\n\nConclusion :\nL\u00e9gers remaniements dystrophiques \u00e0 l'union des quadrants inf\u00e9rieurs du sein droit.\nAbsence d'atypies ou de prolif\u00e9ration tumorale.\n\nCodification :   BHGS0040\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (BHGS0040,)\n\nent = doc.ents[0]\n\nent.label_\n# Out: adicap\n\nent._.adicap.dict()\n# Out: {'code': 'BHGS0040',\n# 'sampling_mode': 'BIOPSIE CHIRURGICALE',\n# 'technic': 'HISTOLOGIE ET CYTOLOGIE PAR INCLUSION',\n# 'organ': \"SEIN (\u00c9GALEMENT UTILIS\u00c9 CHEZ L'HOMME)\",\n# 'pathology': 'PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE',\n# 'pathology_type': 'ETAT SUBNORMAL - LESION MINEURE',\n# 'behaviour_type': 'CARACTERES GENERAUX'}\n</code></pre>"},{"location":"pipes/ner/adicap/#edsnlp.pipes.ner.adicap.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'adicap'</code> </p> <code>pattern</code> <p>The regex pattern to use for matching ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>([A-Z]\\.?[A-Z]\\.?[A-Z]{2}\\.?(?:\\d{4}|\\d{4}|[A-Z...</code> </p> <code>prefix</code> <p>The regex pattern to use for matching the prefix before ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>(?i)(codification|adicap)</code> </p> <code>window</code> <p>Number of tokens to look for prefix. It will never go further the start of the sentence</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>adicap</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'adicap': True}</code> </p>"},{"location":"pipes/ner/adicap/#edsnlp.pipes.ner.adicap.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.adicap</code> pipeline was developed by AP-HP's Data Science team. The codes were downloaded from the website of 'Agence du num\u00e9rique en sant\u00e9' (\"Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions\", sant\u00e9, 2019)</p>"},{"location":"pipes/ner/cim10/","title":"CIM10","text":"<p>The <code>eds.cim10</code> pipeline component extract terms from documents using the CIM10 (French-language ICD) terminology as a reference.</p> <p>Very low recall</p> <p>When using the <code>exact</code> matching mode, this component has a very poor recall performance. We can use the <code>simstring</code> mode to retrieve approximate matches, albeit at the cost of a significantly higher computation time.</p>"},{"location":"pipes/ner/cim10/#edsnlp.pipes.ner.cim10.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.cim10(term_matcher=\"simstring\"))\n\ntext = \"Le patient est suivi pour fi\u00e8vres typho\u00efde et paratypho\u00efde.\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (fi\u00e8vres typho\u00efde et paratypho\u00efde,)\n\nent = doc.ents[0]\n\nent.label_\n# Out: cim10\n\nent.kb_id_\n# Out: A01\n</code></pre>"},{"location":"pipes/ner/cim10/#edsnlp.pipes.ner.cim10.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cim10'</code> </p> <code>attr</code> <p>The default attribute to use for matching.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>'exact'</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cim10'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'cim10': True}</code> </p> RETURNS DESCRIPTION <code>TerminologyMatcher</code>"},{"location":"pipes/ner/cim10/#edsnlp.pipes.ner.cim10.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.cim10</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/ner/covid/","title":"COVID","text":"<p>The <code>eds.covid</code> pipeline component detects mentions of COVID19.</p>"},{"location":"pipes/ner/covid/#edsnlp.pipes.ner.covid.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.covid())\n\ntext = \"Le patient est admis pour une infection au coronavirus.\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (infection au coronavirus,)\n</code></pre>"},{"location":"pipes/ner/covid/#edsnlp.pipes.ner.covid.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'covid'</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'LOWER'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>patterns</code> <p>The regex pattern to use</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>patterns</code> </p> <code>label</code> <p>Label to use for matches</p> <p> TYPE: <code>str</code> DEFAULT: <code>'covid'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'covid': True}</code> </p> RETURNS DESCRIPTION <code>GenericMatcher</code>"},{"location":"pipes/ner/covid/#edsnlp.pipes.ner.covid.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.covid</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/ner/drugs/","title":"Drugs","text":"<p>The <code>eds.drugs</code> pipeline component detects mentions of French drugs (brand names and active ingredients) and adds them to <code>doc.ents</code>. Each drug is mapped to an ATC code through the Romedi terminology (Cossin et al., 2019). The ATC classifies drugs into groups.</p> <ol><li><p><p>Cossin S., Lebrun L., Lobre G., Loustau R., Jouhet V., Griffier R., Mougin F., Diallo G. and Thiessard F., 2019. Romedi: An Open Data Source About French Drugs on the Semantic Web. {Studies in Health Technology and Informatics}. 264, pp.79-82. 10.3233/SHTI190187</p></p></li></ol>"},{"location":"pipes/ner/drugs/#edsnlp.pipes.ner.drugs.factory.create_component--examples","title":"Examples","text":"<p>In this example, we are looking for an oral antidiabetic medication (ATC code: A10B).</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.drugs(term_matcher=\"exact\"))\n\ntext = \"Traitement habituel: Kard\u00e9gic, cardensiel (bisoprolol), glucophage, lasilix\"\n\ndoc = nlp(text)\n\ndrugs_detected = [(x.text, x.kb_id_) for x in doc.ents]\n\ndrugs_detected[0]\n# Out: ('Kard\u00e9gic', 'B01AC06')\n\nlen(drugs_detected)\n# Out: 5\n\noral_antidiabetics_detected = list(\n    filter(lambda x: (x[1].startswith(\"A10B\")), drugs_detected)\n)\noral_antidiabetics_detected\n# Out: [('glucophage', 'A10BA02')]\n</code></pre> <p>Glucophage is the brand name of a medication that contains metformine, the first-line medication for the treatment of type 2 diabetes.</p>"},{"location":"pipes/ner/drugs/#edsnlp.pipes.ner.drugs.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'drugs'</code> </p> <code>attr</code> <p>The default attribute to use for matching.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>'exact'</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'drug'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'drug': True}</code> </p> RETURNS DESCRIPTION <code>TerminologyMatcher</code>"},{"location":"pipes/ner/drugs/#edsnlp.pipes.ner.drugs.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.drugs</code> pipeline was developed by the IAM team and CHU de Bordeaux's Data Science team.</p>"},{"location":"pipes/ner/suicide_attempt/","title":"Suicide Attempt","text":"<p>The <code>eds.suicide_attempt</code> pipeline component detects mentions of Suicide Attempt. It can be used with a span qualifier for contextualisation of the entity (history) and to detect false positives as negation, hypothesis or family. We recommend to use a machine learning qualifier to disambiguate polysemic words, as proposed in Bey et al., 2024.</p> <p>Every matched entity will be labelled <code>suicide_attempt</code>.</p> <ol><li><p><p>Bey R., Cohen A., Trebossen V., Dura B., Geoffroy P., Jean C., Landman B., Petit-Jean T., Chatellier G., Sallah K., Tannier X., Bourmaud A. and Delorme R., 2024. Natural language processing of multi-hospital electronic health records for public health surveillance of suicidality. 10.1038/s44184-023-00046-7</p></p></li></ol>"},{"location":"pipes/ner/suicide_attempt/#edsnlp.pipes.ner.suicide_attempt.factory.create_component--extensions","title":"Extensions","text":"<p>Each entity span will have the suicide attempt modality as an attribute. The available modalities are:</p> <ul> <li><code>suicide_attempt_unspecific</code></li> <li><code>autolysis</code></li> <li><code>intentional_drug_overdose</code></li> <li><code>jumping_from_height</code></li> <li><code>cuts</code></li> <li><code>strangling</code></li> <li><code>self_destructive_behavior</code></li> <li><code>burn_gas_caustic</code></li> </ul>"},{"location":"pipes/ner/suicide_attempt/#edsnlp.pipes.ner.suicide_attempt.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.suicide_attempt())\n\ntext = \"J'ai vu le patient \u00e0 cause d'une IMV avec paracetamol\"\ndoc = nlp(text)\ndoc.ents\n# Out: (IMV,)\n\nent = doc.ents[0]\nent._.suicide_attempt_modality\n# Out: 'intentional_drug_overdose'\n</code></pre> Patterns used for the named entity recognition <pre><code># fmt: off\npatterns = {\n    \"suicide_attempt_unspecific\": [\n        r\"\\b(?&lt;!\\.)(?&lt;!Voie\\s\\d\\s\\:\\s)(?&lt;!Voie\\sd.abord\\s\\:\\s)(?&lt;!surface\\s)(?&lt;!d[\u00e9e]sorientation\\s)(?&lt;!abord\\s)(?&lt;!ECG\\s:\\s)(?&lt;!volume\\s)(?&lt;!\\d\\s[mc]m\\sde\\sla\\s)(?&lt;!\\d[mc]m\\sde\\sla\\s)(?&lt;!au\\scontact\\sde\\sla\\s)T\\.?S\\.?(?![\\.A-Za-z])(?!\\sapyr[e\u00e9]tique)(?!.+TRANSSEPTAL)(?!.+T[34])(?!.+en\\sr.gression)\\b\",\n        r\"(?&lt;!\\.)T\\.S\\.(?![A-Za-z])\",\n        r\"\\b(?&lt;!.)TS\\.\\B\",\n        r\"(?i)tentative[s]?\\s+de\\s+sui?cide\",\n        r\"(?i)tent[\u00e9e]\\s+de\\s+((se\\s+(suicider|tuer))|(mettre\\s+fin\\s+[\u00e0a]\\s+((ses\\s+jours?)|(sa\\s+vie))))\",\n    ],\n    \"autolysis\": [r\"(?i)tentative\\s+d'autolyse\", r\"(?i)autolyse\"],\n    \"intentional_drug_overdose\": [\n        r\"(?i)(intoxication|ingestion)\\s+m[\u00e9e]dicamenteuse\\s+volontaire\",\n        r\"(?i)\\b(i\\.?m\\.?v\\.?)\\b\",\n        r\"(?i)(intoxication|ingestion)\\s*([a-zA-Z0-9_\u00e9\u00e0\u00e8\u00f4\u00ea\\-]+\\s*){0,3}\\s*volontaire\",\n        r\"TS\\s+med\\s+polymedicamenteuse\",\n        r\"TS\\s+(poly)?([\\s-])?m[\u00e9e]dicamenteuse\",\n    ],\n    \"jumping_from_height\": [\n        r\"(?i)tentative[s]?\\s+de\\s+d[\u00e9e]fenestration\",\n        r\"(?i)(?&lt;!id[\u00e9e]es?\\sde\\s)d[\u00e9e]fenestration(?!\\saccidentelle)\",\n        r\"(?i)d[\u00e9e]fenestration\\s+volontaire\",\n        r\"(?i)d[\u00e9e]fenestration\\s+intentionnelle\",\n        r\"(?i)jet.r?\\sd.un\\spont\",\n    ],\n    \"cuts\": [r\"(?i)phl[\u00e9e]botomie\"],\n    \"strangling\": [r\"(?i)pendaison\"],\n    \"self_destructive_behavior\": [r\"(?i)autodestruction\"],\n    \"burn_gas_caustic\": [r\"(?i)ing[e\u00e9]stion\\sde\\s(produit\\s)?caustique\"],\n}\n\n# fmt: on\n</code></pre>"},{"location":"pipes/ner/suicide_attempt/#edsnlp.pipes.ner.suicide_attempt.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'eds.suicide_attempt'</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'TEXT'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>patterns</code> <p>The regex pattern to use</p> <p> TYPE: <code>List[str]</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'suicide_attempt'</code> </p> RETURNS DESCRIPTION <code>SuicideAttemptMatcher</code>"},{"location":"pipes/ner/suicide_attempt/#edsnlp.pipes.ner.suicide_attempt.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.suicide_attempt</code> component was developed by AP-HP's Data Science team, following the insights of the algorithm proposed by Bey et al., 2024.</p>"},{"location":"pipes/ner/tnm/","title":"TNM","text":"<p>The <code>eds.tnm</code> component extracts TNM mentions from clinical documents.</p> <ol><li><p><p>Kempf E., Priou S., Lam\u00e9 G., Daniel C., Bellamine A., Sommacale D., Belkacemi y., Bey R., Galula G., Taright N., Tannier X., Rance B., Flicoteaux R., Hemery F., Audureau E., Chatellier G. and Tournigand C., 2022. Impact of two waves of Sars-Cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: A French multicentric cohort study from a large group of University hospitals. {International Journal of Cancer}. 150, pp.1609-1618. 10.1002/ijc.33928</p></p></li></ol>"},{"location":"pipes/ner/tnm/#edsnlp.pipes.ner.tnm.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.tnm())\n\ntext = \"TNM: pTx N1 M1\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (pTx N1 M1,)\n\nent = doc.ents[0]\nent._.tnm.dict()\n# {'modifier': 'p',\n#  'tumour': None,\n#  'tumour_specification': 'x',\n#  'node': '1',\n#  'node_specification': None,\n#  'metastasis': '1',\n#  'resection_completeness': None,\n#  'version': None,\n#  'version_year': None}\n</code></pre>"},{"location":"pipes/ner/tnm/#edsnlp.pipes.ner.tnm.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'tnm'</code> </p> <code>pattern</code> <p>The regex pattern to use for matching ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>(?:\\b|^)(?&lt;=\\(?(?P&lt;version&gt;uicc|accj|tnm|UICC|A...</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>tnm</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'tnm': True}</code> </p>"},{"location":"pipes/ner/tnm/#edsnlp.pipes.ner.tnm.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The TNM score is based on the development of S. Priou, B. Rance and E. Kempf (Kempf et al., 2022).</p>"},{"location":"pipes/ner/umls/","title":"UMLS","text":"<p>The <code>eds.umls</code> pipeline component matches the UMLS (Unified Medical Language System from NIH) terminology.</p> <p>Very low recall</p> <p>When using the <code>exact</code> matching mode, this component has a very poor recall performance. We can use the <code>simstring</code> mode to retrieve approximate matches, albeit at the cost of a significantly higher computation time.</p>"},{"location":"pipes/ner/umls/#edsnlp.pipes.ner.umls.factory.create_component--examples","title":"Examples","text":"<p><code>eds.umls</code> is an additional module that needs to be setup by:</p> <ol> <li><code>pip install -U umls_downloader</code></li> <li>Signing up for a UMLS Terminology    Services Account. After filling a short form, you will receive your token API    within a few days.</li> <li>Set <code>UMLS_API_KEY</code> locally: <code>export UMLS_API_KEY=your_api_key</code></li> </ol> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.umls())\n\ntext = \"Grosse toux: le malade a \u00e9t\u00e9 mordu par des Amphibiens \" \"sous le genou\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (toux, a, par, Amphibiens, genou)\n\nent = doc.ents[0]\n\nent.label_\n# Out: umls\n\nent._.umls\n# Out: C0010200\n</code></pre> <p>You can easily change the default languages and sources with the <code>pattern_config</code> argument:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\n# Load the French MeSH\npattern_config = dict(languages=[\"FRE\"], sources=[\"MSHFRE\"])\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.umls(pattern_config=pattern_config))\n</code></pre> <p>See more options of languages and sources here.</p>"},{"location":"pipes/ner/umls/#edsnlp.pipes.ner.umls.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'umls'</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The term matcher to use, either \"exact\" or \"simstring\"</p> <p> TYPE: <code>TerminologyTermMatcher</code> DEFAULT: <code>'exact'</code> </p> <code>term_matcher_config</code> <p>The configuration for the term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>pattern_config</code> <p>The pattern retriever configuration</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>dict(languages=['FRE'], sources=None)</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'umls'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'umls': True}</code> </p>"},{"location":"pipes/ner/umls/#edsnlp.pipes.ner.umls.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.umls</code> pipeline was developed by AP-HP's Data Science team and INRIA SODA's team.</p>"},{"location":"pipes/ner/behaviors/","title":"Behaviors","text":""},{"location":"pipes/ner/behaviors/#presentation","title":"Presentation","text":"<p>EDS-NLP offers two components to extract behavioral patterns, namely the tobacco and alcohol consumption status. Each component is based on the ContextualMatcher component. Some general considerations about those components:</p> <ul> <li>Extracted entities are stored in <code>doc.ents</code> and <code>doc.spans</code>. For instance, the <code>eds.tobacco</code> component stores matches in <code>doc.spans[\"tobacco\"]</code>.</li> <li>The matched comorbidity is also available under the <code>ent.label_</code> of each match.</li> <li>Matches have an associated <code>_.status</code> attribute taking the value <code>1</code>, or <code>2</code>. A corresponding <code>_.detailed_status</code> attribute stores the human-readable status, which can be component-dependent. See each component documentation for more details.</li> <li>Some components add additional information to matches. For instance, the <code>tobacco</code> adds, if relevant, extracted pack-year (= paquet-ann\u00e9e). Those information are available under the <code>ent._.assigned</code> attribute.</li> <li>Those components work on normalized documents. Please use the <code>eds.normalizer</code> pipeline with the following parameters:   <pre><code>nlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\n</code></pre></li> </ul> <p>Use qualifiers</p> <p>Those components should be used with a qualification pipeline to avoid extracted unwanted matches. At the very least, you can use available rule-based qualifiers (<code>eds.negation</code>, <code>eds.hypothesis</code> and <code>eds.family</code>). Better, a machine learning qualification component was developed and trained specifically for those components. For privacy reason, the model isn't publicly available yet.</p> <p>Use the ML model</p> <p>The model will soon be available in the models catalogue of AP-HP's CDW.</p>"},{"location":"pipes/ner/behaviors/#usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.tobacco())\nnlp.add_pipe(eds.diabetes())\n\ntext = \"\"\"\nCompte-rendu de consultation.\n\nJe vois ce jour M. SCOTT pour le suivi de sa r\u00e9tinopathie diab\u00e9tique.\nLe patient va bien depuis la derni\u00e8re fois.\nJe le f\u00e9licite pour la poursuite de son sevrage tabagique (toujours \u00e0 10 paquet-ann\u00e9e).\n\nSur le plan de son diab\u00e8te, la glyc\u00e9mie est stable.\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.spans\n# Out: {\n# 'pollutions': [],\n# 'tobacco': [sevrage tabagique (toujours \u00e0 10 paquet-ann\u00e9e],\n# 'diabetes': [r\u00e9tinopathie diab\u00e9tique, diab\u00e8te]\n# }\n\ntobacco_matches = doc.spans[\"tobacco\"]\ntobacco_matches[0]._.detailed_status\n# Out: \"ABSTINENCE\" #\n\ntobacco_matches[0]._.assigned[\"PA\"]  # paquet-ann\u00e9e\n# Out: 10 # (1)\n\n\ndiabetes = doc.spans[\"diabetes\"]\n(diabetes[0]._.detailed_status, diabetes[1]._.detailed_status)\n# Out: ('WITH_COMPLICATION', 'WITHOUT_COMPLICATION') # (2)\n</code></pre> <ol> <li>Here we see an example of additional information that can be extracted</li> <li>Here we see the importance of document-level aggregation to extract the correct severity of each comorbidity.</li> </ol> <ol></ol>"},{"location":"pipes/ner/behaviors/alcohol/","title":"Alcohol consumption","text":"<p>The <code>eds.alcohol</code> pipeline component extracts mentions of alcohol consumption. It won't match occasional consumption, nor acute intoxication.</p> Details of the used patterns <pre><code># fmt: off\ndefault_pattern = dict(\n    source=\"alcohol\",\n    regex=[\n        r\"\\balco[ol]\",\n        r\"\\bethyl\",\n        r\"(?&lt;!(25.{0,10}))\\boh\\b\",\n        r\"exogenose\",\n        r\"delirium.tremens\",\n    ],\n    exclude=[\n        dict(\n            regex=[\n                \"occasion\",\n                \"episod\",\n                \"festi\",\n                \"rare\",\n                \"libre\",  # OH-libres\n                \"aigu\",\n            ],\n            window=(-3, 5),\n        ),\n        dict(\n            regex=[\"pansement\", \"compress\"],\n            window=-3,\n        ),\n    ],\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"stopped\",\n            regex=r\"(\\bex\\b|sevr|arret|stop|ancien)\",\n            window=(-3, 15),\n            reduce_mode=\"keep_first\",\n        ),\n        dict(\n            name=\"zero_after\",\n            regex=r\"(?=^[a-z]*\\s*:?[\\s-]*(0|non|aucun|jamais))\",\n            window=3,\n            reduce_mode=\"keep_first\",\n        ),\n    ],\n)\ndefault_patterns = [default_pattern]\n\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/behaviors/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: either None or <code>\"ABSTINENCE\"</code> if the patient stopped its consumption</li> <li><code>span._.negation</code>: set to True when a mention such as \"alcool: 0\" is found</li> </ul> <p>Use qualifiers !</p> <p>Although the alcohol pipe sometime sets value for the <code>negation</code> attribute, generic qualifier should still be used after the pipe.</p>"},{"location":"pipes/ner/behaviors/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.alcohol())\n</code></pre> <p>Below are a few examples:</p> 12345678 <pre><code>text = \"Patient alcoolique.\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [alcoolique]\n</code></pre> <pre><code>text = \"OH chronique.\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [OH]\n</code></pre> <pre><code>text = \"Prise d'alcool occasionnelle\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Application d'un pansement alcoolis\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Alcoolisme sevr\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcoolisme sevr\u00e9]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': sevr\u00e9}\n</code></pre> <pre><code>text = \"Alcoolisme non sevr\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcoolisme non sevr\u00e9]\n\nspan = spans[0]\n\nspan._.detailed_status  # \"sevr\u00e9\" is negated, so no \"ABTINENCE\" status\n# Out: None\n</code></pre> <pre><code>text = \"Alcool: 0\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcool]\n\nspan = spans[0]\n\nspan._.negation\n# Out: True\n\nspan._.assigned\n# Out: {'zero_after': 0}\n</code></pre> <pre><code>text = \"Le patient est en cours de sevrage \u00e9thylotabagique\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [sevrage \u00e9thylotabagique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': sevrage}\n</code></pre>"},{"location":"pipes/ner/behaviors/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'alcohol', 'regex': ['\\\\balco[ol]',...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>alcohol</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'alcohol': True}</code> </p>"},{"location":"pipes/ner/behaviors/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.alcohol</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/behaviors/tobacco/","title":"Tobacco consumption","text":"<p>The <code>eds.tobacco</code> pipeline component extracts mentions of tobacco consumption.</p> Details of the used patterns <pre><code># fmt: off\nPA = r\"(?:\\bp/?a\\b|paquets?.?annee)\"\nQUANTITY = r\"(?P&lt;quantity&gt;[\\d]{1,3})\"\nPUNCT = r\"\\.,-;\\(\\)\"\n\ndefault_patterns = [\n    dict(\n        source=\"tobacco\",\n        regex=[\n            r\"tabagi\",\n            r\"tabac\",\n            r\"\\bfume\\b\",\n            r\"\\bfumeu\",\n            r\"\\bpipes?\\b\",\n        ],\n        exclude=dict(\n            regex=[\n                \"occasion\",\n                \"moder\",\n                \"quelqu\",\n                \"festi\",\n                \"rare\",\n                \"sujet\",  # Example : Chez le sujet fumeur ... generic sentences\n            ],\n            window=(-3, 5),\n        ),\n        regex_attr=\"NORM\",\n        assign=[\n            dict(\n                name=\"stopped\",\n                regex=r\"(\\bex\\b|sevr|arret|stop|ancien)\",\n                window=(-3, 15),\n                reduce_mode=\"keep_first\",\n            ),\n            dict(\n                name=\"zero_after\",\n                regex=r\"(?=^[a-z]*\\s*:?[\\s-]*(0|non|aucun|jamais))\",\n                window=3,\n                reduce_mode=\"keep_first\",\n            ),\n            dict(\n                name=\"PA\",\n                regex=rf\"{QUANTITY}[^{PUNCT}]{{0,10}}{PA}|{PA}[^{PUNCT}]{{0,10}}{QUANTITY}\",\n                window=(-10, 10),\n                reduce_mode=\"keep_first\",\n            ),\n            dict(\n                name=\"secondhand\",\n                regex=\"(passif)\",\n                window=5,\n                reduce_mode=\"keep_first\",\n            ),\n        ],\n    )\n]\n\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/behaviors/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: either None or <code>\"ABSTINENCE\"</code> if the patient stopped its consumption</li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>PA</code>: the mentioned year-pack (= paquet-ann\u00e9e)</li> <li><code>secondhand</code>: if secondhand smoking</li> </ul> </li> <li><code>span._.negation</code>: set to True when either<ul> <li>A pack-year value of 0 is extracted</li> <li>A mention such as \"tabac: 0\" is found</li> <li>The patient experiences secondhand smoking</li> </ul> </li> </ul> <p>Use qualifiers !</p> <p>Although the tobacco pipe sometime sets value for the <code>negation</code> attribute, generic qualifier should still be used after the pipe.</p>"},{"location":"pipes/ner/behaviors/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.tobacco())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Tabagisme \u00e9valu\u00e9 \u00e0 15 PA\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabagisme \u00e9valu\u00e9 \u00e0 15 PA]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'PA': 15}\n</code></pre> <pre><code>text = \"Patient tabagique\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [tabagique]\n</code></pre> <pre><code>text = \"Tabagisme festif\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"On a un tabagisme ancien\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [tabagisme ancien]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': ancien}\n</code></pre> <pre><code>text = \"Tabac: 0\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabac]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: None\n\nspan._.negation\n# Out: True\n\nspan._.assigned\n# Out: {'zero_after': 0}\n</code></pre> <pre><code>text = \"Tabagisme passif\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabagisme passif]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: None\n\nspan._.negation\n# Out: True\n\nspan._.assigned\n# Out: {'secondhand': passif}\n</code></pre> <pre><code>text = \"Tabac: sevr\u00e9 depuis 5 ans\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabac: sevr\u00e9]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': sevr\u00e9}\n</code></pre>"},{"location":"pipes/ner/behaviors/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'tobacco'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'tobacco', 'regex': ['tabagi', 'tab...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>tobacco</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'tobacco': True}</code> </p>"},{"location":"pipes/ner/behaviors/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.tobacco</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/disorders/","title":"Disorders","text":""},{"location":"pipes/ner/disorders/#presentation","title":"Presentation","text":"<p>The following components extract 16 different conditions from the Charlson Comorbidity Index. Each component is based on the ContextualMatcher component.</p> <p>The components were developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024</p> <p>Some general considerations about those components:</p> <ul> <li>Extracted entities are stored in <code>doc.ents</code> and <code>doc.spans</code>. For instance, the <code>eds.tobacco</code> component stores matches in <code>doc.spans[\"tobacco\"]</code>.</li> <li>The matched comorbidity is also available under the <code>ent.label_</code> of each match.</li> <li>Matches have an associated <code>_.status</code> attribute taking the value <code>1</code>, or <code>2</code>. A corresponding <code>_.detailed_status</code> attribute stores the human-readable status, which can be component-dependent. See each component documentation for more details.</li> <li>Some components add additional information to matches. For instance, the <code>tobacco</code> adds, if relevant, extracted pack-year (= paquet-ann\u00e9e). Those information are available under the <code>ent._.assigned</code> attribute.</li> <li> <p>Those components work on normalized documents. Please use the <code>eds.normalizer</code> pipeline with the following parameters:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n...\n\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\n</code></pre> </li> </ul> <p>Use qualifiers</p> <p>Those components should be used with a qualification pipeline to avoid extracted unwanted matches. At the very least, you can use available rule-based qualifiers (<code>eds.negation</code>, <code>eds.hypothesis</code> and <code>eds.family</code>). Better, a machine learning qualification component was developed and trained specifically for those components. For privacy reason, the model isn't publicly available yet.</p> <p>Use the ML model</p> <p>The model will soon be available in the models catalogue of AP-HP's CDW.</p> <p>On the medical definition of the comorbidities</p> <p>Those components were developped to extract chronic and symptomatic conditions only.</p>"},{"location":"pipes/ner/disorders/#aggregation","title":"Aggregation","text":"<p>For relevant phenotyping, matches should be aggregated at the document-level. For instance, a document might mention a complicated diabetes at the beginning (\"Le patient a une r\u00e9tinopathie diab\u00e9tique\"), and then refer to this diabetes without mentionning that it is complicated anymore (\"Concernant son diab\u00e8te, le patient ...\"). Thus, a good and simple aggregation rule is, for each comorbidity, to</p> <ul> <li>disregard all entities tagged as irrelevant by the qualification component(s)</li> <li>take the maximum (i.e., the most severe) status of the leftover entities</li> </ul> <p>An implementation of this rule is presented here</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/disorders/aids/","title":"AIDS","text":"<p>The <code>eds.aids</code> pipeline component extracts mentions of AIDS. It will notably match:</p> <ul> <li>Mentions of VIH/HIV at the SIDA/AIDS stage</li> <li>Mentions of VIH/HIV with opportunistic(s) infection(s)</li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre> <p>On HIV infection</p> <p>pre-AIDS HIV infection are not extracted, only AIDS.</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/disorders/aids/#edsnlp.pipes.ner.disorders.aids.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>opportunist</code>: list of opportunist infections extracted around the HIV mention</li> <li><code>stage</code>: stage of the HIV infection</li> </ul> </li> </ul>"},{"location":"pipes/ner/disorders/aids/#edsnlp.pipes.ner.disorders.aids.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(f\"eds.aids\")\n</code></pre> <p>Below are a few examples:</p> SIDAVIHCoinfectionVIH stade SIDA <pre><code>text = \"Patient atteint du VIH au stade SIDA.\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH au stade SIDA]\n</code></pre> <pre><code>text = \"Patient atteint du VIH.\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Il y a un VIH avec coinfection pneumocystose\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'opportunist': [coinfection, pneumocystose]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un VIH stade C\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': [C]}\n</code></pre>"},{"location":"pipes/ner/disorders/aids/#edsnlp.pipes.ner.disorders.aids.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'aids'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'aids', 'regex': ['(vih.{1,5}stade....</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>aids</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'aids': True}</code> </p>"},{"location":"pipes/ner/disorders/aids/#edsnlp.pipes.ner.disorders.aids.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.aids</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/disorders/cerebrovascular-accident/","title":"Cerebrovascular accident","text":"<p>The <code>eds.cerebrovascular_accident</code> pipeline component extracts mentions of cerebrovascular accident. It will notably match:</p> <ul> <li>Mentions of AVC/AIT</li> <li>Mentions of bleeding, hemorrhage, thrombus, ischemia, etc., localized in the brain</li> </ul> Details of the used patterns <pre><code># fmt: off\nimport re\n\nfrom edsnlp.utils.resources import get_AVC_care_site\n\nfrom ..terms import BRAIN, HEART, PERIPHERAL\n\nAVC_CARE_SITES_REGEX = [\n    r\"\\b\" + re.escape(cs.strip()) + r\"\\b\" for cs in get_AVC_care_site(prefix=True)\n] + [\n    r\"h[o\u00f4]p\",\n    r\"\\brcp\",\n    r\"service\",\n    r\"\\bsau\",\n    r\"ap.?hp\",\n    r\"\\burg\",\n    r\"finess\",\n    r\"\\bsiret\",\n    r\"[\u00e0a] avc\",\n    r\"consult\",\n]\n\navc = dict(\n    source=\"avc\",\n    regex=[\n        r\"\\bavc\\b\",\n    ],\n    exclude=[\n        dict(\n            regex=AVC_CARE_SITES_REGEX,\n            window=(-5, 5),\n            regex_flags=re.S | re.I,\n            limit_to_sentence=False,\n        ),\n        dict(\n            regex=r\"\\b[a-z]\\.\",\n            window=2,\n            limit_to_sentence=False,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"(hemorr?agie|hematome)\",\n        r\"angiopath\",\n        r\"angioplasti\",\n        r\"infarctus\",\n        r\"occlusion\",\n        r\"saignement\",\n        r\"embol\",\n        r\"vascularite\",\n        r\"\\bhsd\\b\",\n        r\"thrombos\",\n        r\"thrombol[^y]\",\n        r\"thrombophi\",\n        r\"thrombi[^n]\",\n        r\"thrombus\",\n        r\"thrombectomi\",\n        r\"phleb\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=[\n        dict(\n            regex=r\"pulmo|poumon\",\n            window=4,\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"brain_localized\",\n            regex=\"(\" + r\"|\".join(BRAIN) + \")\",\n            window=(-15, 15),\n            limit_to_sentence=False,\n            include_assigned=False,\n        ),\n    ],\n)\n\ngeneral = dict(\n    source=\"general\",\n    regex=[\n        r\"acc?ident.{1,5}\\s*vasculaire?.{1,6}\\s*cereb.{1,5}\",\n        r\"acc?ident.{1,5}\\s*vasculaire?.{1,6}\\s*ischemi\\w+\",\n        r\"acc?ident.{1,5}ischemi\\w+\",\n        r\"moya.?moya\",\n        r\"oc?clusion.{1,5}(arter|veine).{1,20}retine\",\n        r\"vasculo\\s*path\\w+.cerebr?a\\w+.isch\\w+\",\n        r\"maladies?.des.petites.arter\\w+\",\n        r\"maladies?.des.petits.vaisseaux\",\n        r\"thromboly?i?se\",\n        r\"\\bsusac\\b\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bAIC\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nAIT = dict(\n    source=\"AIT\",\n    regex=[\n        r\"\\bAIC\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n        r\"\\bAIT\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nischemia = dict(\n    source=\"ischemia\",\n    regex=[\n        r\"ischemi\",\n    ],\n    exclude=[\n        dict(\n            regex=PERIPHERAL + HEART,\n            window=(-7, 7),\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"brain\",\n            regex=\"(\" + r\"|\".join(BRAIN) + \")\",\n            window=(-10, 15),\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    avc,\n    with_localization,\n    general,\n    acronym,\n    AIT,\n    ischemia,\n]\n\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/disorders/cerebrovascular-accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"pipes/ner/disorders/cerebrovascular-accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.cerebrovascular_accident())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Patient hospitalis\u00e9 \u00e0 AVC.\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Hospitalisation pour un AVC.\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [AVC]\n</code></pre> <pre><code>text = \"Saignement intracranien\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Saignement]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [intracranien]}\n</code></pre> <pre><code>text = \"Thrombose p\u00e9riph\u00e9rique\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Thrombose sylvienne\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Thrombose]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [sylvienne]}\n</code></pre> <pre><code>text = \"Infarctus c\u00e9r\u00e9bral\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Infarctus]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [c\u00e9r\u00e9bral]}\n</code></pre> <pre><code>text = \"Soign\u00e9 via un thrombolyse\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [thrombolyse]\n</code></pre>"},{"location":"pipes/ner/disorders/cerebrovascular-accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'avc', 'regex': ['\\\\bavc\\\\b'], 'exc...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>cerebrovascular_accident</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'cerebrovascular_accident': True}</code> </p>"},{"location":"pipes/ner/disorders/cerebrovascular-accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.cerebrovascular_accident</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/disorders/ckd/","title":"CKD","text":"<p>The <code>eds.CKD</code> pipeline component extracts mentions of CKD (Chronic Kidney Disease). It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Kidney transplantation</li> <li>Chronic dialysis</li> <li>Renal failure from stage 3 to 5. The stage is extracted by trying 3 methods:<ul> <li>Extracting the mentioned stage directly (\"IRC stade IV\")</li> <li>Extracting the severity directly (\"IRC terminale\")</li> <li>Extracting the mentioned GFR (DFG in french) (\"IRC avec DFG estim\u00e9 \u00e0 30   mL/min/1,73m2)\")</li> </ul> </li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/disorders/ckd/#edsnlp.pipes.ner.disorders.ckd.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>stage</code>: mentioned renal failure stage</li> <li><code>status</code>: mentioned renal failure severity (e.g. mod\u00e9r\u00e9e, s\u00e9v\u00e8re, terminale,   etc.)</li> <li><code>dfg</code>: mentioned DFG</li> </ul> </li> </ul>"},{"location":"pipes/ner/disorders/ckd/#edsnlp.pipes.ner.disorders.ckd.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.ckd())\n</code></pre> <p>Below are a few examples:</p> 1234567891011 <pre><code>text = \"Patient atteint d'une glom\u00e9rulopathie.\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [glom\u00e9rulopathie]\n</code></pre> <pre><code>text = \"Patient atteint d'une tubulopathie aig\u00fce.\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Patient transplant\u00e9 r\u00e9nal\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [transplant\u00e9 r\u00e9nal]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une insuffisance r\u00e9nale aig\u00fce sur chronique\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [insuffisance r\u00e9nale aig\u00fce sur chronique]\n</code></pre> <pre><code>text = \"Le patient a \u00e9t\u00e9 dialys\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Le patient est dialys\u00e9 chaque lundi\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [dialys\u00e9 chaque lundi]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'chronic': [lundi]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC s\u00e9v\u00e8re\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC s\u00e9v\u00e8re]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'status': s\u00e9v\u00e8re}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC au stade IV\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC au stade IV]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': IV}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC avec DFG \u00e0 30\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC avec DFG \u00e0 30]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'dfg': 30}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une maladie r\u00e9nale avec DFG \u00e0 110\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"pipes/ner/disorders/ckd/#edsnlp.pipes.ner.disorders.ckd.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['glomerulo\\\\s*nep...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>ckd</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'ckd': True}</code> </p>"},{"location":"pipes/ner/disorders/ckd/#edsnlp.pipes.ner.disorders.ckd.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.ckd</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/disorders/congestive-heart-failure/","title":"Congestive heart failure","text":"<p>The <code>eds.congestive_heart_failure</code> pipeline component extracts mentions of congestive heart failure. It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Heart transplantation</li> <li>AF (Atrial Fibrillation)</li> <li>Pacemaker</li> </ul> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import ASYMPTOMATIC\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"defaill?ance.{1,10}cardi\\w+\",\n        r\"(\u0153|oe)deme?.{1,10}pulmon\",\n        r\"decompensation.{1,10}card\",\n        r\"choc.{1,30}cardio\",\n        r\"greff?e.{1,10}c(\u0153|oe)ur\",\n        r\"greff?e.{1,10}cardia\",\n        r\"transplantation.{1,10}c(\u0153|oe)ur\",\n        r\"transplantation.{1,10}cardia\",\n        r\"arret.{1,10}cardi\",\n        r\"c(\u0153|oe)ur pulmo\",\n        r\"foie.card\",\n        r\"pace.?maker\",\n        r\"stimulateur.cardiaque\",\n        r\"valve.{1,30}(meca|artific)\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nsymptomatic = dict(\n    source=\"symptomatic\",\n    regex=[\n        r\"cardio\\s*path\\w+\",\n        r\"cardio\\s*myopath\\w+\",\n        r\"d(i|y)sfonction.{1,15}(ventricul|\\bvg|cardiaque)\",\n        r\"valvulo\\s*path\\w+?\",\n        r\"\\bic\\b.{1,10}(droite|gauche)\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=ASYMPTOMATIC + [r\"(?&lt;!\\bnon.)ischem\"],  # Exclusion of ischemic events\n        window=5,\n    ),\n)\n\nwith_minimum_severity = dict(\n    source=\"min_severity\",\n    regex=[\n        r\"insuf?fisance.{1,10}(\\bcardi|\\bdiasto|\\bventri|\\bmitral|tri.?cusp)\",\n        r\"(retrecissement|stenose).(aortique|mitral)\",\n        r\"\\brac\\b\",\n        r\"\\brm\\b\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=ASYMPTOMATIC + [\"minime\", \"modere\", r\"non.serre\"],\n        window=5,\n    ),\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bOAP\\b\",\n        r\"\\bCMH\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nAF_main_pattern = dict(\n    source=\"AF_main\",\n    regex=[\n        r\"fibrill?ation.{1,3}(atriale|auriculaire|ventriculaire)\",\n        r\"flutter\",\n        r\"brady.?arythmie\",\n        r\"pace.?maker\",\n    ],\n)\n\nAF_acronym = dict(\n    source=\"AF_acronym\",\n    regex=[\n        r\"\\bFA\\b\",\n        r\"\\bAC.?FA\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    symptomatic,\n    acronym,\n    AF_main_pattern,\n    AF_acronym,\n    with_minimum_severity,\n]\n\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/disorders/congestive-heart-failure/#edsnlp.pipes.ner.disorders.congestive_heart_failure.factory.create_component--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.congestive_heart_failure())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Pr\u00e9sence d'un oed\u00e8me pulmonaire\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [oed\u00e8me pulmonaire]\n</code></pre> <pre><code>text = \"Le patient est \u00e9quip\u00e9 d'un pace-maker\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [pace-maker]\n</code></pre> <pre><code>text = \"Un cardiopathie non d\u00e9compens\u00e9e\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Insuffisance cardiaque\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [Insuffisance cardiaque]\n</code></pre> <pre><code>text = \"Insuffisance cardiaque minime\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"pipes/ner/disorders/congestive-heart-failure/#edsnlp.pipes.ner.disorders.congestive_heart_failure.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>(str)</code> DEFAULT: <code>'congestive_heart_failure'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['defaill?ance.{1,...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>congestive_heart_failure</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'congestive_heart_failure': True}</code> </p>"},{"location":"pipes/ner/disorders/congestive-heart-failure/#edsnlp.pipes.ner.disorders.congestive_heart_failure.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.congestive_heart_failure</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/disorders/connective-tissue-disease/","title":"Connective tissue disease","text":"<p>The <code>eds.connective_tissue_disease</code> pipeline component extracts mentions of connective tissue diseases.</p> Details of the used patterns <pre><code># fmt: off\nTO_EXCLUDE = r\"(?&lt;!a )((\\bacc\\b)|anti.?coag|anti.?corps|buschke|(\\bac\\b)|(\\bbio))\"\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"arth?rites?.{1,5}juveniles?.{1,5}idiopa\\w+\",\n        r\"myosite\",\n        r\"myopath\\w+.{1,5}inflammatoire\",\n        r\"polyarth?rite.{1,5}chroni\\w+.{1,5}evol\",\n        r\"polymyosie\",\n        r\"polyarth?rites?.{1,5}(rhizo|rhuma)\",\n        r\"scleroderm\\w+\",\n        r\"connectivite\",\n        r\"sarcoidose\",\n    ],\n    exclude=dict(\n        regex=[TO_EXCLUDE],\n        window=(-7, 7),\n    ),\n    regex_attr=\"NORM\",\n)\n\nlupus = dict(\n    source=\"lupus\",\n    regex=[\n        r\"\\blupus\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nlupique = dict(\n    source=\"lupique\",\n    regex=[r\"\\blupique\", r\"\\blupic\"],\n    exclude=dict(\n        regex=[TO_EXCLUDE],\n        window=(-7, 7),\n    ),\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronyms\",\n    regex=[\n        r\"\\bAJI\\b\",\n        r\"\\bLED\\b\",\n        r\"\\bPCE\\b\",\n        r\"\\bCREST\\b\",\n        r\"\\bPPR\\b\",\n        r\"\\bMICI\\b\",\n        r\"\\bMNAI\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nnamed_disease = dict(\n    source=\"named_disease\",\n    regex=[\n        r\"libman.?lack\",\n        r\"\\bstill\",\n        r\"felty\",\n        r\"forestier.?certon\",\n        r\"gou(g|j)erot\",\n        r\"raynaud\",\n        r\"thibierge.?weiss\",\n        r\"sjogren\",\n        r\"gou(g|j)erot.?sjogren\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    lupus,\n    lupique,\n    acronym,\n    named_disease,\n]\n\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/disorders/connective-tissue-disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"pipes/ner/disorders/connective-tissue-disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.connective_tissue_disease())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Pr\u00e9sence d'une scl\u00e9rodermie.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [scl\u00e9rodermie]\n</code></pre> <pre><code>text = \"Patient atteint d'un lupus.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [lupus]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'anticoagulants lupiques,\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Il y a une MICI.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [MICI]\n</code></pre> <pre><code>text = \"Syndrome de Raynaud\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [Raynaud]\n</code></pre>"},{"location":"pipes/ner/disorders/connective-tissue-disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'connective_tissue_disease'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['arth?rites?.{1,5...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>connective_tissue_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'connective_tissue_disease': True}</code> </p>"},{"location":"pipes/ner/disorders/connective-tissue-disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.connective_tissue_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/disorders/copd/","title":"COPD","text":"<p>The <code>eds.copd</code> pipeline component extracts mentions of COPD (Chronic obstructive pulmonary disease). It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Pulmonary hypertension</li> <li>Long-term oxygen therapy</li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/disorders/copd/#edsnlp.pipes.ner.disorders.copd.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"pipes/ner/disorders/copd/#edsnlp.pipes.ner.disorders.copd.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.copd())\n</code></pre> <p>Below are a few examples:</p> 123456 <pre><code>text = \"Une fibrose interstitielle diffuse idiopathique\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [fibrose interstitielle diffuse idiopathique]\n</code></pre> <pre><code>text = \"Patient atteint de pneumoconiose\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [pneumoconiose]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une HTAP.\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [HTAP]\n</code></pre> <pre><code>text = \"On voit une hypertension pulmonaire minime\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"La patiente a \u00e9t\u00e9 mis sous oxyg\u00e9norequ\u00e9rance\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"La patiente est sous oxyg\u00e9norequ\u00e9rance au long cours\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [oxyg\u00e9norequ\u00e9rance au long cours]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'long': [long cours]}\n</code></pre>"},{"location":"pipes/ner/disorders/copd/#edsnlp.pipes.ner.disorders.copd.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'copd'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['alveolites?.{1,5...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>copd</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'copd': True}</code> </p>"},{"location":"pipes/ner/disorders/copd/#edsnlp.pipes.ner.disorders.copd.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.copd</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/disorders/dementia/","title":"Dementia","text":"<p>The <code>eds.dementia</code> pipeline component extracts mentions of dementia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"demence\",\n        r\"demense\",\n        r\"dementiel\",\n        r\"corps\\s*de\\s*le[vw]y\",\n        r\"deficits?.chroniques?.cognitifs?\",\n        r\"troubles?.mnesique?\",\n        r\"troubles?.praxique\",\n        r\"troubles?.att?entionel\",\n        r\"troubles?.degeneratifs?.{1,15}fonctions.{1,5}sup\",\n        r\"maladies?.cerebrales?.degen\",\n        r\"troubles?.neurocogn\\w+\",\n        r\"deficits?.cogniti\\w+\",\n        r\"atteinte.{1,7}spheres?cogniti\",\n        r\"syndrome?.{1,10}(frontal|neuro.deg)\",\n        r\"(trouble|d(y|i)sfonction).{1,25}cogni\\w+\",\n        r\"(?&lt;!specialisee)alzheimer\",\n        r\"demence.{1,20}(\\balz|\\bpark)\",\n        r\"binswanger\",\n        r\"gehring\",\n        r\"\\bpick\",\n        r\"de\\s*guam\",\n        r\"[kc]reutzfeld.{1,5}ja[ck]ob\",\n        r\"huntington\",\n        r\"korsako[fv]\",\n        r\"atrophie.{1,10}(cortico|hip?pocamp|cereb|lobe)\",\n    ],\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bSLA\\b\",\n        r\"\\bDFT\\b\",\n        r\"\\bDFT\",\n        r\"\\bTNC\\b\",\n        r\"\\bHTT\\b\",\n        r\"\\bALS\\b\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=r\"\\banti\",  # anticorps\n        window=-15,\n        regex_attr=\"NORM\",\n    ),\n)\n\ncharcot = dict(\n    source=\"charcot\",\n    regex=[\n        r\"maladie.{1,10}charcot\",\n        r\"maladie.{1,10}lou\\s*gehrig\",\n    ],\n    exclude=dict(\n        regex=[\n            \"pied de\",\n            \"marie.?tooth\",\n        ],\n        window=(-3, 3),\n    ),\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    charcot,\n]\n\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/disorders/dementia/#edsnlp.pipes.ner.disorders.dementia.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"pipes/ner/disorders/dementia/#edsnlp.pipes.ner.disorders.dementia.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.dementia())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"D'importants d\u00e9ficits cognitifs\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [d\u00e9ficits cognitifs]\n</code></pre> <pre><code>text = \"Patient atteint de d\u00e9mence\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [d\u00e9mence]\n</code></pre> <pre><code>text = \"On retrouve des anti-SLA\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une maladie de Charcot\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [maladie de Charcot]\n</code></pre>"},{"location":"pipes/ner/disorders/dementia/#edsnlp.pipes.ner.disorders.dementia.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'dementia'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['demence', 'demen...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>dementia</code> </p> <code>span_setter</code> <p>The span setter to use</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'dementia': True}</code> </p>"},{"location":"pipes/ner/disorders/dementia/#edsnlp.pipes.ner.disorders.dementia.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.dementia</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/disorders/diabetes/","title":"Diabetes","text":"<p>The <code>eds.diabetes</code> pipeline component extracts mentions of diabetes.</p> Details of the used patterns <pre><code># fmt: off\nCOMPLICATIONS = [\n    r\"nephropat\",\n    r\"neuropat\",\n    r\"retinopat\",\n    r\"glomerulopathi\",\n    r\"glomeruloscleros\",\n    r\"angiopathi\",\n    r\"origine\",\n]\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"\\bds?n?id\\b\",\n        r\"\\bdiabet[^o]\",\n        r\"\\bdiab\",\n        r\"\\bdb\\b\",\n        r\"\\bdt.?(i|ii|1|2)\\b\",\n    ],\n    exclude=dict(\n        regex=[\n            \"insipide\",\n            \"nephrogenique\",\n            \"aigu\",\n            r\"\\bdr\\b\",  # Dr. ...\n            \"endocrino\",  # Section title\n            \"soins aux pieds\",  # Section title\n            \"nutrition\",  # Section title\n            r\"\\s?:\\n+\\W+(?!oui|non|\\W)\",  # General pattern for section title\n        ],\n        window=(-5, 5),\n    ),\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"complicated_before\",\n            regex=r\"(\" + r\"|\".join(COMPLICATIONS + [\"origine\"]) + r\")\",\n            window=-3,\n        ),\n        dict(\n            name=\"complicated_after\",\n            regex=r\"(\"\n            + r\"|\".join([r\"(?&lt;!sans )compli\", r\"(?&lt;!a)symptomatique\"] + COMPLICATIONS)\n            + r\")\",\n            window=12,\n        ),\n        dict(\n            name=\"type\",\n            regex=r\"type.?\\s*(ii|i|1|2)\",\n            window=6,\n        ),\n        dict(\n            name=\"insulin\",\n            regex=r\"insulino.?(dep|req)\",\n            window=6,\n        ),\n        dict(\n            name=\"corticoid\",\n            regex=r\"(\\bctc\\b|cortico(?:.?induit)?)\",\n            window=6,\n        ),\n    ],\n)\n\ncomplicated_pattern = dict(\n    source=\"complicated\",\n    regex=[\n        r\"(mal|maux).perforants?(.plantaire)?\",\n        r\"pieds? diabeti\",\n    ],\n    exclude=dict(\n        regex=\"soins aux\",  # Section title\n        window=-2,\n    ),\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    complicated_pattern,\n]\n\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/disorders/diabetes/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"WITH_COMPLICATION\"</code> if the diabetes is  complicated (e.g., via organ    damages)</li> <li><code>\"WITHOUT_COMPLICATION\"</code> otherwise</li> </ul> </li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>type</code>: type of diabetes (I or II)</li> <li><code>insulin</code>: if the diabetes is insulin-dependent</li> <li><code>corticoid</code>: if the diabetes is corticoid-induced</li> </ul> </li> </ul>"},{"location":"pipes/ner/disorders/diabetes/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.diabetes())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Pr\u00e9sence d'un DT2\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [DT2]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un DNID\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [DNID]\n</code></pre> <pre><code>text = \"Patient diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [diab\u00e9tique]\n</code></pre> <pre><code>text = \"Un diab\u00e8te insipide\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Atteinte neurologique d'origine diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [origine diab\u00e9tique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n\nspan._.assigned\n# Out: {'complicated_before': [origine]}\n</code></pre> <pre><code>text = \"Une r\u00e9tinopathie diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [r\u00e9tinopathie diab\u00e9tique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n\nspan._.assigned\n# Out: {'complicated_before': [r\u00e9tinopathie]}\n</code></pre> <pre><code>text = \"Il y a un mal perforant plantaire\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [mal perforant plantaire]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n</code></pre>"},{"location":"pipes/ner/disorders/diabetes/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['\\\\bds?n?id\\\\b', ...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>diabetes</code> </p> <code>span_setter</code> <p>The span setter to use</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'diabetes': True}</code> </p>"},{"location":"pipes/ner/disorders/diabetes/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.diabetes</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/disorders/hemiplegia/","title":"Hemiplegia","text":"<p>The <code>eds.hemiplegia</code> pipeline component extracts mentions of hemiplegia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"hemipleg\\w+\",\n        r\"tetrapleg\\w+\",\n        r\"quadripleg\\w+\",\n        r\"parapleg\\w+\",\n        r\"neuropath\\w+.{1,25}motrice.{1,30}type\\s*[5V]\",\n        r\"charcot.?marie.?tooth\",\n        r\"loc?ked.?in\",\n        r\"syndrome?.{1,5}(enfermement|verrouillage)|(desafferen)\",\n        r\"paralysie.{1,10}hemicorps\",\n        r\"paralysie.{1,10}jambe\",\n        r\"paralysie.{1,10}membre\",\n        r\"paralysie.{1,10}cote\",\n        r\"paralysie.{1,5}cerebrale.{1,5}spastique\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLIS\\b\",\n        r\"\\bNMSH\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n]\n\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/disorders/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"pipes/ner/disorders/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.hemiplegia())\n</code></pre> <p>Below are a few examples:</p> 123 <pre><code>text = \"Patient h\u00e9mipl\u00e9gique\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [h\u00e9mipl\u00e9gique]\n</code></pre> <pre><code>text = \"Paralysie des membres inf\u00e9rieurs\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [Paralysie des membres]\n</code></pre> <pre><code>text = \"Patient en LIS\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [LIS]\n</code></pre>"},{"location":"pipes/ner/disorders/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'hemiplegia'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['hemipleg\\\\w+', '...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>hemiplegia</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'hemiplegia': True}</code> </p>"},{"location":"pipes/ner/disorders/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.hemiplegia</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/disorders/leukemia/","title":"Leukemia","text":"<p>The <code>eds.leukemia</code> pipeline component extracts mentions of leukemia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"leucemie?\",\n        r\"(syndrome?.)?myelo\\s*proliferatif\",\n        r\"m[yi]eloprolifer\",\n    ],\n    exclude=dict(\n        regex=[\n            \"plasmocyte\",\n            \"benin\",\n            \"benign\",\n        ],\n        window=5,\n    ),\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLAM\\b\",\n        r\"\\bLAM.?[0-9]\",\n        r\"\\bLAL\\b\",\n        r\"\\bLMC\\b\",\n        r\"\\bLCE\\b\",\n        r\"\\bLMM[JC]\\b\",\n        r\"\\bLCN\\b\",\n        r\"\\bAREB\\b\",\n        r\"\\bAPMF\\b\",\n        r\"\\bLLC\\b\",\n        r\"\\bSMD\\b\",\n        r\"LA my[\u00e9\u00e8e]lomonocytaire\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=\"anti\",\n        window=-20,\n    ),\n)\n\nother = dict(\n    source=\"other\",\n    regex=[\n        r\"myelofibrose\",\n        r\"vaquez\",\n        r\"thrombocytem\\w+.{1,3}essentiell?e?\",\n        r\"splenomegal\\w+.{1,3}myeloide\",\n        r\"mastocytose.{1,5}maligne?\",\n        r\"polyglobul\\w+.{1,10}essentiell?e?\",\n        r\"letterer.?siwe\",\n        r\"anemie.refractaire.{1,20}blaste\",\n        r\"m[iy]elod[iy]splasi\",\n        r\"syndrome.myelo.?dysplasique\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    other,\n]\n\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/disorders/leukemia/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"pipes/ner/disorders/leukemia/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.leukemia())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Sydrome my\u00e9loprolif\u00e9ratif\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [my\u00e9loprolif\u00e9ratif]\n</code></pre> <pre><code>text = \"Sydrome my\u00e9loprolif\u00e9ratif b\u00e9nin\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Patient atteint d'une LAM\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [LAM]\n</code></pre> <pre><code>text = \"Une maladie de Vaquez\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [Vaquez]\n</code></pre>"},{"location":"pipes/ner/disorders/leukemia/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'leukemia'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['leucemie?', '(sy...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>leukemia</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'leukemia': True}</code> </p>"},{"location":"pipes/ner/disorders/leukemia/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.leukemia</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/disorders/liver-disease/","title":"Liver disease","text":"<p>The <code>eds.liver_disease</code> pipeline component extracts mentions of liver disease.</p> Details of the used patterns <pre><code># fmt: off\nmild = dict(\n    source=\"mild\",\n    regex=[\n        r\"cholangites?.{1,10}(sclero|secondaire)\",\n        r\"fibrose.{1,10}(hepatique|foie)\",\n        r\"hepatite.{1,15}chroni\\w+\",\n        r\"hepatopath\\w+\",\n        r\"\\bnash\\b\",\n        r\"(maladie|sydrome?).{1,10}hanot\",\n        r\"surinfections?.{1,5}delta\",\n        r\"\\bcbp\\b\",\n        r\"\\bmaf\\b\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=\"\\bdots?\\b\",\n        window=-5,\n    ),\n)\n\nmoderate_severe = dict(\n    source=\"moderate_severe\",\n    regex=[\n        r\"cirr?hose\",\n        r\"necrose.{1,10}(hepati|foie)\",\n        r\"varice.{1,10}(estomac|oesopha|gastr)\",\n        r\"\\bvo\\b.{1,5}(stade|grade).(1|2|3|i{1,3})\",\n        r\"hypertension.{1,5}portale?\",\n        r\"scleroses?.{1,5}hepato\\s*portale?\",\n        r\"sydrome?.{1,10}hepato.?ren\",\n        r\"insuff?isance.{1,5}hepa\",\n        r\"encephalopath\\w+.{1,5}hepa\",\n        r\"\\btips\\b\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ntransplant = dict(\n    source=\"transplant\",\n    regex=[\n        r\"(?&lt;!pre.?)(gref?fe|transplant).{1,12}(hepatique|foie)\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=\"chc\",\n        window=(-5, 5),\n    ),\n)\n\ndefault_patterns = [\n    mild,\n    moderate_severe,\n    transplant,\n]\n\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/disorders/liver-disease/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"MILD\"</code> for mild liver diseases</li> <li><code>\"MODERATE_TO_SEVERE\"</code> else</li> </ul> </li> </ul>"},{"location":"pipes/ner/disorders/liver-disease/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.liver_disease())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Il y a une fibrose h\u00e9patique\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [fibrose h\u00e9patique]\n</code></pre> <pre><code>text = \"Une h\u00e9patite B chronique\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [h\u00e9patite B chronique]\n</code></pre> <pre><code>text = \"Le patient consulte pour une cirrhose\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [cirrhose]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: MODERATE_TO_SEVERE\n</code></pre> <pre><code>text = \"Greffe h\u00e9patique.\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [Greffe h\u00e9patique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: MODERATE_TO_SEVERE\n</code></pre>"},{"location":"pipes/ner/disorders/liver-disease/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'mild', 'regex': ['cholangites?.{1,...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>liver_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'liver_disease': True}</code> </p>"},{"location":"pipes/ner/disorders/liver-disease/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.liver_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/disorders/lymphoma/","title":"Lymphoma","text":"<p>The <code>eds.lymphoma</code> pipeline component extracts mentions of lymphoma.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"lymphom(?:.{1,10}hodgkin)\",\n        r\"lymphom\",\n        r\"lymphangio\",\n        r\"sezary\",\n        r\"burkitt?\",\n        r\"kaposi\",\n        r\"hodgkin\",\n        r\"amylose\",\n        r\"plasm[ao]cytome\",\n        r\"lympho.{1,3}sarcome\",\n        r\"lympho.?prolif\",\n        r\"hemopathie.{1,10}lymphoide\",\n        r\"macroglobulinemie\",\n        r\"imm?unocytome\",\n        r\"maladie.des.chaines?\",\n        r\"histi?ocytose.{1,5}(maligne|langerhans?)\",\n        r\"waldenst(ro|or)m\",\n        r\"mycos.{1,10}fongoide\",\n        r\"myelome\",\n        r\"maladie.{1,5}imm?uno\\s*proliferative.{1,5}maligne\",\n        r\"leucemie.{1,10}plasmocyte\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLNH\\b\",\n        r\"\\bLH\\b\",\n        r\"\\bEATL\\b\",\n        r\"\\bLAGC\\b\",\n        r\"\\bLDGCB\\b\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=[\"/L\", \"/mL\"],\n        window=10,\n    ),\n)\n\n\ngammapathy = dict(\n    source=\"gammapathy\",\n    regex=[\n        r\"gam?mapath\\w+\\s*monoclonale\",\n    ],\n    exclude=dict(\n        regex=[\n            \"benin\",\n            \"benign\",\n            \"signification.indeter\",\n            \"NMSI\",\n            \"MGUS\",\n        ],\n        window=(0, 5),\n    ),\n    regex_attr=\"NORM\",\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    # gammapathy,\n]\n\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/disorders/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul> <p>Monoclonal gammapathy</p> <p>Monoclonal gammapathies are not extracted by this pipeline</p>"},{"location":"pipes/ner/disorders/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.lymphoma())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Un lymphome de Hodgkin.\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [lymphome de Hodgkin]\n</code></pre> <pre><code>text = \"Atteint d'un Waldenst\u00f6rm\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [Waldenst\u00f6rm]\n</code></pre> <pre><code>text = \"Un LAGC\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [LAGC]\n</code></pre> <pre><code>text = \"anti LAGC: 10^4/mL\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"pipes/ner/disorders/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['lymphom(?:.{1,10...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>lymphoma</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'lymphoma': True}</code> </p>"},{"location":"pipes/ner/disorders/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.lymphoma</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/disorders/myocardial-infarction/","title":"Myocardial infarction","text":"<p>The <code>eds.myocardial_infarction</code> pipeline component extracts mentions of myocardial infarction. It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Mentions of stents with a heart localization</li> </ul> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import HEART\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"coronaropath\\w+\",  # changed\n        r\"angor.{1,5}instable\",\n        r\"cardiopathie(?!.{0,20}non).{0,20}(ischem|arteriosc)\",\n        r\"cardio.?myopathie(?!.{0,20}non).{0,20}(ischem|arteriosc)\",\n        r\"ischemi.{1,15}myocard\",\n        r\"syndrome?.{1,5}corona.{1,10}aigu\",  # changed\n        r\"syndrome?.{1,5}corona.{1,10}st\",  # changed\n        r\"pontage.{1,5}mammaire\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"\\bstent\",\n        r\"endoprothese\",\n        r\"pontage\",\n        r\"anevr[iy]sme\",\n        r\"infa?r?a?ctus\",  # changed\n        r\"angioplast\\w+\",  # changed\n    ],\n    assign=[\n        dict(\n            name=\"heart_localized\",\n            regex=\"(\" + r\"|\".join(HEART) + \")\",\n            window=(-10, 10),\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bidm\\b\",\n        r\"\\bsca\\b\",\n        r\"\\batl\\b\",\n    ],\n    regex_attr=\"NORM\",\n    assign=dict(\n        name=\"segment\",\n        regex=r\"st([+-])\",\n        window=2,\n    ),\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    with_localization,\n    acronym,\n]\n\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/disorders/myocardial-infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>heart_localized</code>: localization of the stent or bypass</li> </ul> </li> </ul>"},{"location":"pipes/ner/disorders/myocardial-infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.myocardial_infarction())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Une cardiopathie isch\u00e9mique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [cardiopathie isch\u00e9mique]\n</code></pre> <pre><code>text = \"Une cardiopathie non-isch\u00e9mique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un stent sur la marginale\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [stent sur la marginale]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'heart_localized': [marginale]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un stent p\u00e9riph\u00e9rique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"infarctus du myocarde\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [infarctus du myocarde]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'heart_localized': [myocarde]}\n</code></pre>"},{"location":"pipes/ner/disorders/myocardial-infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'myocardial_infarction'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['coronaropath\\\\w+...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>myocardial_infarction</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'myocardial_infarction': True}</code> </p>"},{"location":"pipes/ner/disorders/myocardial-infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.myocardial_infarction</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/disorders/peptic-ulcer-disease/","title":"Peptic ulcer disease","text":"<p>The <code>eds.peptic_ulcer_disease</code> pipeline component extracts mentions of peptic ulcer disease.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"ulcere?.{1,10}gastr\",\n        r\"ulcere?.{1,10}duoden\",\n        r\"ulcere?.{1,10}antra\",\n        r\"ulcere?.{1,10}pept\",\n        r\"ulcere?.{1,10}estomac?\",\n        r\"ulcere?.{1,10}curling\",\n        r\"ulcere?.{1,10}bulb\",\n        r\"(\u0153|oe)sophagites?.{1,5}pepti.{1,10}ulcer\",\n        r\"gastrite.{1,20}ulcer\",\n        r\"antrite.{1,5}ulcer\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bUGD\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ngeneric = dict(\n    source=\"generic\",\n    regex=r\"ulcere?\",\n    regex_attr=\"NORM\",\n    assign=dict(\n        name=\"is_peptic\",\n        regex=r\"\\b(gastr|digest)\",\n        window=(-20, 20),\n        limit_to_sentence=False,\n    ),\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    generic,\n]\n\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/disorders/peptic-ulcer-disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that matches, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"pipes/ner/disorders/peptic-ulcer-disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.peptic_ulcer_disease())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Beaucoup d'ulc\u00e8res gastriques\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [ulc\u00e8res gastriques]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'UGD\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [UGD]\n</code></pre> <pre><code>text = \"La patient \u00e0 des ulc\u00e8res\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Au niveau gastrique: blabla blabla blabla blabla blabla quelques ulc\u00e8res\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [gastrique: blabla blabla blabla blabla blabla quelques ulc\u00e8res]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'is_peptic': [gastrique]}\n</code></pre>"},{"location":"pipes/ner/disorders/peptic-ulcer-disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['ulcere?.{1,10}ga...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>peptic_ulcer_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'peptic_ulcer_disease': True}</code> </p>"},{"location":"pipes/ner/disorders/peptic-ulcer-disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.peptic_ulcer_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/disorders/peripheral-vascular-disease/","title":"Peripheral vascular disease","text":"<p>The <code>eds.peripheral_vascular_disease</code> pipeline component extracts mentions of peripheral vascular disease.</p> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import ASYMPTOMATIC, BRAIN, HEART, PERIPHERAL\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bAOMI\\b\",\n        r\"\\bACOM\\b\",\n        r\"\\bTAO\\b\",\n        r\"\\bSAPL\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n        r\"\\bSCS\\b\",\n        r\"\\bTVP\\b\",\n        r\"\\bCAPS\\b\",\n        r\"\\bMTEV\\b\",\n        r\"\\bPTT\\b\",\n        r\"\\bMAT\\b\",\n        r\"\\bSHU\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nother = dict(\n    source=\"other\",\n    regex=[\n        r\"\\bbuerger\",\n        r\"takayasu\",\n        r\"\\bhorton\",\n        r\"wegener\",\n        r\"churg.{1,10}strauss\",\n        r\"\\bsnedd?on\",\n        r\"budd.chiari\",\n        r\"infa?r?a?ctus.{1,5}(renal|spleni\\w+|polaire|pulmo)\",\n        r\"ulcere?.{1,5}arter\",\n        r\"syndrome?.?hemolytique.{1,8}uremi\\w+\",\n        r\"granulomatose.{1,10}polyangeite\",\n        r\"occlusion.{1,10}(artere?|veine).{1,20}retine\",\n        r\"syndrome?.{1,20}anti.?phospho\",\n        r\"embolie.{1,5}pulm\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"angiopath\\w+\",\n        r\"arteriopathies?.{1,5}obliterante?\",\n        r\"gangren\",\n        r\"claudication\",\n        r\"dissection.{1,10}(aort|arter)\",\n        r\"tromboangeit\",\n        r\"tromboarterit\",\n        r\"(pontage|angioplastie).{1,10}(\\bfem|\\bpop|\\bren|\\bjamb)\",\n        r\"arterite\",\n        r\"(ischemie|infa?r?a?ctus).{1,10}mesenteri\\w+\",\n        r\"endarteriectom\\w+\",\n        r\"vascularite\",\n        r\"occlusion.{1,10}terminaisons?\\s*carotid\",\n        r\"cryoglobulinemie\",\n        r\"colite.{1,5}ischemi\",\n        r\"embole.{1,10}cholesterol\",\n        r\"purpura.?thrombopenique.?idiopa\",\n        r\"micro.?angiopathie.?th?rombotique\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART + ASYMPTOMATIC + [r\"inr\\srecommande\\ssous\\savk\"],\n            window=(-8, 8),\n            limit_to_sentence=False,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nthrombosis = dict(\n    source=\"thrombosis\",\n    regex=[\n        r\"thrombos\",\n        r\"thrombol[^y]\",\n        r\"thrombophi\",\n        r\"thrombi[^n]\",\n        r\"thrombus\",\n        r\"thrombectomi\",\n        r\"thrombo.?embo\",\n        r\"phlebit\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART + [\"superficiel\", \"\\biv\\b\", \"intra.?vein\"],\n            window=(-15, 15),\n            limit_to_sentence=False,\n        ),\n        dict(\n            regex=[\n                \"pre\",\n                \"anti\",\n                \"bilan\",\n            ],\n            window=-4,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\n\nischemia = dict(\n    source=\"ischemia\",\n    regex=[\n        r\"ischemi\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART,\n            window=(-7, 7),\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"peripheral\",\n            regex=\"(\" + r\"|\".join(PERIPHERAL) + \")\",\n            window=15,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nep = dict(\n    source=\"ep\",\n    regex=r\"\\bEP(?![\\w\\./-])\",\n    regex_attr=\"TEXT\",\n    exclude=[\n        dict(\n            regex=[\n                r\"fibreux\",\n                r\"retin\",\n                r\"\\bfove\",\n                r\"\\boct\\b\",\n                r\"\\bmacula\",\n                r\"prosta\",\n                r\"\\bip\\b\",\n                r\"protocole\",\n                r\"seance\",\n                r\"echange\",\n                r\"ritux\",\n                r\"ivig\",\n                r\"ig.?iv\",\n                r\"\\bctc\",\n                r\"corticoide\",\n                r\"serum\",\n                r\"\\bcure\",\n                r\"plasma\",\n                r\"mensuel\",\n                r\"semaine\",\n                r\"serologi\",\n                r\"espaces.porte\",\n                r\"projet\",\n                r\"bolus\",\n            ],\n            window=(-25, 25),\n            limit_to_sentence=False,\n            regex_attr=\"NORM\",\n        ),\n        dict(\n            regex=[r\"rdv\", r\"les\", r\"des\", r\"angine\"],\n            window=(-3, 0),\n            regex_attr=\"NORM\",\n        ),\n    ],\n)\n\nhypertension = dict(\n    source=\"main\",\n    regex=[\n        r\"\\bhta\\b\",\n        r\"hyper.?tension.?arte\",\n        r\"hyper.?tendu\",\n        r\"hyper.?tension.?essenti\",\n        r\"hypertensi\",\n    ],\n    exclude=dict(\n        regex=\"(pulmo|porta)\",\n        window=3,\n    ),\n)\n\ndefault_patterns = [\n    acronym,\n    other,\n    with_localization,\n    thrombosis,\n    ep,\n    ischemia,\n    hypertension,\n]\n\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"pipes/ner/disorders/peripheral-vascular-disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"pipes/ner/disorders/peripheral-vascular-disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.peripheral_vascular_disease())\n</code></pre> <p>Below are a few examples:</p> 12345678910111213 <pre><code>text = \"Un AOMI\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [AOMI]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un infarctus r\u00e9nal\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [infarctus r\u00e9nal]\n</code></pre> <pre><code>text = \"Une angiopathie c\u00e9r\u00e9brale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une angiopathie\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [angiopathie]\n</code></pre> <pre><code>text = \"Une thrombose c\u00e9r\u00e9brale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une thrombose des veines superficielles\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une thrombose\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [thrombose]\n</code></pre> <pre><code>text = \"Effectuer un bilan pre-trombose\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une isch\u00e9mie des MI est remarqu\u00e9e.\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [isch\u00e9mie des MI]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'peripheral': [MI]}\n</code></pre> <pre><code>text = \"Plusieurs cas d'EP\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [EP]\n</code></pre> <pre><code>text = \"Effectuer des cures d'EP\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Le patient est hypertendu\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [hypertendu]\n</code></pre> <pre><code>text = \"Une hypertension portale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"pipes/ner/disorders/peripheral-vascular-disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'acronym', 'regex': ['\\\\bAOMI\\\\b', ...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>peripheral_vascular_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'peripheral_vascular_disease': T...</code> </p>"},{"location":"pipes/ner/disorders/peripheral-vascular-disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.peripheral_vascular_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"pipes/ner/disorders/solid-tumor/","title":"Solid tumor","text":"<p>The <code>eds.solid_tumor</code> pipeline component extracts mentions of solid tumors. It will notably match:</p> Details of the used patterns <pre><code># fmt: off\nBENINE = r\"benign|benin|(grade.?\\b[i1]\\b)\"\nSTAGE = r\"stade ([^\\s]*)\"\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"carcinom(?!.{0,10}in.?situ)\",\n        r\"seminome\",\n        r\"(?&lt;!lympho)(?&lt;!lympho-)sarcome\",\n        r\"blastome\",\n        r\"cancer([^o]|\\s|\\b)\",\n        r\"adamantinome\",\n        r\"chordome\",\n        r\"cranio\\s*pharyngiome\",\n        r\"melanome\",\n        r\"neoplasie\",\n        r\"neoplasme\",\n        r\"linite\",\n        r\"melanome\",\n        r\"mesoth?eliome\",\n        # r\"mesotheliome\", #removed same as above\n        # r\"seminome\", #removed same as above\n        r\"myxome\",\n        r\"paragangliome\",\n        # r\"craniopharyngiome\",  #removed same as above\n        r\"k\\s*.{0,5}(prostate|sein)\",\n        r\"pancoast.?tobias\",\n        r\"syndrome?.{1,10}lynch\",\n        r\"li.?fraumeni\",\n        r\"germinome\",\n        r\"adeno[\\s-]?k\",\n        r\"thymome\",\n        r\"\\bnut\\b\",\n        r\"\\bgist\\b\",\n        r\"\\bchc\\b\",\n        r\"\\badk\\b\",\n        r\"\\btves\\b\",\n        r\"\\btv.tves\\b\",\n        r\"lesion.{1,20}tumor\",\n        r\"tumeur\",\n        r\"carcinoid\",\n        r\"histiocytome\",\n        r\"ependymome\",\n        # r\"primitif\", Trop de FP\n    ],\n    exclude=dict(\n        regex=BENINE,\n        window=(0, 5),\n    ),\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"metastasis\",\n            regex=r\"(metasta|multinodul)\",\n            window=(-3, 7),\n            reduce_mode=\"keep_last\",\n        ),\n        dict(\n            name=\"stage\",\n            regex=STAGE,\n            window=7,\n            reduce_mode=\"keep_last\",\n        ),\n    ],\n)\n\nmetastasis_pattern = dict(\n    source=\"metastasis\",\n    regex=[\n        r\"cellule.{1,5}tumorale.{1,5}circulantes\",\n        r\"metasta\",\n        r\"multinodul\",\n        r\"carcinose\",\n        r\"ruptures?.{1,5}corticale\",\n        r\"envahissement.{0,15}parties\\s*molle\",\n        r\"(localisation|lesion)s?.{0,20}second\",\n        r\"(lymphangite|meningite).{1,5}carcinomateuse\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=r\"goitre\",\n        window=-3,\n    ),\n)\n\n# Patterns developed for CT-Scan reports\nmetastasis_ct_scan = dict(\n    source=\"metastasis_ct_scan\",\n    regex=[\n        r\"(?i)(m[\u00e9e]tasta(se|tique)s?)\",\n        r\"(diss[\u00e9e]min[\u00e9e]e?s?)\",\n        r\"(carcinose)\",\n        r\"(((allure|l[\u00e9e]sion|localisation|progression)s?\\s)(suspecte?s?)?.{0,50}(secondaire)s?)\",\n        r\"(l(a|\u00e2)ch(\u00e9|e|er)\\sde\\sballons?)\",\n        r\"(l[\u00e9e]sions?\\s(non\\s)?cibles?)\",\n        r\"(rupture.{1,20}corticale)\",\n        r\"(envahissement.{0,15}parties\\smolles)\",\n        r\"((l[i,y]se).{1,20}os)|ost[e\u00e9]ol[i,y]|rupture.{1,20}corticale|envahissement.{1,20}parties\\smolles|ost[e\u00e9]ocondensa.{1,20}(suspect|secondaire|[\u00e9e]volutive)\",\n        r\"(l[\u00e9e]sion|anomalie|image).{1,20}os.{1,30}(suspect|secondaire|[\u00e9e]volutive)\",\n        r\"os.{1,30}(l[\u00e9e]sion|anomalie|image).{1,20}(suspect|secondaire|[\u00e9e]volutive)\",\n        r\"(l[\u00e9e]sion|anomalie|image).{1,20}l[i,y]tique\",\n        r\"(l[\u00e9e]sion|anomalie|image).{1,20}condensant.{1,20}(suspect|secondaire|[\u00e9e]volutive)\",\n        r\"fracture.{1,30}(suspect|secondaire|[\u00e9e]volutive)\",\n        r\"((l[\u00e9e]sion|anomalie|image|nodule).{1,80}(secondaire))\",\n        r\"((l[\u00e9e]sion|anomalie|image|nodule)s.{1,40}suspec?ts?)\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    metastasis_pattern,\n]\n\n# fmt: on\n</code></pre> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li><li><p><p>Kempf E., Priou S., Lam\u00e9 G., Daniel C., Bellamine A., Sommacale D., Belkacemi y., Bey R., Galula G., Taright N., Tannier X., Rance B., Flicoteaux R., Hemery F., Audureau E., Chatellier G. and Tournigand C., 2022. Impact of two waves of Sars-Cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: A French multicentric cohort study from a large group of University hospitals. {International Journal of Cancer}. 150, pp.1609-1618. 10.1002/ijc.33928</p></p></li></ol>"},{"location":"pipes/ner/disorders/solid-tumor/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"METASTASIS\"</code> for tumors at the metastatic stage</li> <li><code>\"LOCALIZED\"</code> else</li> </ul> </li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>stage</code>: stage of the tumor</li> </ul> </li> </ul>"},{"location":"pipes/ner/disorders/solid-tumor/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.solid_tumor())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Pr\u00e9sence d'un carcinome intra-h\u00e9patique.\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [carcinome]\n</code></pre> <pre><code>text = \"Patient avec un K sein.\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [K sein]\n</code></pre> <pre><code>text = \"Il y a une tumeur b\u00e9nigne\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Tumeur m\u00e9tastas\u00e9e\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Tumeur m\u00e9tastas\u00e9e]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n\nspan._.assigned\n# Out: {'metastasis': m\u00e9tastas\u00e9e}\n</code></pre> <pre><code>text = \"Cancer du poumon au stade 4\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Cancer du poumon au stade 4]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n\nspan._.assigned\n# Out: {'stage': 4}\n</code></pre> <pre><code>text = \"Cancer du poumon au stade 2\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Cancer du poumon au stade 2]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': 2}\n</code></pre> <pre><code>text = \"Pr\u00e9sence de nombreuses l\u00e9sions secondaires\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [l\u00e9sions secondaires]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n</code></pre>"},{"location":"pipes/ner/disorders/solid-tumor/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'solid_tumor'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['carcinom(?!.{0,1...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>solid_tumor</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'solid_tumor': True}</code> </p> <code>use_tnm</code> <p>Whether to use TNM scores matching as well</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_patterns_metastasis_ct_scan</code> <p>Whether to use the metastasis patterns developed for the CT-Scans</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"pipes/ner/disorders/solid-tumor/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.solid_tumor</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024 and Kempf et al., 2022.</p>"},{"location":"pipes/ner/scores/","title":"Scores Overview","text":"<p>EDS-NLP provides multiple matchers for typical scores (Charlson, SOFA...) found in clinical documents. To extract a score, the matcher:</p> <ul> <li>extracts the score's name via the provided regular expressions</li> <li>extracts the score's raw value via another set of RegEx</li> <li>normalize the score's value via a normalising function</li> </ul>"},{"location":"pipes/ner/scores/#available-scores","title":"Available scores","text":"Component Description <code>eds.charlson</code> A Charlson score extractor <code>eds.emergency_ccmu</code> A CCMU score extractor <code>eds.emergency_gemsa</code> A GEMSA score extractor <code>eds.emergency_priority</code> A priority score extractor <code>eds.sofa</code> A SOFA score extractor <code>eds.tnm</code> A TNM score extractor"},{"location":"pipes/ner/scores/#implementing-your-own-score","title":"Implementing your own score","text":"<p>Using the <code>eds.score</code> pipeline, you only have to change its configuration in order to implement a simple score extraction algorithm. As an example, let us see the configuration used for the <code>eds.charlson</code> pipe The configuration consists of 4 items:</p> <ul> <li><code>score_name</code>: The name of the score</li> <li><code>regex</code>: A list of regular expression to detect the score's mention</li> <li><code>value_extract</code>: A regular expression to extract the score's value in the context of the score's mention</li> <li><code>score_normalization</code>: A function name used to normalise the score's raw value</li> </ul> <p>Note</p> <p>Functions passed as parameters to components need to be registered as follow</p> <pre><code>import spacy\n\n\n@spacy.registry.misc(\"score_normalization.charlson\")\ndef my_normalization_score(raw_score: str):\n    # Implement some filtering here\n    # Return None if you want the score to be discarded\n    return normalized_score\n</code></pre> <p>The values used for the <code>eds.charlson</code> pipe are the following:</p> <pre><code>import spacy\n\n\n@spacy.registry.misc(\"score_normalization.charlson\")\ndef score_normalization(extracted_score):\n\"\"\"\n    Charlson score normalization.\n    If available, returns the integer value of the Charlson score.\n    \"\"\"\n    score_range = list(range(0, 30))\n    if (extracted_score is not None) and (int(extracted_score) in score_range):\n        return int(extracted_score)\n\n\ncharlson_config = dict(\n    score_name=\"charlson\",\n    regex=[r\"charlson\"],\n    value_extract=r\"charlson.*[\\n\\W]*(\\d+)\",\n    score_normalization=\"score_normalization.charlson\",\n)\n</code></pre>"},{"location":"pipes/ner/scores/charlson/","title":"Charlson","text":"<p>The <code>eds.charlson</code> component extracts the Charlson Comorbidity Index.</p>"},{"location":"pipes/ner/scores/charlson/#edsnlp.pipes.ner.scores.charlson.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.charlson())\n\ntext = \"\"\"\nCharlson \u00e0 l'admission: 7.\nCharlson:\nOMS:\n\"\"\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (Charlson \u00e0 l'admission: 7,)\n</code></pre> <p>We can see that only one occurrence was extracted. The second mention of Charlson in the text doesn't contain any numerical value, so it isn't extracted.</p>"},{"location":"pipes/ner/scores/charlson/#edsnlp.pipes.ner.scores.charlson.factory.create_component--extensions","title":"Extensions","text":"<p>Each extraction exposes 2 extensions:</p> <pre><code>ent = doc.ents[0]\n\nent._.score_name\n# Out: 'charlson'\n\nent._.score_value\n# Out: 7\n</code></pre>"},{"location":"pipes/ner/scores/charlson/#edsnlp.pipes.ner.scores.charlson.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'charlson'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'charlson'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'charlson': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"pipes/ner/scores/elston-ellis/","title":"Elston-Ellis","text":"<p>Matcher for the Elston-Ellis score.</p>"},{"location":"pipes/ner/scores/elston-ellis/#edsnlp.pipes.ner.scores.elston_ellis.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.elston_ellis())\n</code></pre>"},{"location":"pipes/ner/scores/elston-ellis/#edsnlp.pipes.ner.scores.elston_ellis.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'elston_ellis'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'TEXT'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'elston_ellis'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'elston_ellis': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"pipes/ner/scores/emergency-ccmu/","title":"Emergency CCMU","text":"<p>Matcher for explicit mentions of the French CCMU emergency score.</p>"},{"location":"pipes/ner/scores/emergency-ccmu/#edsnlp.pipes.ner.scores.emergency.ccmu.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.emergency_ccmu())\n</code></pre>"},{"location":"pipes/ner/scores/emergency-ccmu/#edsnlp.pipes.ner.scores.emergency.ccmu.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'emergency_ccmu'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value otherwise</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_ccmu'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'emergency_ccmu': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"pipes/ner/scores/emergency-gemsa/","title":"Emergency GEMSA","text":"<p>Matcher for explicit mentions of the French GEMSA emergency score.</p>"},{"location":"pipes/ner/scores/emergency-gemsa/#edsnlp.pipes.ner.scores.emergency.gemsa.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.emergency_gemsa())\n</code></pre>"},{"location":"pipes/ner/scores/emergency-gemsa/#edsnlp.pipes.ner.scores.emergency.gemsa.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_gemsa'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value otherwise</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_gemsa'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'emergency_gemsa': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"pipes/ner/scores/emergency-priority/","title":"Emergency Priority","text":"<p>Matcher for explicit mentions of the French priority emergency score.</p>"},{"location":"pipes/ner/scores/emergency-priority/#edsnlp.pipes.ner.scores.emergency.priority.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.emergency_priority())\n</code></pre>"},{"location":"pipes/ner/scores/emergency-priority/#edsnlp.pipes.ner.scores.emergency.priority.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_priority'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_priority'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'emergency_priority': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"pipes/ner/scores/sofa/","title":"SOFA","text":"<p>The <code>eds.sofa</code> component extracts Sequential Organ Failure Assessment (SOFA) scores, used to track a person's status during the stay in an intensive care unit to determine the extent of a person's organ function or rate failure.</p>"},{"location":"pipes/ner/scores/sofa/#edsnlp.pipes.ner.scores.sofa.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sofa())\n\ntext = \"\"\"\nSOFA (\u00e0 24H) : 12.\nOMS:\n\"\"\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (SOFA (\u00e0 24H) : 12,)\n</code></pre>"},{"location":"pipes/ner/scores/sofa/#edsnlp.pipes.ner.scores.sofa.factory.create_component--extensions","title":"Extensions","text":"<p>Each extraction exposes 3 extensions:</p> <pre><code>ent = doc.ents[0]\n\nent._.score_name\n# Out: 'sofa'\n\nent._.score_value\n# Out: 12\n\nent._.score_method\n# Out: '24H'\n</code></pre> <p>Score method can here be \"24H\", \"Maximum\", \"A l'admission\" or \"Non pr\u00e9cis\u00e9e\"</p>"},{"location":"pipes/ner/scores/sofa/#edsnlp.pipes.ner.scores.sofa.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'sofa'</code> </p> <code>regex</code> <p>A list of regexes to identify the SOFA score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('CUSTOM_NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex to extract the score value</p> <p> TYPE: <code>Dict[str, str]</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex, and should return - None if no score could be extracted - The desired score value else</p> <p> TYPE: <code>Callable[[Union[str, None]], Any]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Flags to pass to the regex</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'sofa'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'sofa': True}</code> </p>"},{"location":"pipes/qualifiers/","title":"Qualifier Overview","text":"<p>In EDS-NLP, we call qualifiers the suite of components designed to qualify a pre-extracted entity for a linguistic modality.</p>"},{"location":"pipes/qualifiers/#available-components","title":"Available components","text":"Pipeline Description <code>eds.negation</code> Rule-based negation detection <code>eds.family</code> Rule-based family context detection <code>eds.hypothesis</code> Rule-based speculation detection <code>eds.reported_speech</code> Rule-based reported speech detection <code>eds.history</code> Rule-based medical history detection"},{"location":"pipes/qualifiers/#rationale","title":"Rationale","text":"<p>In a typical medical NLP pipeline, a group of clinicians would define a list of synonyms for a given concept of interest (say, for example, diabetes), and look for that terminology in a corpus of documents.</p> <p>Now, consider the following example:</p> FrenchEnglish <pre><code>Le patient n'est pas diab\u00e9tique.\nLe patient est peut-\u00eatre diab\u00e9tique.\nLe p\u00e8re du patient est diab\u00e9tique.\n</code></pre> <pre><code>The patient is not diabetic.\nThe patient could be diabetic.\nThe patient's father is diabetic.\n</code></pre> <p>There is an obvious problem: none of these examples should lead us to include this particular patient into the cohort.</p> <p>Warning</p> <p>We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library.</p> <p>To curb this issue, EDS-NLP proposes rule-based pipes that qualify entities to help the user make an informed decision about which patient should be included in a real-world data cohort.</p>"},{"location":"pipes/qualifiers/#edsnlp.pipes.base.SpanGetterArg","title":"Where do we get our spans ?","text":"<p>A component get entities from a document by looking up <code>doc.ents</code> or <code>doc.spans[group]</code>. This behavior is set by the <code>span_getter</code> argument in components that support it.</p> <p>Valid values for the <code>span_getter</code> argument of a component can be :</p> <ul> <li>a (doc) -&gt; spans callable</li> <li>a span group name</li> <li>a list of span group names</li> <li>a dict of group name to True or list of labels</li> </ul> <p>The group name <code>\"ents\"</code> is a special case, and will get the matches from <code>doc.ents</code></p>"},{"location":"pipes/qualifiers/#edsnlp.pipes.base.SpanGetterArg--examples","title":"Examples","text":"<ul> <li><code>span_getter=[\"ents\", \"ckd\"]</code> will get the matches from both <code>doc.ents</code> and <code>doc.spans[\"ckd\"]</code>. It is equivalent to <code>{\"ents\": True, \"ckd\": True}</code>.</li> <li><code>span_getter={\"ents\": [\"foo\", \"bar\"]}</code> will get the matches with label \"foo\" and \"bar\" from <code>doc.ents</code>.</li> <li><code>span_getter=\"ents\"</code> will get all matches from <code>doc.ents</code>.</li> <li><code>span_getter=\"ckd\"</code> will get all matches from <code>doc.spans[\"ckd\"]</code>.</li> </ul>"},{"location":"pipes/qualifiers/#under-the-hood","title":"Under the hood","text":"<p>Our qualifier pipes all follow the same basic pattern:</p> <ol> <li> <p>The pipeline extracts cues. We define three (possibly overlapping) kinds :</p> <ul> <li><code>preceding</code>, ie cues that precede modulated entities ;</li> <li><code>following</code>, ie cues that follow modulated entities ;</li> <li>in some cases, <code>verbs</code>, ie verbs that convey a modulation (treated as preceding cues).</li> </ul> </li> <li> <p>The pipeline splits the text between sentences and propositions, using annotations from a sentencizer pipeline and <code>termination</code> patterns, which define syntagma/proposition terminations.</p> </li> <li> <p>For each pre-extracted entity, the pipeline checks whether there is a cue between the start of the syntagma and the start of the entity, or a following cue between the end of the entity and the end of the proposition.</p> </li> </ol> <p>Albeit simple, this algorithm can achieve very good performance depending on the modality. For instance, our <code>eds.negation</code> pipeline reaches 88% F1-score on our dataset.</p> <p>Dealing with pseudo-cues</p> <p>The pipeline can also detect pseudo-cues, ie phrases that contain cues but that are not cues themselves. For instance: <code>sans doute</code>/<code>without doubt</code> contains <code>sans/without</code>, but does not convey negation.</p> <p>Detecting pseudo-cues lets the pipeline filter out any cue that overlaps with a pseudo-cue.</p> <p>Sentence boundaries are required</p> <p>The rule-based algorithm detects cues, and propagate their modulation on the rest of the syntagma. For that reason, a qualifier pipeline needs a sentencizer component to be defined, and will fail otherwise.</p> <p>You may use EDS-NLP's:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\n...\nnlp.add_pipe(eds.sentences())\n</code></pre>"},{"location":"pipes/qualifiers/#persisting-the-results","title":"Persisting the results","text":"<p>Our qualifier pipelines write their results to a custom spaCy extension, defined on both <code>Span</code> and <code>Token</code> objects. We follow the convention of naming said attribute after the pipeline itself, eg <code>Span._.negation</code> for the<code>eds.negation</code> pipeline.</p> <p>We also provide a string representation of the result, computed on the fly by declaring a getter that reads the boolean result of the pipeline. Following spaCy convention, we give this attribute the same name, followed by a <code>_</code>.</p>"},{"location":"pipes/qualifiers/family/","title":"Family Context","text":"<p>The <code>eds.family</code> component uses a simple rule-based algorithm to detect spans that describe a family member (or family history) of the patient rather than the patient themself.</p>"},{"location":"pipes/qualifiers/family/#edsnlp.pipes.qualifiers.family.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks the family context of the extracted entities. It is complete, and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(\n    eds.matcher(terms=dict(douleur=\"douleur\", osteoporose=\"ost\u00e9oporose\")),\n)\nnlp.add_pipe(eds.family())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Il a des ant\u00e9c\u00e9dents familiaux d'ost\u00e9oporose\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, ost\u00e9oporose)\n\ndoc.ents[0]._.family\n# Out: False\n\ndoc.ents[1]._.family\n# Out: True\n</code></pre>"},{"location":"pipes/qualifiers/family/#edsnlp.pipes.qualifiers.family.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.family</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>family</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token relates to a family member.</li> <li>The <code>family_</code> property is a human-readable string, computed from the <code>family</code>    attribute. It implements a simple getter function that outputs <code>PATIENT</code> or    <code>FAMILY</code>, depending on the value of <code>family</code>.</li> </ol>"},{"location":"pipes/qualifiers/family/#edsnlp.pipes.qualifiers.family.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>family</code> <p>List of terms indicating family reference.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of syntagms termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_sections</code> <p>Whether to use annotated sections (namely <code>ant\u00e9c\u00e9dents familiaux</code>).</p> <p> TYPE: <code>bool, by default `False`</code> DEFAULT: <code>True</code> </p>"},{"location":"pipes/qualifiers/family/#edsnlp.pipes.qualifiers.family.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.family</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/qualifiers/history/","title":"Medical History","text":"<p>The <code>eds.history</code> pipeline uses a simple rule-based algorithm to detect spans that describe medical history rather than the diagnostic of a given visit.</p> <p>The mere definition of a medical history is not straightforward. Hence, this component only tags entities that are explicitly described as part of the medical history, e.g., preceded by a synonym of \"medical history\".</p> <p>This component may also use the output of:</p> <ul> <li>the <code>eds.sections</code> component In that case, the entire <code>ant\u00e9c\u00e9dent</code> section is tagged as a medical history.</li> </ul> <p>Sections</p> <p>Be careful, the <code>eds.sections</code> component may oversize the <code>ant\u00e9c\u00e9dents</code> section. Indeed, it detects section titles and tags the entire text between a title and the next as a section. Hence, should a section title goes undetected after the <code>ant\u00e9c\u00e9dents</code> title, some parts of the document will erroneously be tagged as a medical history.</p> <p>To curb that possibility, using the output of the <code>eds.sections</code> component is deactivated by default.</p> <ul> <li>the <code>eds.dates</code> component. In that case, it will take the   dates into account to tag extracted entities as a medical history or not.</li> </ul> <p>Dates</p> <p>To take the most of the <code>eds.dates</code> component, you may set a value for <code>doc._.note_datetime</code>, either directly:</p> <pre><code>doc = nlp.make_doc(text)\ndoc._.note_datetime = datetime.datetime(2022, 8, 28)\nnlp(doc)\n</code></pre> <p>or using a converter such as the <code>omop</code> converter</p> <p>It allows the component to compute the duration of absolute dates (e.g., le 28 ao\u00fbt 2022/August 28, 2022). The <code>birth_datetime</code> context allows the component to exclude the birthdate from the extracted dates.</p>"},{"location":"pipes/qualifiers/history/#edsnlp.pipes.qualifiers.history.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are history or not. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sections())\nnlp.add_pipe(eds.dates())\nnlp.add_pipe(eds.matcher(terms=dict(douleur=\"douleur\", malaise=\"malaises\")))\nnlp.add_pipe(\n    eds.history(\n        use_sections=True,\n        use_dates=True,\n    ),\n)\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Il a des ant\u00e9c\u00e9dents de malaises.\"\n    \"ANT\u00c9C\u00c9DENTS : \"\n    \"- le patient a d\u00e9j\u00e0 eu des malaises. \"\n    \"- le patient a eu une douleur \u00e0 la jambe il y a 10 jours\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, malaises, malaises, douleur)\n\ndoc.ents[0]._.history\n# Out: False\n\ndoc.ents[1]._.history\n# Out: True\n\ndoc.ents[2]._.history  # (1)\n# Out: True\n\ndoc.ents[3]._.history  # (2)\n# Out: False\n</code></pre> <ol> <li>The entity is in the section <code>ant\u00e9c\u00e9dent</code>.</li> <li>The entity is in the section <code>ant\u00e9c\u00e9dent</code>, however the extracted <code>relative_date</code> refers to an event that took place within 14 days.</li> </ol>"},{"location":"pipes/qualifiers/history/#edsnlp.pipes.qualifiers.history.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.history</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>history</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is a medical history.</li> <li>The <code>history_</code> property is a human-readable string, computed from the <code>history</code>    attribute. It implements a simple getter function that outputs <code>CURRENT</code> or    <code>ATCD</code>, depending on the value of <code>history</code>.</li> </ol>"},{"location":"pipes/qualifiers/history/#edsnlp.pipes.qualifiers.history.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'history'</code> </p> <code>history</code> <p>List of terms indicating medical history reference.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of syntagms termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>use_sections</code> <p>Whether to use section pipeline to detect medical history section.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_dates</code> <p>Whether to use dates pipeline to detect if the event occurs  a long time before the document date.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>history_limit</code> <p>The number of days after which the event is considered as history.</p> <p> TYPE: <code>Union[int, timedelta]</code> DEFAULT: <code>14</code> </p> <code>exclude_birthdate</code> <p>Whether to exclude the birthdate from history dates.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>closest_dates_only</code> <p>Whether to include the closest dates only.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>tz</code> <p>The timezone to use. Defaults to \"Europe/Paris\".</p> <p> TYPE: <code>Optional[Union[str, tzinfo]]</code> DEFAULT: <code>None</code> </p>"},{"location":"pipes/qualifiers/history/#edsnlp.pipes.qualifiers.history.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.history</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/qualifiers/hypothesis/","title":"Hypothesis","text":"<p>The <code>eds.hypothesis</code> pipeline uses a simple rule-based algorithm to detect spans that are speculations rather than certain statements.</p> <p>The component looks for five kinds of expressions in the text :</p> <ul> <li>preceding hypothesis, ie cues that precede a hypothetical expression</li> <li>following hypothesis, ie cues that follow a hypothetical expression</li> <li>pseudo hypothesis : contain a hypothesis cue, but are not hypothesis   (eg \"pas de doute\"/\"no doubt\")</li> <li>hypothetical verbs : verbs indicating hypothesis (eg \"douter\")</li> <li>classic verbs conjugated to the conditional, thus indicating hypothesis</li> </ul> <ol><li><p><p>Dalloux C., Claveau V. and Grabar N., 2017. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. https://hal.archives-ouvertes.fr/hal-01659637</p></p></li><li><p><p>Grabar N., Claveau V. and Dalloux C., 2018. CAS: French Corpus with Clinical Cases. https://hal.archives-ouvertes.fr/hal-01937096</p></p></li></ol>"},{"location":"pipes/qualifiers/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are part of a speculation. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(douleur=\"douleur\", fracture=\"fracture\")))\nnlp.add_pipe(eds.hypothesis())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Possible fracture du radius.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, fracture)\n\ndoc.ents[0]._.hypothesis\n# Out: False\n\ndoc.ents[1]._.hypothesis\n# Out: True\n</code></pre>"},{"location":"pipes/qualifiers/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.hypothesis</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>hypothesis</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is a speculation.</li> <li>The <code>hypothesis_</code> property is a human-readable string, computed from the    <code>hypothesis</code> attribute. It implements a simple getter function that outputs    <code>HYP</code> or <code>CERT</code>, depending on the value of <code>hypothesis</code>.</li> </ol>"},{"location":"pipes/qualifiers/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--performance","title":"Performance","text":"<p>The component's performance is measured on three datasets :</p> <ul> <li>The ESSAI (Dalloux et al., 2017) and CAS (Grabar et al., 2018) datasets were developed   at the CNRS. The two are concatenated.</li> <li>The NegParHyp corpus was specifically developed at APHP's CDW to test the   component on actual clinical notes, using pseudonymised notes from the APHP's CDW.</li> </ul> Dataset Hypothesis F1 CAS/ESSAI 49% NegParHyp 52% <p>NegParHyp corpus</p> <p>The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for hypothesis, speculation and hypothesis context.</p>"},{"location":"pipes/qualifiers/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'hypothesis'</code> </p> <code>attr</code> <p>spaCy's attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>pseudo</code> <p>List of pseudo hypothesis cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of preceding hypothesis cues</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of following hypothesis cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs_hyp</code> <p>List of hypothetical verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs_eds</code> <p>List of mainstream verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"pipes/qualifiers/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.hypothesis</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/qualifiers/negation/","title":"Negation","text":"<p>The <code>eds.negation</code> component uses a simple rule-based algorithm to detect negated spans. It was designed at AP-HP's EDS, following the insights of the NegEx algorithm by Chapman et al., 2001.</p> <p>The component looks for five kinds of expressions in the text :</p> <ul> <li>preceding negations, i.e., cues that precede a negated expression</li> <li>following negations, i.e., cues that follow a negated expression</li> <li>pseudo negations : contain a negation cue, but are not negations   (eg \"pas de doute\"/\"no doubt\")</li> <li>negation verbs, i.e., verbs that indicate a negation</li> <li>terminations, i.e., words that delimit propositions.   The negation spans from the preceding cue to the termination.</li> </ul> <ol><li><p><p>Chapman W.W., Bridewell W., Hanbury P., Cooper G.F. and Buchanan B.G., 2001. A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge Summaries. Journal of Biomedical Informatics. 34, pp.301--310. 10.1006/jbin.2001.1029</p></p></li><li><p><p>Dalloux C., Claveau V. and Grabar N., 2017. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. https://hal.archives-ouvertes.fr/hal-01659637</p></p></li><li><p><p>Grabar N., Claveau V. and Dalloux C., 2018. CAS: French Corpus with Clinical Cases. https://hal.archives-ouvertes.fr/hal-01937096</p></p></li></ol>"},{"location":"pipes/qualifiers/negation/#edsnlp.pipes.qualifiers.negation.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks the polarity of the extracted entities. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(patient=\"patient\", fracture=\"fracture\")))\nnlp.add_pipe(eds.negation())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Le scanner ne d\u00e9tecte aucune fracture.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (patient, fracture)\n\ndoc.ents[0]._.negation  # (1)\n# Out: False\n\ndoc.ents[1]._.negation\n# Out: True\n</code></pre> <ol> <li>The result of the component is kept in the <code>negation</code> custom extension.</li> </ol>"},{"location":"pipes/qualifiers/negation/#edsnlp.pipes.qualifiers.negation.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.negation</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>negation</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is negated.</li> <li>The <code>negation_</code> property is a human-readable string, computed from the <code>negation</code>    attribute. It implements a simple getter function that outputs <code>AFF</code> or <code>NEG</code>,    depending on the value of <code>negation</code>.</li> </ol>"},{"location":"pipes/qualifiers/negation/#edsnlp.pipes.qualifiers.negation.factory.create_component--performance","title":"Performance","text":"<p>The component's performance is measured on three datasets :</p> <ul> <li>The ESSAI (Dalloux et al., 2017) and CAS (Grabar et al., 2018) datasets were developed   at the CNRS. The two are concatenated.</li> <li>The NegParHyp corpus was specifically developed at AP-HP to test the component   on actual clinical notes, using pseudonymised notes from the AP-HP.</li> </ul> Dataset Negation F1 CAS/ESSAI 71% NegParHyp 88% <p>NegParHyp corpus</p> <p>The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context.</p>"},{"location":"pipes/qualifiers/negation/#edsnlp.pipes.qualifiers.negation.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>attr</code> <p>spaCy's attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>pseudo</code> <p>List of pseudo negation cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of preceding negation cues</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding_regex</code> <p>List of preceding negation cues, but as regexes.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of following negation cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs</code> <p>List of negation verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"pipes/qualifiers/negation/#edsnlp.pipes.qualifiers.negation.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.negation</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/qualifiers/reported-speech/","title":"Reported Speech","text":"<p>The <code>eds.reported_speech</code> component uses a simple rule-based algorithm to detect spans that relate to reported speech (eg when the doctor quotes the patient). It was designed at AP-HP's EDS.</p>"},{"location":"pipes/qualifiers/reported-speech/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are part of a reported speech. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(patient=\"patient\", alcool=\"alcoolis\u00e9\")))\nnlp.add_pipe(eds.reported_speech())\n\ntext = (\n    \"Le patient est admis aux urgences ce soir pour une douleur au bras. \"\n    \"Il nie \u00eatre alcoolis\u00e9.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (patient, alcoolis\u00e9)\n\ndoc.ents[0]._.reported_speech\n# Out: False\n\ndoc.ents[1]._.reported_speech\n# Out: True\n</code></pre>"},{"location":"pipes/qualifiers/reported-speech/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.reported_speech</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>reported_speech</code> attribute is a boolean, set to <code>True</code> if the component    predicts that the span/token is reported.</li> <li>The <code>reported_speech_</code> property is a human-readable string, computed from the    <code>reported_speech</code> attribute. It implements a simple getter function that outputs    <code>DIRECT</code> or <code>REPORTED</code>, depending on the value of <code>reported_speech</code>.</li> </ol>"},{"location":"pipes/qualifiers/reported-speech/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>quotation</code> <p>String gathering all quotation cues.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>verbs</code> <p>List of reported speech verbs.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of terms following a reported speech.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of terms preceding a reported speech.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"pipes/qualifiers/reported-speech/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.reported_speech</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"pipes/trainable/","title":"Trainable components overview","text":"<p>In addition to its rule-based pipeline components, EDS-NLP offers new trainable components to fit and run machine learning models for classic biomedical information extraction tasks.</p> <p>All trainable components implement the <code>TorchComponent</code> class, which provides a common API for training and inference.</p>"},{"location":"pipes/trainable/#available-components","title":"Available components :","text":"Name Description <code>eds.transformer</code> Embed text with a transformer model <code>eds.text_cnn</code> Contextualize embeddings with a CNN <code>eds.span_pooler</code> A span embedding component that aggregates word embeddings <code>eds.ner_crf</code> A trainable component to extract entities <code>eds.extractive_qa</code> A trainable component for extractive question answering <code>eds.span_classifier</code> A trainable component for multi-class multi-label span classification <code>eds.span_linker</code> A trainable entity linker (i.e. to a list of concepts) <code>eds.biaffine_dep_parser</code> A trainable biaffine dependency parser"},{"location":"pipes/trainable/biaffine-dependency-parser/","title":"Trainable Biaffine Dependency Parser","text":"<p>The <code>eds.biaffine_dep_parser</code> component is a trainable dependency parser based on a biaffine model (Dozat and Manning, 2017). For each token, the model predicts a score for each possible head in the document, and a score for each possible label for each head. The results are then decoded either greedily by picking the best scoring head for each token independently, or holistically by computing the Maximum Spanning Tree (MST) over the graph of token \u2192 head scores.</p> <p>Experimental</p> <p>This component is experimental. In particular, it expects the input to be sentences and not full documents, as it has not been optimized for memory efficiency yet and computed the full matrix of scores for all pairs of tokens in a document.</p> <p>At the moment, it is mostly used for benchmarking and research purposes.</p> <ol><li><p><p>Dozat T. and Manning C.D., 2017. Deep Biaffine Attention for Neural Dependency Parsing. https://arxiv.org/abs/1611.01734</p></p></li><li><p><p>Grobol L. and Crabb\u00e9 B., 2021. Analyse en d\u00e9pendances du fran\u00e7ais avec des plongements contextualis\u00e9s. https://hal.archives-ouvertes.fr/hal-03223424</p></p></li></ol>"},{"location":"pipes/trainable/biaffine-dependency-parser/#edsnlp.pipes.trainable.biaffine_dep_parser.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.biaffine_dep_parser(\n        embedding=eds.transformer(model=\"prajjwal1/bert-tiny\"),\n        hidden_size=128,\n        dropout_p=0.1,\n        # labels unset, will be inferred from the data in `post_init`\n        decoding_mode=\"mst\",\n    ),\n    name=\"dep_parser\"\n)\n</code></pre> <p>Dependency parsers are typically trained on CoNLL-formatted Universal Dependencies corpora, which you can load using the <code>edsnlp.data.read_conll</code> function.</p> <p>To train the model, you can adapt the the Training NER tutorial.</p>"},{"location":"pipes/trainable/biaffine-dependency-parser/#edsnlp.pipes.trainable.biaffine_dep_parser.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'biaffine_dep_parser'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). For example <code>{\"section\": \"conclusion\"}</code> to predict dependencies in the conclusion section of documents.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>use_attrs</code> <p>The attributes to use as features for the model (ex. <code>[\"pos_\"]</code> to use the POS tag). By default, no attributes are used.</p> <p>Note that if you train a model with attributes, you will need to provide the same attributes during inference, and the model might not work well if the attributes were not annotated accurately on the test data.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>attr_size</code> <p>The size of the attribute embeddings.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>hidden_size</code> <p>The size of the hidden layer in the MLP.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>dropout_p</code> <p>The dropout probability to use in the MLP.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>labels</code> <p>The labels to predict. The labels can also be inferred from the data during <code>nlp.post_init(...)</code>.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>['root']</code> </p> <code>decoding_mode</code> <p>Whether to decode the dependencies greedily or using the Maximum Spanning Tree algorithm.</p> <p> TYPE: <code>Literal['greedy', 'mst']</code> DEFAULT: <code>mst</code> </p>"},{"location":"pipes/trainable/biaffine-dependency-parser/#edsnlp.pipes.trainable.biaffine_dep_parser.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.biaffine_dep_parser</code> trainable pipe was developed by AP-HP's Data Science team, and heavily inspired by the implementation of Grobol and Crabb\u00e9, 2021. The biaffine architecture is based on the biaffine parser of Dozat and Manning, 2017.</p>"},{"location":"pipes/trainable/extractive-qa/","title":"Extractive Question Answering","text":"<p>The <code>eds.extractive_qa</code> component is a trainable extractive question answering component. This can be seen as a Named Entity Recognition (NER) component where the types of entities predicted by the model are not pre-defined during the training but are provided as prompts (i.e., questions) at inference time.</p> <p>The <code>eds.extractive_qa</code> shares a lot of similarities with the <code>eds.ner_crf</code> component, and therefore most of the arguments are the same.</p> <p>Extractive vs Abstractive Question Answering</p> <p>Extractive Question Answering differs from Abstractive Question Answering in that the answer is extracted from the text, rather than generated (\u00e0 la ChatGPT) from scratch. To normalize the answers, you can use the <code>eds.span_linker</code> component in <code>synonym</code> mode and search for the closest <code>synonym</code> in a predefined list.</p>"},{"location":"pipes/trainable/extractive-qa/#edsnlp.pipes.trainable.extractive_qa.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.extractive_qa(\n        embedding=eds.transformer(\n            model=\"prajjwal1/bert-tiny\",\n            window=128,\n            stride=96,\n        ),\n        mode=\"joint\",\n        target_span_getter=\"ner-gold\",\n        span_setter=\"ents\",\n        questions={\n            \"disease\": \"What disease does the patient have?\",\n            \"drug\": \"What drug is the patient taking?\",\n        },  # (1)!\n    ),\n    name=\"qa\",\n)\n</code></pre> <p>To train the model, refer to the Training tutorial.</p> <p>Once the model is trained, you can use the questions attribute (next section) on the document you run the model on, or you can change the global questions attribute:</p> <pre><code>nlp.pipes.qa.questions = {\n    \"disease\": \"When did the patient get sick?\",\n}\n</code></pre>"},{"location":"pipes/trainable/extractive-qa/#edsnlp.pipes.trainable.extractive_qa.factory.create_component--dynamic-questions","title":"Dynamic Questions","text":"<p>You can also provide</p> <pre><code>eds.extractive_qa(..., questions_attribute=\"questions\")\n</code></pre> <p>to get the questions dynamically from an attribute on the Doc or Span objects (e.g., <code>doc._.questions</code>). This is useful when you want to have different questions depending on the document.</p> <p>To provide questions from a dataframe, you can use the following code:</p> <pre><code>dataframe = pd.DataFrame({\"questions\": ..., \"note_text\": ..., \"note_id\": ...})\nstream = edsnlp.data.from_pandas(\n    dataframe,\n    converter=\"omop\",\n    doc_attributes={\"questions\": \"questions\"},\n)\nstream.map_pipeline(nlp)\nstream.set_processing(backend=\"multiprocessing\")\nout = stream.to_pandas(converters=\"ents\")\n</code></pre>"},{"location":"pipes/trainable/extractive-qa/#edsnlp.pipes.trainable.extractive_qa.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'extractive_qa'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>questions</code> <p>The questions to ask, as a mapping between the entity type and the list of questions to ask for this entity type (or single string if only one question).</p> <p> TYPE: <code>Dict[str, AsList[str]]</code> DEFAULT: <code>{}</code> </p> <code>questions_attribute</code> <p>The attribute to use to get the questions dynamically from the Doc or Span objects (as returned by the <code>context_getter</code> argument). If None, the questions will be fixed and only taken from the <code>questions</code> argument.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>questions</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). For example <code>{\"section\": \"conclusion\"}</code> to only extract the entities from the conclusion.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>target_span_getter</code> <p>Method to call to get the gold spans from a document, for scoring or training. By default, takes all entities in <code>doc.ents</code>, but we recommend you specify a given span group name instead.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use to set the predicted spans on the Doc object. If None, the component will infer the span setter from the target_span_getter config.</p> <p> TYPE: <code>Optional[SpanSetterArg]</code> DEFAULT: <code>None</code> </p> <code>infer_span_setter</code> <p>Whether to complete the span setter from the target_span_getter config. False by default, unless the span_setter is None.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>The CRF mode to use : independent, joint or marginal</p> <p> TYPE: <code>Literal['independent', 'joint', 'marginal']</code> DEFAULT: <code>joint</code> </p> <code>window</code> <p>The window size to use for the CRF. If 0, will use the whole document, at the cost of a longer computation time. If 1, this is equivalent to assuming that the tags are independent and will the component be faster, but with degraded performance. Empirically, we found that a window size of 10 or 20 works well.</p> <p> TYPE: <code>int</code> DEFAULT: <code>40</code> </p> <code>stride</code> <p>The stride to use for the CRF windows. Defaults to <code>window // 2</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p>"},{"location":"pipes/trainable/ner/","title":"Trainable NER","text":"<p>The <code>eds.ner_crf</code> component is a general purpose trainable named entity recognizer. It can extract:</p> <ul> <li>flat entities</li> <li>overlapping entities of different labels</li> </ul> <p>However, at the moment, the model cannot currently extract entities that are nested inside larger entities of the same label.</p> <p>It is based on a CRF (Conditional Random Field) layer and should therefore work well on dataset composed of entities will ill-defined boundaries. We offer a compromise between speed and performance by allowing the user to specify a window size for the CRF layer. The smaller the window, the faster the model will be, but at the cost of degraded performance.</p> <p>The pipeline assigns both <code>doc.ents</code> (in which overlapping entities are filtered out) and <code>doc.spans</code>. These destinations can be inferred from the <code>target_span_getter</code> parameter, combined with the <code>post_init</code> step.</p> <ol><li><p><p>Wajsb\u00fcrt P., 2021. Extraction and normalization of simple and structured entities in medical documents. https://hal.archives-ouvertes.fr/tel-03624928</p></p></li></ol>"},{"location":"pipes/trainable/ner/#edsnlp.pipes.trainable.ner_crf.factory.create_component--architecture","title":"Architecture","text":"<p>The model performs token classification using the BIOUL (Begin, Inside, Outside, Unary, Last) tagging scheme. To extract overlapping entities, each label has its own tag sequence, so the model predicts <code>n_labels</code> sequences of O, I, B, L, U tags. The architecture is displayed in the figure below.</p> <p>To enforce the tagging scheme, (ex: I cannot follow O but only B, ...), we use a stack of CRF (Conditional Random Fields) layers, one per label during both training and prediction.</p> <p> </p> Nested NER architecture"},{"location":"pipes/trainable/ner/#edsnlp.pipes.trainable.ner_crf.factory.create_component--examples","title":"Examples","text":"<p>Let us define a pipeline composed of a transformer, and a NER component.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.ner_crf(\n        embedding=eds.transformer(\n            model=\"prajjwal1/bert-tiny\",\n            window=128,\n            stride=96,\n        ),\n        mode=\"joint\",\n        target_span_getter=\"ner-gold\",\n        span_setter=\"ents\",\n        window=10,\n    ),\n    name=\"ner\"\n)\n</code></pre> <p>To train the model, refer to the Training tutorial.</p>"},{"location":"pipes/trainable/ner/#edsnlp.pipes.trainable.ner_crf.factory.create_component--extensions","title":"Extensions","text":"<p>Experimental Confidence Score</p> <p>The NER confidence score feature is experimental and the API and underlying algorithm may change.</p> <p>The <code>eds.ner_crf</code> pipeline declares one extension on the <code>Span</code> object:</p> <ul> <li><code>span._.ner_confidence_score</code>: The confidence score of the Named Entity Recognition (NER) model for the given span.</li> </ul> <p>The <code>ner_confidence_score</code> is computed based on the Average Entity Confidence Score using the following formula:</p> <p>$$ \\text{Average Entity Confidence Score} = \\frac{1}{n} \\sum_{i \\in \\text{tokens}} (1 - p(O)_i) $$</p> <p>Where:</p> <ul> <li>$n$ is the number of tokens.</li> <li>$\\text{tokens}$ refers to the tokens within the span.</li> <li>$p(O)_i$ represents the probability of token $i$ belonging to class 'O' (Outside entity).</li> </ul> <p>Confidence score is not computed by default</p> <p>By default, the confidence score is not computed, as it adds around 5% to inference time. You can enable its computation with: <pre><code>nlp.pipes.ner.compute_confidence_score = True\n</code></pre></p>"},{"location":"pipes/trainable/ner/#edsnlp.pipes.trainable.ner_crf.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'ner_crf'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>target_span_getter</code> <p>Method to call to get the gold spans from a document, for scoring or training. By default, takes all entities in <code>doc.ents</code>, but we recommend you specify a given span group name instead.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>labels</code> <p>The labels to predict. The labels can also be inferred from the data during <code>nlp.post_init(...)</code></p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use to set the predicted spans on the Doc object. If None, the component will infer the span setter from the target_span_getter config.</p> <p> TYPE: <code>Optional[SpanSetterArg]</code> DEFAULT: <code>None</code> </p> <code>infer_span_setter</code> <p>Whether to complete the span setter from the target_span_getter config. False by default, unless the span_setter is None.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). For example <code>{\"section\": \"conclusion\"}</code> to only extract the entities from the conclusion.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>The CRF mode to use : independent, joint or marginal</p> <p> TYPE: <code>Literal['independent', 'joint', 'marginal']</code> </p> <code>window</code> <p>The window size to use for the CRF. If 0, will use the whole document, at the cost of a longer computation time. If 1, this is equivalent to assuming that the tags are independent and will the component be faster, but with degraded performance. Empirically, we found that a window size of 10 or 20 works well.</p> <p> TYPE: <code>int</code> DEFAULT: <code>40</code> </p> <code>stride</code> <p>The stride to use for the CRF windows. Defaults to <code>window // 2</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p>"},{"location":"pipes/trainable/ner/#edsnlp.pipes.trainable.ner_crf.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.ner_crf</code> pipeline was developed by AP-HP's Data Science team.</p> <p>The deep learning model was adapted from Wajsb\u00fcrt, 2021.</p>"},{"location":"pipes/trainable/span-classifier/","title":"Trainable Span Classifier","text":"<p>The <code>eds.span_classifier</code> component is a trainable attribute predictor. In this context, the span classification task consists in assigning values (boolean, strings or any object) to attributes/extensions of spans such as:</p> <ul> <li><code>span._.negation</code>,</li> <li><code>span._.date.mode</code></li> <li><code>span._.cui</code></li> </ul> <p>In the rest of this page, we will refer to a pair of (attribute, value) as a \"binding\". For instance, the binding <code>(\"_.negation\", True)</code> means that the attribute <code>negation</code> of the span is (or should be, when predicted) set to <code>True</code>.</p>"},{"location":"pipes/trainable/span-classifier/#edsnlp.pipes.trainable.span_classifier.factory.create_component--architecture","title":"Architecture","text":"<p>The model performs span classification by:</p> <ol> <li>Calling a word pooling embedding such as <code>eds.span_pooler</code> to compute a single embedding for each span</li> <li>Computing logits for each possible binding using a linear layer</li> <li> <p>Splitting these bindings into groups of exclusive values such as</p> <ul> <li><code>event=start</code> and <code>event=stop</code></li> <li><code>negated=False</code> and <code>negated=True</code></li> </ul> <p>Note that the above groups are not exclusive, but the values within each group are.</p> </li> <li> <p>Applying the best scoring binding in each group to each span</p> </li> </ol>"},{"location":"pipes/trainable/span-classifier/#edsnlp.pipes.trainable.span_classifier.factory.create_component--examples","title":"Examples","text":"<p>To create a span classifier component, you can use the following code:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.span_classifier(\n        # To embed the spans, we will use a span pooler\n        embedding=eds.span_pooler(\n            pooling_mode=\"mean\",  # mean pooling\n            # that will use a transformer to embed the doc words\n            embedding=eds.transformer(\n                model=\"prajjwal1/bert-tiny\",\n                window=128,\n                stride=96,\n            ),\n        ),\n        span_getter=[\"ents\", \"sc\"],\n        # For every span embedded by the span pooler\n        # (doc.ents and doc.spans[\"sc\"]), we will predict both\n        # span._.negation and span._.event_type\n        attributes=[\"_.negation\", \"_.event_type\"],\n    ),\n    name=\"span_classifier\",\n)\n</code></pre> <p>To infer the values of the attributes, you can use the pipeline <code>post_init</code> method:</p> <pre><code>nlp.post_init(gold_data)\n</code></pre> <p>To train the model, refer to the Training tutorial.</p> <p>You can inspect the bindings that will be used for training and prediction <pre><code>print(nlp.pipes.attr.bindings)\n# list of (attr name, span labels or True if all, values)\n# Out: [\n#   ('_.negation', True, [True, False]),\n#   ('_.event_type', True, ['start', 'stop'])\n# ]\n</code></pre></p> <p>You can also change these values and update the bindings by calling the <code>update_bindings</code> method. Don't forget to retrain the model if new values are added !</p>"},{"location":"pipes/trainable/span-classifier/#edsnlp.pipes.trainable.span_classifier.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'span_classifier'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>SpanEmbeddingComponent</code> </p> <code>label_weights</code> <p>The weight of each label for each attribute. The keys are the attribute names and the values are dictionaries with the labels as keys and the weights as values. For instance, <code>{\"_.negation\": {True: 1, False: 2}}</code> will give a weight of 1 to the <code>True</code> value of the <code>negation</code> attribute and 2 to the <code>False</code> value.</p> <p> TYPE: <code>Dict[str, Dict[Any, float]]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>How to extract the candidate spans and the attributes to predict or train on.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). This can be:</p> <ul> <li>a <code>SpanGetterArg</code> to retrieve contexts from a whole document. For example   <code>{\"section\": \"conclusion\"}</code> to only use the conclusion as context (you   must ensure that all spans produced by the <code>span_getter</code> argument do fall   in the conclusion in this case)</li> <li>a callable, that gets a span and should return a context for this span.   For instance, <code>lambda span: span.sent</code> to use the sentence as context.</li> </ul> <p> TYPE: <code>Optional[Union[Callable, SpanGetterArg]]</code> DEFAULT: <code>None</code> </p> <code>attributes</code> <p>The attributes to predict or train on. If a dict is given, keys are the attributes and values are the labels for which the attr is allowed, or True if the attr is allowed for all labels.</p> <p> TYPE: <code>AttributesArg</code> DEFAULT: <code>None</code> </p> <code>keep_none</code> <p>If False, skip spans for which a attr returns None. If True (default), the None values will be learned and predicted, just as any other value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"pipes/trainable/span-linker/","title":"Trainable Span Linker","text":"<p>The <code>eds.span_linker</code> component is a trainable span concept predictor, typically used to match spans in the text with concepts in a knowledge base. This task is known as \"Entity Linking\", \"Named Entity Disambiguation\" or \"Normalization\" (the latter is mostly used in the biomedical machine learning community).</p> <p>Entity Linking vs Named Entity Recognition</p> <p>Entity Linking is the task of linking existing entities to their concept in a knowledge base, while Named Entity Recognition is the task of detecting spans in the text that correspond to entities. The <code>eds.span_linker</code> component should therefore be used after the Named Entity Recognition step (e.g. using the <code>eds.ner_crf</code> component).</p> <ol><li><p><p>Wajsb\u00fcrt P., Sarfati A. and Tannier X., 2021. Medical concept normalization in French using multilingual terminologies and contextual embeddings. Journal of Biomedical Informatics. 114, pp.103684. https://doi.org/10.1016/j.jbi.2021.103684</p></p></li></ol>"},{"location":"pipes/trainable/span-linker/#edsnlp.pipes.trainable.span_linker.factory.create_component--how-it-works","title":"How it works","text":"<p>To perform this task, this components compare the embedding of a given query span (e.g. \"aspirin\") with the embeddings in the knowledge base, where each embedding represents a concept (e.g. \"B01AC06\"), and selects the most similar embedding and returns its concept id. This comparison is done using either:</p> <ul> <li>the cosine similarity between the input and output embeddings (recommended)</li> <li>a simple dot product</li> </ul> <p>We filter out the concepts that are not relevant for a given query by using groups. For each span to link, we use its label to select a group of concepts to compare with. For example, if the span is labeled as \"drug\", we only compare it with concepts that are drugs. These concepts groups are inferred from the training data when running the <code>post_init</code> method, or can be provided manually using the <code>pipe.update_concepts(concepts, mapping, [embeddings])</code> method. If a label is not found in the mapping, the span is compared with all concepts.</p> <p>We support comparing entity queries against two kind of references : either the embeddings of the concepts themselves (<code>reference_mode = \"concept\"</code>), or the embeddings of the synonyms of the concepts (<code>reference_mode = \"synonym\"</code>).</p>"},{"location":"pipes/trainable/span-linker/#edsnlp.pipes.trainable.span_linker.factory.create_component--synonym-similarity","title":"Synonym similarity","text":"<p>When performing span linking in <code>synonym</code> mode, the span linker embedding matrix contains one embedding vector per concept per synonym, and each embedding maps to the concept of its synonym. This mode is slower and more memory intensive, since you have to store multiple embeddings per concept, but it can yield good results in zero-shot scenarios (see example below).</p> <p> </p> Entity linking based on synonym similarity"},{"location":"pipes/trainable/span-linker/#edsnlp.pipes.trainable.span_linker.factory.create_component--concept-similarity","title":"Concept similarity","text":"<p>In <code>concept</code> mode, the span linker embedding matrix contains one embedding vector per concept : imagine a single vector that approximately averages all the synonyms of a concept (e.g. B01AC06 = average of \"aspirin\", \"acetyl-salicylic acid\", etc.). This mode is faster and more memory efficient, but usually requires that the concept weights are fine-tuned.</p> <p> </p> Entity linking based on concept similarity"},{"location":"pipes/trainable/span-linker/#edsnlp.pipes.trainable.span_linker.factory.create_component--examples","title":"Examples","text":"<p>Here is how you can use the <code>eds.span_linker</code> component to link spans without training, in <code>synonym</code> mode. You will still need to pre-compute the embeddings of the target synonyms.</p> <p>First, initialize the component:</p> <pre><code>import pandas as pd\nimport edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.span_linker(\n        rescale=20.0,\n        threshold=0.8,\n        metric=\"cosine\",\n        reference_mode=\"synonym\",\n        probability_mode=\"sigmoid\",\n        span_getter=[\"ents\"],\n        embedding=eds.span_pooler(\n            hidden_size=128,\n            embedding=eds.transformer(\n                model=\"prajjwal1/bert-tiny\",\n                window=128,\n                stride=96,\n            ),\n        ),\n    ),\n    name=\"linker\",\n)\n</code></pre> <p>We will assume you have a list of synonyms with their concept and label with the columns:</p> <ul> <li><code>STR</code>: synonym text</li> <li><code>CUI</code>: concept id</li> <li><code>GRP</code>: label.</li> </ul> <p>All we need to do is to initialize the component with the synonyms and that's it ! Since we have set <code>init_weights</code> to True, and we are in <code>synonym</code> mode, the embeddings of the synonyms will be stored in the component and used to compute the similarity scores</p> <pre><code>synonyms_df = pd.read_csv(\"synonyms.csv\")\n\ndef make_doc(row):\n    doc = nlp.make_doc(row[\"STR\"])\n    span = doc[:]\n    span.label_ = row[\"GRP\"]\n    doc.ents = [span]\n    span._.cui = row[\"CUI\"]\n    return doc\n\nnlp.post_init(\n    edsnlp.data.from_pandas(\n        synonyms_df,\n        converter=make_doc,\n    )\n)\n</code></pre> <p>Now, you can now use it in a text: <pre><code>doc = nlp.make_doc(\"Aspirin is a drug\")\nspan = doc[0:1]  # \"Aspirin\"\nspan.label_ = \"Drug\"\ndoc.ents = [span]\n\ndoc = nlp(doc)\nprint(doc.ents[0]._.cui)\n# \"B01AC06\"\n</code></pre></p> <p>To use the <code>eds.span_linker</code> component in <code>class</code> mode, we refer to the following repository: deep_multilingual_normalization based on the work of Wajsb\u00fcrt et al., 2021.</p>"},{"location":"pipes/trainable/span-linker/#edsnlp.pipes.trainable.span_linker.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Spacy vocabulary</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'span_linker'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>SpanEmbeddingComponent</code> </p> <code>metric</code> <p>Whether to compute the cosine similarity between the input and output embeddings or the dot product.</p> <p> TYPE: <code>Literal[\"cosine\", \"dot\"] = \"cosine\"</code> DEFAULT: <code>cosine</code> </p> <code>rescale</code> <p>Rescale the output cosine similarities by a constant factor.</p> <p> TYPE: <code>float</code> DEFAULT: <code>20</code> </p> <code>threshold</code> <p>Threshold probability to consider a concept as valid</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>attribute</code> <p>The attribute to store the concept id</p> <p> TYPE: <code>str</code> DEFAULT: <code>cui</code> </p> <code>reference_mode</code> <p>Whether to compare the embeddings with the concepts embeddings (one per concept) or the synonyms embeddings (one per concept per synonym). See above for more details.</p> <p> TYPE: <code>Literal['concept', 'synonym']</code> DEFAULT: <code>concept</code> </p> <code>span_getter</code> <p>How to extract the candidate spans to predict or train on.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the entity only, so no context)</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>probability_mode</code> <p>Whether to compute the probabilities using a softmax or a sigmoid function. This will also determine the loss function to use, either cross-entropy or binary cross-entropy.</p> <p>Subsetting the concepts</p> <p>The probabilities returned in <code>softmax</code> mode depend on the number of concepts (as an extreme cas, if you have only one concept, its softmax probability will always be 1). This is why we recommend using the <code>sigmoid</code> mode in which the probabilities are computed independently for each concept.</p> <p> TYPE: <code>Literal['softmax', 'sigmoid']</code> DEFAULT: <code>sigmoid</code> </p> <code>init_weights</code> <p>Whether to initialize the weights of the component with the embeddings of the entities of the docs provided to the <code>post_init</code> method. How this is done depends on the <code>reference_mode</code> parameter:</p> <ul> <li><code>concept</code>: the embeddings are averaged</li> <li><code>synonym</code>: the embeddings are stored as is</li> </ul> <p>By default, this is set to <code>True</code> if <code>reference_mode</code> is <code>synonym</code>, and <code>False</code> otherwise.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"pipes/trainable/span-linker/#edsnlp.pipes.trainable.span_linker.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.span_linker</code> component was developed by AP-HP's Data Science team.</p> <p>The deep learning concept-based architecture was adapted from Wajsb\u00fcrt et al., 2021.</p>"},{"location":"pipes/trainable/embeddings/span_pooler/","title":"Span Pooler","text":"<p>The <code>eds.span_pooler</code> component is a trainable span embedding component. It generates span embeddings from a word embedding component and a span getter. It can be used to train a span classifier, as in <code>eds.span_classifier</code>.</p>"},{"location":"pipes/trainable/embeddings/span_pooler/#edsnlp.pipes.trainable.embeddings.span_pooler.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[Pipeline]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'span_pooler'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>pooling_mode</code> <p>How word embeddings are aggregated into a single embedding per span.</p> <p> TYPE: <code>Literal['max', 'sum', 'mean']</code> DEFAULT: <code>mean</code> </p> <code>hidden_size</code> <p>The size of the hidden layer. If None, no projection is done and the output of the span pooler is used directly.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p>"},{"location":"pipes/trainable/embeddings/text_cnn/","title":"Text CNN","text":"<p>The <code>eds.text_cnn</code> component is a simple 1D convolutional network to contextualize word embeddings (as computed by the <code>embedding</code> component passed as argument).</p> <p>To be memory efficient when handling batches of variable-length sequences, this module employs sequence packing, while taking care of avoiding contamination between the different docs.</p>"},{"location":"pipes/trainable/embeddings/text_cnn/#edsnlp.pipes.trainable.embeddings.text_cnn.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> </p> <code>embedding</code> <p>Embedding module to apply to the input</p> <p> TYPE: <code>TorchComponent[WordEmbeddingBatchOutput, BatchInput]</code> </p> <code>output_size</code> <p>Size of the output embeddings Defaults to the <code>input_size</code></p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>out_channels</code> <p>Number of channels</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>kernel_sizes</code> <p>Window size of each kernel</p> <p> TYPE: <code>Sequence[int]</code> DEFAULT: <code>(3, 4, 5)</code> </p> <code>activation</code> <p>Activation function to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>relu</code> </p> <code>residual</code> <p>Whether to use residual connections</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>normalize</code> <p>Whether to normalize before or after the residual connection</p> <p> TYPE: <code>Literal['pre', 'post', 'none']</code> DEFAULT: <code>pre</code> </p>"},{"location":"pipes/trainable/embeddings/transformer/","title":"Transformer","text":"<p>The <code>eds.transformer</code> component is a wrapper around HuggingFace's transformers library. If you are not familiar with transformers, a good way to start is the Illustrated Transformer tutorial.</p> <p>Compared to using the raw Huggingface model, we offer a simple mechanism to split long documents into strided windows before feeding them to the model.</p>"},{"location":"pipes/trainable/embeddings/transformer/#edsnlp.pipes.trainable.embeddings.transformer.factory.create_component--windowing","title":"Windowing","text":"<p>EDS-NLP's Transformer component splits long documents into smaller windows before feeding them to the model. This is done to avoid hitting the maximum number of tokens that can be processed by the model on a single device. The window size and stride can be configured using the <code>window</code> and <code>stride</code> parameters. The default values are 512 and 256 respectively, which means that the model will process windows of 512 tokens, each separated by 256 tokens. Whenever a token appears in multiple windows, the embedding of the \"most contextualized\" occurrence is used, i.e. the occurrence that is the closest to the center of its window.</p> <p>Here is an overview how this works to produce embeddings (shown in red) for each word of the document :</p> <p></p>"},{"location":"pipes/trainable/embeddings/transformer/#edsnlp.pipes.trainable.embeddings.transformer.factory.create_component--examples","title":"Examples","text":"<p>Here is an example of how to define a pipeline with a Transformer component:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.transformer(\n        model=\"prajjwal1/bert-tiny\",\n        window=128,\n        stride=96,\n    ),\n)\n</code></pre> <p>You can then compose this embedding with a task specific component such as <code>eds.ner_crf</code>.</p>"},{"location":"pipes/trainable/embeddings/transformer/#edsnlp.pipes.trainable.embeddings.transformer.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline instance</p> <p> </p> <code>name</code> <p>The component name</p> <p> </p> <code>model</code> <p>The Huggingface model name or path</p> <p> </p> <code>window</code> <p>The window size to use when splitting long documents into smaller windows before feeding them to the Transformer model (default: 512 = 512 - 2)</p> <p> DEFAULT: <code>128</code> </p> <code>stride</code> <p>The stride (distance between windows) to use when splitting long documents into smaller windows: (default: 96)</p> <p> DEFAULT: <code>96</code> </p> <code>training_stride</code> <p>If False, the stride will be set to the window size during training, meaning that there will be no overlap between windows. If True, the stride will be set to the <code>stride</code> parameter during training, just like during inference.</p> <p> DEFAULT: <code>True</code> </p> <code>max_tokens_per_device</code> <p>The maximum number of tokens that can be processed by the model on a single device. This does not affect the results but can be used to reduce the memory usage of the model, at the cost of a longer processing time.</p> <p>If \"auto\", the component will try to estimate the maximum number of tokens that can be processed by the model on the current device at a given time.</p> <p> DEFAULT: <code>auto</code> </p> <code>span_getter</code> <p>Which spans of the document should be embedded. Defaults to the full document if None.</p> <p> DEFAULT: <code>None</code> </p>"},{"location":"training/","title":"Index","text":""},{"location":"training/loggers/","title":"Loggers","text":"<p>When training a model, it is important to keep track of the training process, model performance at different stages, and statistics about the training data over time. This is where loggers come in. Loggers are used to store such information to be able to analyze and visualize it later.</p> <p>The EDS-NLP training API (<code>edsnlp.train</code>) relies on accelerate's integration of popular loggers, as well as a few custom loggers. You can configure loggers in <code>edsnlp.train</code> via the <code>logger</code> parameter of the <code>train</code> function by specifying:</p> <ul> <li> <p>a string or a class instance or partially initialized class instance of a logger, e.g.</p> Via the Python APIVia a config file <pre><code>from edsnlp.training.loggers import CSVLogger\nfrom edsnlp.training import train\n\nlogger = CSVLogger.draft()\ntrain(..., logger=logger)\n# or train(..., logger=\"csv\")\n</code></pre> <pre><code>train:\n...\nlogger:\n\"@loggers\": csv\n...\n</code></pre> </li> <li> <p>or a list of string / logger instances, e.g.</p> Via the Python APIVia a config file <pre><code>from edsnlp.training.loggers import CSVLogger\nfrom edsnlp.training import train\n\nloggers = [\"tensorboard\", CSVLogger.draft(...)]\ntrain(..., logger=loggers)\n</code></pre> <pre><code>train:\n...\nlogger:\n- tensorboard  # as a string\n- \"@loggers\": csv !draft\n...\n</code></pre> </li> </ul> <p>Draft objects</p> <p><code>edsnlp.train</code> can provide a default project name and logging dir for loggers that require these parameters. For these loggers, if you don't want to set the project name yourself, you can either:</p> <ul> <li>call <code>CSVLogger.draft(...)</code> without the normal init parameters minus the <code>project_name</code> or <code>logging_dir</code> parameters,   which will cause a <code>Draft[CSVLogger]</code> object to be returned if some required parameters are missing</li> <li>or use <code>\"@loggers\": csv !draft</code> in the config file, which will also cause a <code>Draft[CSVLogger]</code> object to be returned if some required   parameters are missing</li> <li>use the shorthand <code>logger: [\"csv\", \"tensorboard\", ...]</code>, which will use the default project name and logging dir</li> </ul> <p>The supported loggers are listed below.</p>"},{"location":"training/loggers/#edsnlp.training.loggers.RichLogger","title":"RichLogger","text":"<p>A logger that displays logs in a Rich-based table using rich-logger. This logger is also available via the loggers registry as <code>rich</code>.</p> <p>No Disk Logging</p> <p>This logger doesn't save logs to disk. It's meant for displaying logs in a pretty table during training. If you need to save logs to disk, consider combining this logger with any other logger.</p>"},{"location":"training/loggers/#edsnlp.training.loggers.RichLogger.__init__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>fields</code> <p>Field descriptors containing goal (\"lower_is_better\" or \"higher_is_better\"),  format and display name The key is a regex that will be used to match the fields to log Each entry of the dictionary should match the following scheme:</p> <ul> <li>key: a regex to match columns</li> <li>value: either a Dict or False to hide the column, the dict format is<ul> <li>name: the name of the column</li> <li>goal: \"lower_is_better\" or \"higher_is_better\"</li> </ul> </li> </ul> <p>This defaults to a set of metrics and stats that are commonly logged during EDS-NLP training.</p> <p> TYPE: <code>Dict[str, Union[Dict, bool]]</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>Key to group the logs</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>hijack_tqdm</code> <p>Whether to replace the tqdm progress bar with a rich progress bar. Indeed, rich progress bars integrate better with the rich table.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"training/loggers/#edsnlp.training.loggers.CSVLogger","title":"CSVLogger","text":"<p>A simple CSV-based logger that writes logs to a CSV file. By default, with <code>edsnlp.train</code> the CSV file is located under a local directory <code>${CWD}/artifact/metrics.csv</code>.</p> <p>Consistent Keys</p> <p>This logger expects that the <code>values</code> dictionary passed to <code>log</code> has consistent keys across all calls. If a new key is encountered in a subsequent call, it will be ignored and a warning will be issued.</p>"},{"location":"training/loggers/#edsnlp.training.loggers.CSVLogger.__init__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>logging_dir</code> <p>Directory in which to store the CSV.</p> <p> TYPE: <code>str or PathLike</code> </p> <code>file_name</code> <p>Name of the CSV file. Defaults to \"metrics.csv\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'metrics.csv'</code> </p>"},{"location":"training/loggers/#edsnlp.training.loggers.JSONLogger","title":"JSONLogger","text":"<p>A simple JSON-based logger that writes logs to a JSON file as a list of dictionaries. By default, with <code>edsnlp.train</code> the JSON file is located under a local directory <code>${CWD}/artifact/metrics.json</code>.</p> <p>This method is not recommended for large and frequent logging, as it re-writes the entire JSON file on every call. Prefer <code>CSVLogger</code> for frequent and heavy logging.</p>"},{"location":"training/loggers/#edsnlp.training.loggers.JSONLogger.__init__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>logging_dir</code> <p>Directory in which to store the JSON file.</p> <p> TYPE: <code>str or PathLike</code> </p> <code>file_name</code> <p>Name of the JSON file. Defaults to \"metrics.json\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'metrics.json'</code> </p>"},{"location":"training/loggers/#edsnlp.training.loggers.TensorBoardLogger","title":"TensorBoardLogger","text":"<p>Logger for TensorBoard. This logger is also available via the loggers registry as <code>tensorboard</code>.</p>"},{"location":"training/loggers/#edsnlp.training.loggers.TensorBoardLogger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>project_name</code> <p>Name of the project.</p> <p> TYPE: <code>str</code> </p> <code>logging_dir</code> <p>Directory in which to store the TensorBoard logs. Logs of different runs will be stored in <code>logging_dir/project_name</code>. The environment variable <code>TENSORBOARD_LOGGING_DIR</code> takes precedence over this argument.</p> <p> TYPE: <code>Optional[Union[str, PathLike]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to <code>tensorboard.SummaryWriter</code>.</p> <p> </p>"},{"location":"training/loggers/#edsnlp.training.loggers.AimLogger","title":"AimLogger","text":"<p>Logger for Aim.</p>"},{"location":"training/loggers/#edsnlp.training.loggers.AimLogger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>project_name</code> <p>Name of the project.</p> <p> TYPE: <code>str</code> </p> <code>logging_dir</code> <p>Directory in which to store the Aim logs. The environment variable <code>AIM_LOGGING_DIR</code> takes precedence over this argument.</p> <p> TYPE: <code>Optional[Union[str, PathLike]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the Aim init function.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"training/loggers/#edsnlp.training.loggers.WandBLogger","title":"WandBLogger","text":"<p>Logger for Weights &amp; Biases. This logger is also available via the loggers registry as <code>wandb</code>.</p>"},{"location":"training/loggers/#edsnlp.training.loggers.WandBLogger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>project_name</code> <p>Name of the project. This will become the <code>project</code> parameter in <code>wandb.init</code>.</p> <p> TYPE: <code>str</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the WandB init function.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"training/loggers/#edsnlp.training.loggers.MLflowLogger","title":"MLflowLogger","text":"<p>Logger for MLflow. This logger is also available via the loggers registry as <code>mlflow</code>.</p>"},{"location":"training/loggers/#edsnlp.training.loggers.MLflowLogger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>project_name</code> <p>Name of the project. This will become the mlflow experiment name.</p> <p> TYPE: <code>str</code> </p> <code>logging_dir</code> <p>Directory in which to store the MLflow logs.</p> <p> TYPE: <code>Optional[Union[str, PathLike]]</code> DEFAULT: <code>None</code> </p> <code>run_id</code> <p>If specified, get the run with the specified UUID and log parameters and metrics under that run. The run\u2019s end time is unset and its status is set to running, but the run\u2019s other attributes (source_version, source_type, etc.) are not changed. Environment variable MLFLOW_RUN_ID has priority over this argument.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>An optional <code>dict</code> of <code>str</code> keys and values, or a <code>str</code> dump from a <code>dict</code>, to set as tags on the run. If a run is being resumed, these tags are set on the resumed run. If a new run is being created, these tags are set on the new run. Environment variable MLFLOW_TAGS has priority over this argument.</p> <p> TYPE: <code>Optional[Union[Dict[str, Any], str]]</code> DEFAULT: <code>None</code> </p> <code>nested_run</code> <p>Controls whether run is nested in parent run. True creates a nested run. Environment variable MLFLOW_NESTED_RUN has priority over this argument.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> <code>run_name</code> <p>Name of new run (stored as a mlflow.runName tag). Used only when <code>run_id</code> is unspecified.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>description</code> <p>An optional string that populates the description box of the run. If a run is being resumed, the description is set on the resumed run. If a new run is being created, the description is set on the new run.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p>"},{"location":"training/loggers/#edsnlp.training.loggers.CometMLLogger","title":"CometMLLogger","text":"<p>Logger for CometML. This logger is also available via the loggers registry as <code>cometml</code>.</p>"},{"location":"training/loggers/#edsnlp.training.loggers.CometMLLogger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>project_name</code> <p>Name of the project.</p> <p> TYPE: <code>str</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the CometML Experiment object.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"training/loggers/#edsnlp.training.loggers.DVCLiveLogger","title":"DVCLiveLogger","text":""},{"location":"training/training-api/","title":"Training API","text":"<p>Under the hood, EDS-NLP uses PyTorch to train and run deep-learning models. EDS-NLP acts as a sidekick to PyTorch, providing a set of tools to perform preprocessing, composition and evaluation. The trainable <code>TorchComponents</code> are actually PyTorch modules with a few extra methods to handle the feature preprocessing and postprocessing. Therefore, EDS-NLP is fully compatible with the PyTorch ecosystem.</p> <p>To build and train a deep learning model, you can either build a training script from scratch (check out the Make a training script tutorial), or use the provided training API. The training API is designed to be flexible and can handle various types of models, including Named Entity Recognition (NER) models, span classifiers, and more. However, if you need more control over the training process, consider writing your own training script.</p> <p>EDS-NLP supports training models either from the command line or from a Python script or notebook, and switching between the two is relatively straightforward thanks to the use of Confit.</p> A word about Confit <p>EDS-NLP makes heavy use of Confit, a configuration library that allows you call functions from Python or the CLI, and validate and optionally cast their arguments.</p> <p>The EDS-NLP function described on this page is the <code>train</code> function of the <code>edsnlp.train</code> module. When passing a dict to a type-hinted argument (either from a <code>config.yml</code> file, or by calling the function in Python), Confit will instantiate the correct class with the arguments provided in the dict. For instance, we pass a dict to the <code>train_data</code> parameter, which is actually type hinted as a <code>TrainingData</code>: this dict will actually be used as keyword arguments to instantiate this <code>TrainingData</code> object. You can also instantiate a <code>TrainingData</code> object directly and pass it to the function.</p> <p>You can also tell Confit specifically which class you want to instantiate by using the <code>@register_name = \"name_of_the_registered_class\"</code> key and value in a dict or config section. We make a heavy use of this mechanism to build pipeline architectures.</p>"},{"location":"training/training-api/#how-it-works","title":"How it works","text":"<p>To train a model with EDS-NLP, you need the following ingredients:</p> <ul> <li> <p>Pipeline: a pipeline with at least one trainable component. Components that share parameters or that must be updated together are trained in the same phase.</p> </li> <li> <p>Training streams: one or more streams of documents wrapped in a TrainingData object. Each of these specifies how to shuffle the stream, how to batch it with a stat expression such as <code>2000 words</code> or <code>16 spans</code>, whether to split batches into sub batches for gradient accumulation, and which components it feeds.</p> </li> <li> <p>Validation streams: optional streams of documents used for periodic evaluation.</p> </li> <li> <p>Scorer: a scorer that defines the metrics to compute on the validation set. By default, it reports speed and uses autocast during scoring unless disabled.</p> </li> <li> <p>Optimizer: an optimizer. Defaults to AdamW with linear warmup and two groups of parameters, one for the transformer with lr 5\u202210^-5, and one for the rest of the model with lr 3\u202210^-4.</p> </li> <li> <p>A bunch of hyperparameters: finally, the function expects various hyperparameters (most of them set to sensible defaults) to the function, such as <code>max_steps</code>, <code>seed</code>, <code>validation_interval</code>, <code>checkpoint_interval</code>, <code>grad_max_norm</code>, and more.</p> </li> </ul> <p>The training then proceeds in several steps:</p> <p>Setup The function prepares the device with Accelerate, creates the output folders, materializes the validation set from the user-provided stream, and runs a post-initialization pass on the training data when requested. This <code>post_init</code> op let's the pipeline inspect the data before learning to adjust the number of heads depending on the labels encountered. Finally, the optimizer is instantiated.</p> <p>Phases Training runs by phases. A phase groups components that should be optimized together because they share parameters (think for instance of a BERT shared between multiple models). During a phase, losses are computed for each of these \"active\" components at each step, and only their parameters are updated.</p> <p>Data preparation Each TrainingData object turns its streams of documents into device ready batches. It optionally shuffles the stream, preprocess the documents for the active components, builds stat-aware batches (for instance, limiting the number of tokens per batch), optionally splits batches into sub batches for gradient accumulation, then converts everything into device-ready tensors. This can be done in parallel to the actual deep-learning work.</p> <p>Optimization For every training step the function draws one batch from each training stream (in case there are more than one) and synchronizes statistics across processes (in case we're doing multi-GPU training) to keep supports and losses consistent. It runs forward passes for the phase components. When several components reuse the same intermediate features a cache avoids recomputation. Gradients are accumulated over sub batches.</p> <p>Gradient safety Gradients are always clipped to <code>grad_max_norm</code>. Optionally the function tracks an exponential moving mean and variance of the gradient norm. If a spike is detected you can clip to the running mean or to a threshold or skip the update depending on <code>grad_dev_policy</code>. This protects training from rare extreme updates.</p> <p>Validation and logging At regular intervals the scorer evaluates the pipeline on the validation documents. It isolates each task by copying docs and disabling unrelated pipes to avoid leakage. It reports throughput and metrics for NER and span attribute classifiers plus any custom metrics.</p> <p>Checkpoints and output The model is saved on schedule and at the end in <code>output_dir/model-last</code> unless saving is disabled.</p>"},{"location":"training/training-api/#tutorials-and-examples","title":"Tutorials and examples","text":"<p> Writing a training script</p><p>Learn how EDS-NLP handles training deep-neural networks, and how to write a training script on your own.</p><p> Training a NER model</p><p>Learn how to quickly train a NER model with <code>edsnlp.train</code>.</p><p> Training a Span Classifier model</p><p>Learn how to quickly train a biopsy date classifier model model with <code>edsnlp.train</code>.</p><p> Hyperparameter Tuning</p><p>Learn how to tune hyperparameters of a model with <code>edsnlp.tune</code>.</p>"},{"location":"training/training-api/#edsnlp.training.trainer.train","title":"Parameters of <code>edsnlp.train</code>","text":"<p>Here are the parameters you can pass to the <code>train</code> function:</p> PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline that will be trained in place.</p> <p> TYPE: <code>Pipeline</code> </p> <code>train_data</code> <p>The training data. Can be a single TrainingData object, a dict that will be cast or a list of these objects.</p> <code>TrainingData</code> object/dictionary PARAMETER DESCRIPTION <code>data</code> <p>The stream of documents to train on. The documents will be preprocessed and collated according to the pipeline's components.</p> <p> TYPE: <code>Stream</code> </p> <code>batch_size</code> <p>The batch size. Can be a batching expression like \"2000 words\", an int (number of documents), or a tuple (batch_size, batch_by). The batch_by argument should be a statistic produced by the pipes that will be trained. For instance, the <code>eds.span_pooler</code> component produces a \"spans\" statistic, that can be used to produce batches of no more than 16 spans by setting batch_size to \"16 spans\".</p> <p> TYPE: <code>BatchSizeArg</code> </p> <code>shuffle</code> <p>The shuffle strategy. Can be \"dataset\" to shuffle the entire dataset (this can be memory-intensive for large file based datasets), \"fragment\" to shuffle the fragment-based datasets like parquet files, or a batching expression like \"2000 words\" to shuffle the dataset in chunks of 2000 words.</p> <p> TYPE: <code>Union[str, Literal[False]]</code> </p> <code>sub_batch_size</code> <p>How to split each batch into sub-batches that will be fed to the model independently to accumulate gradients over. To split a batch of 8000 tokens into smaller batches of 1000 tokens each, just set this to \"1000 tokens\".</p> <p>You can also request a number of splits, like \"4 splits\", to split the batch into N parts each close to (but less than) batch_size / N.</p> <p> TYPE: <code>Optional[BatchSizeArg]</code> DEFAULT: <code>None</code> </p> <code>pipe_names</code> <p>The names of the pipes that should be trained on this data. If None, defaults to all trainable pipes.</p> <p> TYPE: <code>Optional[AsList[str]]</code> DEFAULT: <code>None</code> </p> <code>post_init</code> <p>Whether to call the pipeline's post_init method with the data before training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p> TYPE: <code>AsList[TrainingData]</code> </p> <code>val_data</code> <p>The validation data. Can be a single Stream object or a list of Stream.</p> <p> TYPE: <code>AsList[Stream]</code> DEFAULT: <code>[]</code> </p> <code>seed</code> <p>The random seed</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>max_steps</code> <p>The maximum number of training steps</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>optimizer</code> <p>The optimizer. If None, a default optimizer will be used.</p> <code>ScheduledOptimizer</code> object/dictionary PARAMETER DESCRIPTION <code>optim</code> <p>The optimizer to use. If a string (like \"adamw\") or a type to instantiate, the <code>module</code> and <code>groups</code> must be provided.</p> <p> TYPE: <code>Union[str, Type[Optimizer], Optimizer]</code> </p> <code>module</code> <p>The module to optimize. Usually the <code>nlp</code> pipeline object.</p> <p> TYPE: <code>Optional[Union[PipelineProtocol, Module]]</code> DEFAULT: <code>None</code> </p> <code>total_steps</code> <p>The total number of steps, used for schedules.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>groups</code> <p>The groups to optimize. Each group is a dictionary containing:</p> <ul> <li>a regex <code>selector</code> key to match the parameter of that group by their names   (as listed by <code>nlp.named_parameters()</code>)</li> <li>and several other keys that define the optimizer parameters for that   group, such as <code>lr</code>, <code>weight_decay</code> etc. The value for these keys can   be a <code>Schedule</code> instance or a simple value</li> <li>an <code>exclude</code> key that can be set to True to exclude parameters</li> </ul> <p>The matching is performed by running <code>regex.search(selector, name)</code> so you do not have to match the full name. Note that the order of the groups matters. If a parameter name matches multiple selectors, the configurations of these selectors are combined in reverse order (from the last matched selector to the first), allowing later selectors to complete options from earlier ones. If a selector contains <code>exclude=True</code>, any parameter matching it is excluded from optimization.</p> <p> TYPE: <code>Optional[List[Group]]</code> DEFAULT: <code>None</code> </p> <p> TYPE: <code>Union[Draft[ScheduledOptimizer], ScheduledOptimizer, Optimizer]</code> DEFAULT: <code>None</code> </p> <code>validation_interval</code> <p>The number of steps between each evaluation. Defaults to 1/10 of max_steps</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>checkpoint_interval</code> <p>The number of steps between each model save. Defaults to validation_interval</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>grad_max_norm</code> <p>The maximum gradient norm</p> <p> TYPE: <code>float</code> DEFAULT: <code>5.0</code> </p> <code>grad_dev_policy</code> <p>The policy to apply when a gradient spike is detected, ie. when the gradient norm is higher than the mean + std * grad_max_dev. Can be:</p> <ul> <li>\"clip_mean\": clip the gradients to the mean gradient norm</li> <li>\"clip_threshold\": clip the gradients to the mean + std * grad_max_dev</li> <li>\"skip\": skip the step</li> </ul> <p>These do not apply to <code>grad_max_norm</code> that is always enforced when it is not None, since <code>grad_max_norm</code> is not adaptive and would most likely prohibit the model from learning during the early stages of training when gradients are expected to be high.</p> <p> TYPE: <code>Optional[Literal['clip_mean', 'clip_threshold', 'skip']]</code> DEFAULT: <code>None</code> </p> <code>grad_ewm_window</code> <p>Approximately how many steps should we look back to compute the average gradient norm and variance to detect gradient deviation spikes.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>grad_max_dev</code> <p>The threshold to apply to detect gradient spikes. A spike is detected when the value is higher than the mean + variance * threshold.</p> <p> TYPE: <code>float</code> DEFAULT: <code>7.0</code> </p> <code>loss_scales</code> <p>The loss scales for each component (useful for multi-task learning)</p> <p> TYPE: <code>Dict[str, float]</code> DEFAULT: <code>{}</code> </p> <code>scorer</code> <p>How to score the model. Expects a <code>GenericScorer</code> object or a dict containing a mapping of metric names to metric objects.</p> <code>GenericScorer</code> object/dictionary PARAMETER DESCRIPTION <code>batch_size</code> <p>The batch size to use for scoring. Can be an int (number of documents) or a string (batching expression like \"2000 words\").</p> <p> TYPE: <code>Union[int, str]</code> DEFAULT: <code>1</code> </p> <code>speed</code> <p>Whether to compute the model speed (words/documents per second)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>autocast</code> <p>Whether to use autocasting for mixed precision during the evaluation, defaults to True.</p> <p> TYPE: <code>Union[bool, Any]</code> DEFAULT: <code>None</code> </p> <code>metrics</code> <p>A keyword arguments mapping of metric names to metrics objects. See the metrics documentation for more info.</p> <p> DEFAULT: <code>{}</code> </p> <p> TYPE: <code>GenericScorer</code> DEFAULT: <code>GenericScorer()</code> </p> <code>num_workers</code> <p>The number of workers to use for preprocessing the data in parallel. Setting it to 0 means no parallelization : data is processed on the main thread which may induce latency slow down the training. To avoid this, a good practice consist in doing the preprocessing either before training or in parallel in a separate process. Because of how EDS-NLP handles stream multiprocessing, changing this value will affect the order of the documents in the produces batches. A stream [1, 2, 3, 4, 5, 6] split in batches of size 3 will produce:</p> <ul> <li>[1, 2, 3] and [4, 5, 6] with 1 worker</li> <li>[1, 3, 5] and [2, 4, 6] with 2 workers</li> </ul> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>cpu</code> <p>Whether to use force training on CPU. On MacOS, this might be necessary to get around some <code>mps</code> backend issues.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>mixed_precision</code> <p>The mixed precision mode. Can be \"no\", \"fp16\", \"bf16\" or \"fp8\".</p> <p> TYPE: <code>Literal['no', 'fp16', 'bf16', 'fp8']</code> DEFAULT: <code>'no'</code> </p> <code>output_dir</code> <p>The output directory, which will contain a <code>model-last</code> directory with the last model, and a <code>train_metrics.json</code> file with the training metrics and stats.</p> <p> TYPE: <code>Union[Path, str]</code> DEFAULT: <code>Path('artifacts')</code> </p> <code>output_model_dir</code> <p>The directory where to save the model. If None, defaults to <code>output_dir / \"model-last\"</code>.</p> <p> TYPE: <code>Optional[Union[Path, str]]</code> DEFAULT: <code>None</code> </p> <code>save_model</code> <p>Whether to save the model or not. This can be useful if you are only interested in the metrics, but no the model, and want to avoid spending time dumping the model weights to the disk.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>logger</code> <p>The logger to use. Can be a boolean to use the default loggers (rich and json), a list of logger names, or a list of logger objects.</p> <p>You can use huggingface accelerate integrated loggers (<code>tensorboard</code>, <code>wandb</code>, <code>comet_ml</code>, <code>aim</code>, <code>mlflow</code>, <code>clearml</code>, <code>dvclive</code>), or EDS-NLP simple loggers, or a combination of both:</p> <ul> <li><code>csv</code>: logs to a CSV file in <code>output_dir</code> (<code>artifacts/metrics.csv</code>)</li> <li><code>json</code>: logs to a JSON file in <code>output_dir</code> (<code>artifacts/metrics.json</code>)</li> <li><code>rich</code>: logs to a rich table in the terminal</li> </ul> <p> TYPE: <code>Union[bool, AsList[Union[str, GeneralTracker, Draft[GeneralTracker]]]]</code> DEFAULT: <code>True</code> </p> <code>log_weight_grads</code> <p>Whether to log the weight gradients during training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>on_validation_callback</code> <p>A callback function invoked during validation steps to handle custom logic.</p> <p> TYPE: <code>Optional[Callable[[Dict], None]]</code> DEFAULT: <code>None</code> </p> <code>project_name</code> <p>The project name, used to group experiments in some loggers. If None, defaults to the path of the config file, relative to the home directory, with slashes replaced by double underscores.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>We provide step-by-step guides to get you started. We cover the following use-cases:</p>"},{"location":"tutorials/#base-tutorials","title":"Base tutorials","text":"<p> Spacy representations</p><p>Learn the basics of how documents are represented with spaCy.</p><p> Matching a terminology</p><p>Extract phrases that belong to a given terminology.</p><p> Qualifying entities</p><p>Ensure extracted concepts are not invalidated by linguistic modulation.</p><p> Detecting dates</p><p>Detect and parse dates in a text.</p><p> Processing multiple texts</p><p>Improve the inference speed of your pipeline</p><p> Detecting hospitalisation reason</p><p>Identify spans mentioning the reason for hospitalisation or tag entities as the reason.</p><p>\u21b5 Detecting false endlines</p><p>Classify each line end and add the <code>excluded</code> attribute to these tokens.</p><p> Aggregating results</p><p>Aggregate the results of your pipeline at the document level.</p><p> FastAPI</p><p>Deploy your pipeline as an API.</p><p> Visualization</p><p>Quickly visualize the results of your pipeline as annotations or tables.</p>"},{"location":"tutorials/#deep-learning-tutorials","title":"Deep learning tutorials","text":"<p>We also provide tutorials on how to train deep-learning models with EDS-NLP. These tutorials cover the training API, hyperparameter tuning, and more.</p> <p> Writing a training script</p><p>Learn how EDS-NLP handles training deep-neural networks, and how to write a training script on your own.</p><p> Training a NER model</p><p>Learn how to quickly train a NER model with <code>edsnlp.train</code>.</p><p> Training a Span Classifier model</p><p>Learn how to quickly train a biopsy date classifier model model with <code>edsnlp.train</code>.</p><p> Hyperparameter Tuning</p><p>Learn how to tune hyperparameters of a model with <code>edsnlp.tune</code>.</p>"},{"location":"tutorials/aggregating-results/","title":"Aggregating results","text":""},{"location":"tutorials/aggregating-results/#rationale","title":"Rationale","text":"<p>In some cases, you are not interested in individual extractions, but rather in document-level aggregated variables. For instance, you may be interested to know if a patient is diabetic without caring abou the actual mentions of diabetes. Here, we propose a simple and generic rule which work by:</p> <ul> <li>Extracting entities via methods of your choice</li> <li>Qualifiy those entities and discard appropriate entities</li> <li>Set a threshold on the minimal number of entities that should be present in the document to aggregate them.</li> </ul>"},{"location":"tutorials/aggregating-results/#an-example-for-the-disorders-pipes","title":"An example for the disorders pipes","text":"<p>Below is a simple implementation of this aggregation rule (this can be adapted for other comorbidity components and other qualification methods):</p> <pre><code>MIN_NUMBER_ENTITIES = 2  # (1)!\n\nif not Doc.has_extension(\"aggregated\"):\n    Doc.set_extension(\"aggregated\", default={})  # (2)!\n\nspans = doc.spans[\"diabetes\"]  # (3)!\nkept_spans = [\n    (span, span._.status, span._.detailed_status)\n    for span in spans\n    if not any([span._.negation, span._.hypothesis, span._.family])\n]  # (4)!\n\nif len(kept_spans) &lt; MIN_NUMBER_ENTITIES:  # (5)!\n    status = \"ABSENT\"\n\nelse:\n    status = max(kept_spans, key=itemgetter(1))[2]  # (6)!\n\ndoc._.aggregated[\"diabetes\"] = status\n</code></pre> <ol> <li>We want at least 2 correct entities</li> <li>Storing the status in the <code>doc._.aggregated</code> dictionary</li> <li>Getting status for the <code>diabetes</code> component</li> <li>Disregarding entities which are either negated, hypothetical, or not about the patient himself</li> <li>Setting the status to 0 if less than 2 relevant entities are left:</li> <li>Getting the maximum severity status</li> </ol> <ol></ol>"},{"location":"tutorials/detecting-dates/","title":"Detecting dates","text":"<p>We now know how to match a terminology and qualify detected entities, which covers most use cases for a typical medical NLP project. In this tutorial, we'll see how to use EDS-NLP to detect and normalise date mentions using <code>eds.dates</code>.</p> <p>This can have many applications, for dating medical events in particular. The <code>eds.consultation_dates</code> component, for instance, combines the date detection capabilities with a few simple patterns to detect the date of the consultation, when mentioned in clinical reports.</p>"},{"location":"tutorials/detecting-dates/#dates-in-clinical-notes","title":"Dates in clinical notes","text":"<p>Consider the following example:</p> FrenchEnglish <pre><code>Le patient est admis le 21 janvier pour une douleur dans le cou.\nIl se plaint d'une douleur chronique qui a d\u00e9but\u00e9 il y a trois ans.\n</code></pre> <pre><code>The patient is admitted on January 21st for a neck pain.\nHe complains about chronique pain that started three years ago.\n</code></pre> <p>Clinical notes contain many different types of dates. To name a few examples:</p> Type Description Examples Absolute Explicit date <code>2022-03-03</code> Partial Date missing the day, month or year <code>le 3 janvier/on January 3rd</code>, <code>en 2021/in 2021</code> Relative Relative dates <code>hier/yesterday</code>, <code>le mois dernier/last month</code> Duration Durations <code>pendant trois mois/for three months</code> <p>Warning</p> <p>We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library.</p>"},{"location":"tutorials/detecting-dates/#extracting-dates","title":"Extracting dates","text":"<p>The followings snippet adds the <code>eds.dates</code> component to the pipeline:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.dates())  # (1)\n\ntext = (\n    \"Le patient est admis le 21 janvier pour une douleur dans le cou.\\n\"\n    \"Il se plaint d'une douleur chronique qui a d\u00e9but\u00e9 il y a trois ans.\"\n)\n\n# Detecting dates becomes trivial\ndoc = nlp(text)\n\n# Likewise, accessing detected dates is hassle-free\ndates = doc.spans[\"dates\"]  # (2)\n</code></pre> <ol> <li>The date detection component is declared with <code>eds.dates</code></li> <li>Dates are saved in the <code>doc.spans[\"dates\"]</code> key</li> </ol> <p>After this, accessing dates and there normalisation becomes trivial:</p> <pre><code># \u2191 Omitted code above \u2191\n\ndates  # (1)\n# Out: [21 janvier, il y a trois ans]\n</code></pre> <ol> <li><code>dates</code> is a list of spaCy <code>Span</code> objects.</li> </ol>"},{"location":"tutorials/detecting-dates/#normalisation","title":"Normalisation","text":"<p>We can review each date and get its normalisation:</p> <code>date.text</code> <code>date._.date</code> <code>21 janvier</code> <code>{\"day\": 21, \"month\": 1}</code> <code>il y a trois ans</code> <code>{\"direction\": \"past\", \"year\": 3}</code> <p>Dates detected by the pipeline component are parsed into a dictionary-like object. It includes every information that is actually contained in the text.</p> <p>To get a more usable representation, you may call the <code>to_datetime()</code> method. If there's enough information, the date will be represented in a <code>datetime.datetime</code> or <code>datetime.timedelta</code> object. If some information is missing, It will return <code>None</code>. Alternatively for this case, you can optionally set to <code>True</code> the parameter <code>infer_from_context</code> and you may also give a value for <code>note_datetime</code>.</p> <p>Date normalisation</p> <p>Since dates can be missing some information (eg <code>en ao\u00fbt</code>), we refrain from outputting a <code>datetime</code> object in that case. Doing so would amount to guessing, and we made the choice of letting you decide how you want to handle missing dates.</p>"},{"location":"tutorials/detecting-dates/#what-next","title":"What next?","text":"<p>The <code>eds.dates</code> pipe component's role is merely to detect and normalise dates. It is the user's responsibility to use this information in a downstream application.</p> <p>For instance, you could use this pipeline to date medical entities. Let's do that.</p>"},{"location":"tutorials/detecting-dates/#a-medical-event-tagger","title":"A medical event tagger","text":"<p>Our pipeline will detect entities and events separately, and we will post-process the output <code>Doc</code> object to determine whether a given entity can be linked to a date.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom datetime import datetime\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.dates())\nnlp.add_pipe(\n    eds.matcher(\n        regex=dict(admission=[\"admissions?\", \"admise?\", \"prise? en charge\"]),\n        attr=\"LOWER\",\n    )\n)\n\ntext = (\n    \"Le patient est admis le 12 avril pour une douleur \"\n    \"survenue il y a trois jours. \"\n    \"Il avait \u00e9t\u00e9 pris en charge l'ann\u00e9e derni\u00e8re. \"\n    \"Il a \u00e9t\u00e9 diagnostiqu\u00e9 en mai 1995.\"\n)\n\ndoc = nlp(text)\n</code></pre> <p>At this point, the document is ready to be post-processed: its <code>ents</code> and <code>spans[\"dates\"]</code> are populated:</p> <pre><code># \u2191 Omitted code above \u2191\n\ndoc.ents\n# Out: (admis, pris en charge)\n\ndoc.spans[\"dates\"]\n# Out: [12 avril, il y a trois jours, l'ann\u00e9e derni\u00e8re, mai 1995]\n\nnote_datetime = datetime(year=1999, month=8, day=27)\n\nfor i, date in enumerate(doc.spans[\"dates\"]):\n    print(\n        i,\n        \" - \",\n        date,\n        \" - \",\n        date._.date.to_datetime(\n            note_datetime=note_datetime, infer_from_context=False, tz=None\n        ),\n    )\n# Out: 0  -  12 avril  -  None\n# Out: 1  -  il y a trois jours  -  1999-08-24 00:00:00\n# Out: 2  -  l'ann\u00e9e derni\u00e8re  -  1998-08-27 00:00:00\n# Out: 3  -  mai 1995  -  None\n\n\nfor i, date in enumerate(doc.spans[\"dates\"]):\n    print(\n        i,\n        \" - \",\n        date,\n        \" - \",\n        date._.date.to_datetime(\n            note_datetime=note_datetime,\n            infer_from_context=True,\n            tz=None,\n            default_day=15,\n        ),\n    )\n# Out: 0  -  12 avril  -  1999-04-12 00:00:00\n# Out: 1  -  il y a trois jours  -  1999-08-24 00:00:00\n# Out: 2  -  l'ann\u00e9e derni\u00e8re  -  1998-08-27 00:00:00\n# Out: 3  -  mai 1995  -  1995-05-15 00:00:00\n</code></pre> <p>As a first heuristic, let's consider that an entity can be linked to a date if the two are in the same sentence. In the case where multiple dates are present, we'll select the closest one.</p> utils.py<pre><code>from spacy.tokens import Span\nfrom typing import List, Optional\n\n\ndef candidate_dates(ent: Span) -&gt; List[Span]:\n\"\"\"Return every dates in the same sentence as the entity\"\"\"\n    return [date for date in ent.doc.spans[\"dates\"] if date.sent == ent.sent]\n\n\ndef get_event_date(ent: Span) -&gt; Optional[Span]:\n\"\"\"Link an entity to the closest date in the sentence, if any\"\"\"\n\n    dates = candidate_dates(ent)  # (1)\n\n    if not dates:\n        return\n\n    dates = sorted(\n        dates,\n        key=lambda d: min(abs(d.start - ent.end), abs(ent.start - d.end)),\n    )\n\n    return dates[0]  # (2)\n</code></pre> <ol> <li>Get all dates present in the same sentence.</li> <li>Sort the dates, and keep the first item.</li> </ol> <p>We can apply this simple function:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom datetime import datetime\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.dates())\nnlp.add_pipe(\n    eds.matcher(\n        regex=dict(admission=[\"admissions?\", \"admise?\", \"prise? en charge\"]),\n        attr=\"LOWER\",\n    )\n)\n\ntext = (\n    \"Le patient est admis le 12 avril pour une douleur \"\n    \"survenue il y a trois jours. \"\n    \"Il avait \u00e9t\u00e9 pris en charge l'ann\u00e9e derni\u00e8re.\"\n)\n\ndoc = nlp(text)\nnow = datetime.now()\n\nfor ent in doc.ents:\n    if ent.label_ != \"admission\":\n        continue\n    date = get_event_date(ent)\n    print(\n        f\"{ent.text:&lt;20}{date.text:&lt;20}{date._.date.to_datetime(now).strftime('%d/%m/%Y'):&lt;15}{date._.date.to_duration(now)}\"\n    )\n# Out: admis               12 avril            12/04/2023     21 weeks 4 days 6 hours 3 minutes 26 seconds\n# Out: pris en charge      l'ann\u00e9e derni\u00e8re    10/09/2022     -1 year\n</code></pre> <p>Which will output:</p> <code>ent</code> <code>get_event_date(ent)</code> <code>get_event_date(ent)._.date.to_datetime()</code> admis 12 avril <code>2020-04-12T00:00:00+02:00</code> pris en charge l'ann\u00e9e derni\u00e8re <code>-1 year</code> <ol></ol>"},{"location":"tutorials/endlines/","title":"Detecting end-of-lines","text":"<p>A common problem in medical corpus is that the character <code>\\n</code> does not necessarily correspond to a real new line as in other domains.</p> <p>For example, it is common to find texts like:</p> <pre><code>Il doit prendre\nle medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin\ndans 1 mois.\n</code></pre> <p>Inserted new line characters</p> <p>This issue is especially impactful for clinical notes that have been extracted from PDF documents. In that case, the new line character could be deliberately inserted by the doctor, or more likely added to respect the layout during the edition of the PDF.</p> <p>The aim of this tutorial is to train a unsupervised model to detect this false endlines and to use it for inference. The implemented model is based on the work of Zweigenbaum et alZweigenbaum et al., 2016.</p>"},{"location":"tutorials/endlines/#training-the-model","title":"Training the model","text":"<p>Let's train the model using an example corpus of three documents:</p> <pre><code>import edsnlp\nfrom edsnlp.pipes.core.endlines.model import EndLinesModel\n\nnlp = edsnlp.blank(\"eds\")\n\ntext1 = \"\"\"Le patient est arriv\u00e9 hier soir.\nIl est accompagn\u00e9 par son fils\n\nANTECEDENTS\nIl a fait une TS en 2010;\nFumeur, il est arr\u00eat\u00e9 il a 5 mois\nChirurgie de coeur en 2011\nCONCLUSION\nIl doit prendre\nle medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin\ndans 1 mois.\nDIAGNOSTIC :\n\nAntecedents Familiaux:\n- 1. P\u00e8re avec diab\u00e8te\n\"\"\"\n\ntext2 = \"\"\"J'aime le \\nfromage...\\n\"\"\"\ntext3 = (\n    \"/n\"\n    \"Intervention(s) - acte(s) r\u00e9alis\u00e9(s) :/n\"\n    \"Parathyro\u00efdectomie \u00e9lective le [DATE]\"\n)\n\ntexts = [\n    text1,\n    text2,\n    text3,\n]\n\ncorpus = nlp.pipe(texts)\n\n# Fit the model\nendlines = EndLinesModel(nlp=nlp)  # (1)\ndf = endlines.fit_and_predict(corpus)  # (2)\n\n# Save model\nPATH = \"/tmp/path_to_model\"\nendlines.save(PATH)\n</code></pre> <ol> <li>Initialize the <code>EndLinesModel</code>    object and then fit (and predict) in the training corpus.</li> <li>The corpus should be an iterable of edsnlp documents.</li> </ol>"},{"location":"tutorials/endlines/#use-a-trained-model-for-inference","title":"Use a trained model for inference","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nPATH = \"/path_to_model\"\nnlp.add_pipe(eds.endlines(model_path=PATH))  # (1)\nnlp.add_pipe(eds.sentences())  # (1)\n\ndocs = list(nlp.pipe([text1, text2, text3]))\n\ndoc = docs[1]\ndoc\n# Out: J'aime le\n# Out: fromage...\n\nlist(doc.sents)[0]\n# Out: J'aime le\n# Out: fromage...\n</code></pre> <ol> <li>You should specify the path to the trained model here.</li> <li>All fake new line are excluded by setting their <code>tag</code> to 'EXCLUDED' and all true new lines' <code>tag</code> are set to 'ENDLINE'.</li> </ol>"},{"location":"tutorials/endlines/#declared-extensions","title":"Declared extensions","text":"<p>It lets downstream matchers skip excluded tokens (see normalisation) for more detail.</p> <ol><li><p><p>Zweigenbaum P., Grouin C. and Lavergne T., 2016. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (End-of-line classification with no supervision). https://aclanthology.org/2016.jeptalnrecital-poster.7</p></p></li></ol>"},{"location":"tutorials/make-a-training-script/","title":"Writing a training script","text":"<p>In this tutorial, we'll see how we can write our own deep learning model training script with EDS-NLP. We will implement a script to train a named-entity recognition (NER) model.</p> <p>If you do not care about the details and just want to train a model, we suggest that you use the training API and move on to the next tutorial.</p> <p>Hardware requirements</p> <p>Training a modern deep learning model requires a lot of computational resources. We recommend using a machine with a GPU, ideally with at least 16GB of VRAM. If you don't have access to a GPU, you can use a cloud service like Google Colab, Kaggle, Paperspace or Vast.ai.</p> <p>Under the hood, EDS-NLP uses PyTorch to train deep-learning models. EDS-NLP acts as a sidekick to PyTorch, providing a set of tools to perform preprocessing, composition and evaluation. The trainable <code>TorchComponents</code> are actually PyTorch modules with a few extra methods to handle the feature preprocessing and postprocessing. Therefore, EDS-NLP is fully compatible with the PyTorch ecosystem.</p>"},{"location":"tutorials/make-a-training-script/#step-by-step-walkthrough","title":"Step-by-step walkthrough","text":"<p>Training a supervised deep-learning model consists in feeding batches of annotated samples taken from a training corpus to a model and optimizing its parameters of the model to decrease its prediction error. The process of training a pipeline with EDS-NLP is structured as follows:</p>"},{"location":"tutorials/make-a-training-script/#1-defining-the-model","title":"1. Defining the model","text":"<p>We first start by seeding the random states and instantiating a new trainable pipeline composed of trainable pipes. The model described here computes text embeddings with a pre-trained transformer followed by a CNN, and performs the NER prediction task using a Conditional Random Field (CRF) token classifier.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom confit.utils.random import set_seed\n\nset_seed(42)\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.ner_crf(  # (1)!\n        mode=\"joint\",  # (2)!\n        target_span_getter=\"gold-ner\",  # (3)!\n        window=20,\n        embedding=eds.text_cnn(  # (4)!\n            kernel_sizes=[3],\n            embedding=eds.transformer(  # (5)!\n                model=\"prajjwal1/bert-tiny\",  # (6)!\n                window=128,\n                stride=96,\n            ),\n        ),\n    ),\n    name=\"ner\",\n)\n</code></pre> <ol> <li>We use the <code>eds.ner_crf</code> NER task module, which classifies word embeddings into NER labels (BIOUL scheme) using a CRF.</li> <li>Each component of the pipeline can be configured with a dictionary, using the parameter described in the component's page.</li> <li>The <code>target_span_getter</code> parameter defines the name of the span group used to train the NER model. In this case, the model will look for the entities to train on in <code>doc.spans[\"gold-ner\"]</code>. This is important because we might store entities in other span groups with a different purpose (e.g. <code>doc.spans[\"sections\"]</code> contain the sections Spans, but we don't want to train on these). We will need to make sure the entities from the training dataset are assigned to this span group (next section).</li> <li>The word embeddings used by the CRF are computed by a CNN, which builds on top of another embedding layer.</li> <li>The base embedding layer is a pretrained transformer, which computes contextualized word embeddings.</li> <li>We chose the <code>prajjwal1/bert-tiny</code> model in this tutorial for testing purposes, but we recommend using a larger model like <code>bert-base-cased</code> or <code>camembert-base</code> (French) for real-world applications.</li> </ol>"},{"location":"tutorials/make-a-training-script/#2-loading-the-raw-dataset-and-convert-it-into-doc-objects","title":"2. Loading the raw dataset and convert it into Doc objects","text":"<p>To train a pipeline, we must convert our annotated data into <code>Doc</code> objects that will be either used as training samples or evaluation samples. We will assume the dataset is in Standoff format, usually produced by the Brat annotation tool, but any format can be used.</p> <p>At this step, we might also want to perform data augmentation, filtering, splitting or any other data transformation. In this tutorial, we will split on line jumps and filter out empty documents from the training data. We will use our Stream API to handle the data processing, but you can use any method you like, so long as you end up with a collection of <code>Doc</code> objects.</p> <pre><code>import edsnlp\n\n\ndef skip_empty_docs(batch):\n    for doc in batch:\n        if len(doc.ents) &gt; 0:\n            yield doc\n\n\ntraining_data = (\n    edsnlp.data.read_standoff(  # (1)!\n        train_data_path,\n        tokenizer=nlp.tokenizer,  # (2)!\n        span_setter=[\"ents\", \"gold-ner\"],  # (3)!\n    )\n    .map(eds.split(regex=\"\\n\\n\"))  # (4)!\n    .map_batches(skip_empty_docs)  # (5)!\n)\n</code></pre> <ol> <li>Read the data from the brat directory and convert it into Docs.</li> <li>Tokenize the training docs with the same tokenizer as the trained model</li> <li>Store the annotated Brat entities as spans in <code>doc.ents</code>, and <code>doc.spans[\"gold-ner\"]</code></li> <li>Split the documents on line jumps.</li> <li>Filter out empty documents.</li> </ol> <p>As for the validation data, we will keep all the documents, even empty ones, to obtain representative metrics.</p> <pre><code>val_data = edsnlp.data.read_standoff(\n    val_data_path,\n    tokenizer=nlp.tokenizer,\n    span_setter=[\"ents\", \"gold-ner\"],\n)\nval_docs = list(val_data)  # (1)!\n</code></pre> <ol> <li>Cache the stream result into a list of <code>Doc</code></li> </ol>"},{"location":"tutorials/make-a-training-script/#3-complete-the-initialization-of-the-model","title":"3. Complete the initialization of the model","text":"<p>We initialize the missing or incomplete components attributes (such as label vocabularies) with the training dataset. Indeed, when defining the model, we specified the architecture of the model, but we did not specify the types of named entities that the model will predict. This can be done either</p> <ul> <li>explicitly by setting the <code>labels</code> parameter in <code>eds.ner_crf</code> in the definition above,</li> <li>automatically with <code>post_init</code>: then <code>eds.ner_crf</code> looks in <code>doc.spans[target_span_getter]</code> of all docs in <code>training_data</code> to infer the labels.</li> </ul> <pre><code>nlp.post_init(training_data)\n</code></pre>"},{"location":"tutorials/make-a-training-script/#4-making-the-stream-of-mini-batches","title":"4. Making the stream of mini-batches","text":"<p>The training dataset of <code>Doc</code> objects is then preprocessed into features to be fed to the model during the training loop. We will continue to use EDS-NLP's streams to handle the data processing :</p> <ul> <li> <p>We first request the training data stream to loop on the input data, since we want that each example is seen multiple times during the training until a given number of steps is reached</p> Looping in EDS-NLP Streams <p>Note that in EDS-NLP, looping on a stream is always done on the input data, no matter when <code>loop()</code> is called. This means that shuffling or any further preprocessing step will be applied multiple times, each time we loop. This is usually a good thing if preprocessing contains randomness to increase the diversity of the training samples while avoiding loading multiple versions of a same document in memory. To loop after preprocessing, we can collect the stream into a list and loop on the list (<code>edsnlp.data.from_iterable(list(training_data)), loop=True</code>).</p> </li> <li> <p>We shuffle the data before batching to diversify the samples in each mini-batch</p> </li> <li>We extract the features and labels required by each component (and sub-components) of the pipeline</li> <li>Finally, we group the samples into mini-batches, such that each mini-batch contains a maximum number of tokens, or any other batching criterion and assemble (or \"collate\") the features into tensors</li> </ul> <pre><code>from edsnlp.utils.batching import stat_batchify\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # (1)!\nbatches = (\n   training_data.loop()\n    .shuffle(\"dataset\")  # (2)!\n    .map(nlp.preprocess, kwargs={\"supervision\": True})  # (3)!\n    .batchify(batch_size=32 * 128, batch_by=stat_batchify(\"tokens\"))  # (4)!\n    .map(nlp.collate, kwargs={\"device\": device})\n)\n</code></pre> <ol> <li>Check if a GPU is available and set the device accordingly.</li> <li>Apply shuffling to our stream. If our dataset is too large to fit in memory, instead of \"dataset\" we can set the shuffle batch size to \"100 docs\" for example, or \"fragment\" for parquet datasets.</li> <li>This will call the <code>preprocess_supervised</code> method of the TorchComponent class and return a nested dictionary containing the required features and labels.</li> <li>Make batches that contain at most 32 * 128 tokens (e.g. 32 samples of 128 tokens, but this accounts samples may have different lengths). We use the <code>stat_batchify</code> function to look for a key containing <code>tokens</code> in the features <code>stats</code> sub-dictionary and add samples to the batch until the sum of the <code>*tokens*</code> stats exceeds 32 * 128.</li> </ol> <p>and that's it ! We now have a looping stream of mini-batches that we can feed to our model. For better efficiency, we can also perform this in parallel in a separate worker by setting <code>num_cpu_workers</code> to 1 or more. Note that streams in EDS-NLP are lazy, meaning that the execution has not started yet, and the data is not loaded in memory. This will only happen when we start iterating over the stream in the next section.</p> <pre><code>batches = batches.set_processing(\n   num_cpu_workers=1,\n   process_start_method=\"spawn\"  # (1)!\n)\n</code></pre> <ol> <li>Since we use a GPU, we must use the \"spawn\" method to create the workers. This is because the default multiprocessing \"fork\" method is not compatible with CUDA.</li> </ol>"},{"location":"tutorials/make-a-training-script/#5-the-training-loop","title":"5. The training loop","text":"<p>We instantiate a pytorch optimizer and start the training loop</p> <pre><code>from itertools import chain, repeat\nfrom tqdm import tqdm\nimport torch\n\nlr = 3e-4\nmax_steps = 400\n\n# Move the model to the GPU\nnlp.to(device)\n\noptimizer = torch.optim.AdamW(\n    params=nlp.parameters(),\n    lr=lr,\n)\n\niterator = iter(batches)\n\nfor step in tqdm(range(max_steps), \"Training model\", leave=True):\n    batch = next(iterator)\n    optimizer.zero_grad()\n</code></pre>"},{"location":"tutorials/make-a-training-script/#6-optimizing-the-weights","title":"6. Optimizing the weights","text":"<p>Inside the training loop, the trainable components are fed the collated batches from the dataloader by calling the <code>TorchComponent.forward</code> method (via a simple call) to compute the losses. In the case we train a multitask model (not in this tutorial) and the outputs of a shared embedding are reused between components, we enable caching by wrapping this step in a cache context. The training loop is otherwise carried in a similar fashion to a standard pytorch training loop.</p> <pre><code>    with nlp.cache():\n        loss = torch.zeros((), device=device)\n        for name, component in nlp.torch_components():\n            output = component(batch[name])\n            if \"loss\" in output:\n                loss += output[\"loss\"]\n\n    loss.backward()\n\n    optimizer.step()\n</code></pre>"},{"location":"tutorials/make-a-training-script/#7-evaluating-the-model","title":"7. Evaluating the model","text":"<p>Finally, the model is evaluated on the validation dataset and saved at regular intervals. We will use the <code>NerExactMetric</code> to evaluate the NER performance using Precision, Recall and F1 scores. This metric only counts an entity as correct if it matches the label and boundaries of a target entity.</p> <pre><code>from edsnlp.metrics.ner import NerExactMetric\nfrom copy import deepcopy\n\nmetric = NerExactMetric(span_getter=nlp.pipes.ner.target_span_getter)\n\n    ...\n    if ((step + 1) % 100) == 0:\n        with nlp.select_pipes(enable=[\"ner\"]):  # (1)!\n            preds = deepcopy(val_docs)\n            for doc in preds:\n                doc.ents = doc.spans[\"gold-ner\"] = []  # (2)!\n            preds = nlp.pipe(preds)  # (3)!\n            print(metric(val_docs, preds))\n\n    nlp.to_disk(\"model\")  #(4)!\n</code></pre> <ol> <li>In the case we have multiple pipes in our model, we may want to selectively evaluate each pipe, thus we use the <code>select_pipes</code> method to disable every pipe except \"ner\".</li> <li>Clean the documents that our model will annotate</li> <li>We use the <code>pipe</code> method to run the \"ner\" component on the validation dataset. This method is similar to the <code>__call__</code> method of EDS-NLP components, but it is used to run a component on a list of    Docs. This is also equivalent to     <pre><code>preds = (\n    edsnlp.data\n   .from_iterable(preds)\n   .map_pipeline(nlp)\n)\n</code></pre></li> <li>We could also have saved the model with <code>torch.save(model, \"model.pt\")</code>, but <code>nlp.to_disk</code> avoids pickling and allows to inspect the model's files by saving them into a structured directory.</li> </ol>"},{"location":"tutorials/make-a-training-script/#full-example","title":"Full example","text":"<p>Let's wrap the training code in a function, and make it callable from the command line using confit !</p> train.py <pre><code>from copy import deepcopy\nfrom typing import Iterator\n\nimport torch\nfrom confit import Cli\nfrom tqdm import tqdm\n\nimport edsnlp\nimport edsnlp.pipes as eds\nfrom edsnlp.metrics.ner import NerExactMetric\nfrom edsnlp.utils.batching import stat_batchify\n\napp = Cli(pretty_exceptions_show_locals=False)\n\n\n@app.command(name=\"train\", registry=edsnlp.registry)  # (1)!\ndef train_model(\n    nlp: edsnlp.Pipeline,\n    train_data_path: str,\n    val_data_path: str,\n    batch_size: int = 32 * 128,\n    lr: float = 1e-4,\n    max_steps: int = 400,\n    num_preprocessing_workers: int = 1,\n    evaluation_interval: int = 100,\n):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Define function to skip empty docs\n    def skip_empty_docs(batch: Iterator) -&gt; Iterator:\n        for doc in batch:\n            if len(doc.ents) &gt; 0:\n                yield doc\n\n    # Load and process training data\n    training_data = (\n        edsnlp.data.read_standoff(\n            train_data_path,\n            span_setter=[\"ents\", \"gold-ner\"],\n            tokenizer=nlp.tokenizer,\n        )\n        .map(eds.split(regex=\"\\n\\n\"))\n        .map_batches(skip_empty_docs)\n    )\n\n    # Load validation data\n    val_data = edsnlp.data.read_standoff(\n        val_data_path,\n        span_setter=[\"ents\", \"gold-ner\"],\n        tokenizer=nlp.tokenizer,\n    )\n    val_docs = list(val_data)\n\n    # Initialize components\n    nlp.post_init(training_data)\n\n    # Prepare the stream of batches\n    batches = (\n        training_data.loop()\n        .shuffle(\"dataset\")\n        .map(nlp.preprocess, kwargs={\"supervision\": True})\n        .batchify(batch_size=batch_size, batch_by=stat_batchify(\"tokens\"))\n        .map(nlp.collate, kwargs={\"device\": device})\n        .set_processing(num_cpu_workers=1, process_start_method=\"spawn\")\n    )\n\n    # Move the model to the GPU if available\n    nlp.to(device)\n\n    # Initialize optimizer\n    optimizer = torch.optim.AdamW(params=nlp.parameters(), lr=lr)\n\n    metric = NerExactMetric(span_getter=nlp.pipes.ner.target_span_getter)\n\n    # Training loop\n    iterator = iter(batches)\n    for step in tqdm(range(max_steps), \"Training model\", leave=True):\n        batch = next(iterator)\n        optimizer.zero_grad()\n\n        with nlp.cache():\n            loss = torch.zeros((), device=device)\n            for name, component in nlp.torch_components():\n                output = component(batch[name])\n                if \"loss\" in output:\n                    loss += output[\"loss\"]\n\n        loss.backward()\n        optimizer.step()\n\n        # Evaluation and model saving\n        if ((step + 1) % evaluation_interval) == 0:\n            with nlp.select_pipes(enable=[\"ner\"]):\n                # Clean the documents that our model will annotate\n                preds = deepcopy(val_docs)\n                for doc in preds:\n                    doc.ents = doc.spans[\"gold-ner\"] = []\n                preds = nlp.pipe(preds)\n                print(metric(val_docs, preds))\n\n            nlp.to_disk(\"model\")\n\n\nif __name__ == \"__main__\":\n    nlp = edsnlp.blank(\"eds\")\n    nlp.add_pipe(\n        eds.ner_crf(\n            mode=\"joint\",\n            target_span_getter=\"gold-ner\",\n            window=20,\n            embedding=eds.text_cnn(\n                kernel_sizes=[3],\n                embedding=eds.transformer(\n                    model=\"prajjwal1/bert-tiny\",\n                    window=128,\n                    stride=96,\n                ),\n            ),\n        ),\n        name=\"ner\",\n    )\n    train_model(\n        nlp,\n        train_data_path=\"my_brat_data/train\",\n        val_data_path=\"my_brat_data/val\",\n        batch_size=32 * 128,\n        lr=1e-4,\n        max_steps=1000,\n        num_preprocessing_workers=1,\n        evaluation_interval=100,\n    )\n</code></pre> <ol> <li>This will become useful in the next section, when we will use the configuration file to define the pipeline. If you don't want to use a configuration file, you can remove this decorator.</li> </ol> <p>We can now copy the above code in a notebook and run it, or call this script from the command line:</p> <pre><code>python train.py\n</code></pre> <p>At the end of the training, the pipeline is ready to use since every trained component of the pipeline is self-sufficient, ie contains the preprocessing, inference and postprocessing code required to run it.</p>"},{"location":"tutorials/make-a-training-script/#configuration","title":"Configuration","text":"<p>To decouple the configuration and the code of our training script, let's define a configuration file where we will describe both our training parameters and the pipeline. You can either write the config of the pipeline by hand, or generate a pipeline config draft from an instantiated pipeline by running:</p> <pre><code>print(nlp.config.to_yaml_str())\n</code></pre> config.yml<pre><code>nlp:\n\"@core\": \"pipeline\"\nlang: \"eds\"\ncomponents:\nner:\n\"@factory\": \"eds.ner_crf\"\nmode: \"joint\"\ntarget_span_getter: \"gold-ner\"\nwindow: 20\n\nembedding:\n\"@factory\": \"eds.text_cnn\"\nkernel_sizes: [3]\n\nembedding:\n\"@factory\": \"eds.transformer\"\nmodel: \"prajjwal1/bert-tiny\"\nwindow: 128\nstride: 96\n\ntrain:\nnlp: ${ nlp }\ntrain_data_path: my_brat_data/train\nval_data_path: my_brat_data/val\nbatch_size: ${ 32 * 128 }\nlr: 1e-4\nmax_steps: 400\nnum_preprocessing_workers: 1\nevaluation_interval: 100\n</code></pre> <p>And replace the end of the script by</p> <pre><code>if __name__ == \"__main__\":\n    app.run()\n</code></pre> <p>That's it ! We can now call the training script with the configuration file as a parameter, and override some of its values:</p> <pre><code>python train.py --config config.cfg --nlp.components.ner.embedding.embedding.transformer.window=64 --seed 43\n</code></pre>"},{"location":"tutorials/make-a-training-script/#going-further","title":"Going further","text":"<p>EDS-NLP also provides a generic training script that follows the same structure as the one we just wrote. You can learn more about in the next NER model training tutorial through EDS-NLP training API.</p> <p>This tutorial gave you a glimpse of the training API of EDS-NLP. To build a custom trainable component, you can refer to the TorchComponent class or look up the implementation of some of the trainable components on GitHub.</p> <p>We also recommend looking at an existing project as a reference, such as eds-pseudo or mlg-norm.</p>"},{"location":"tutorials/matching-a-terminology/","title":"Matching a terminology","text":"<p>Matching a terminology is perhaps the most basic application of a medical NLP pipeline.</p> <p>In this tutorial, we will cover :</p> <ul> <li>Matching a terminology using spaCy's matchers, as well as RegExps</li> <li>Matching on a specific attribute</li> </ul> <p>You should consider reading the matcher's specific documentation for a description.</p> <p>Comparison to spaCy's matcher</p> <p>spaCy's <code>Matcher</code> and <code>PhraseMatcher</code> use a very efficient algorithm that compare a hashed representation token by token. They are not components by themselves, but can underpin rule-based pipes.</p> <p>EDS-NLP's <code>RegexMatcher</code> lets the user match entire expressions using regular expressions. To achieve this, the matcher has to get to the text representation, match on it, and get back to spaCy's abstraction.</p> <p>The <code>EDSPhraseMatcher</code> lets EDS-NLP reuse spaCy's efficient algorithm, while adding the ability to skip pollution tokens (see the normalizer documentation for detail)</p>"},{"location":"tutorials/matching-a-terminology/#a-simple-use-case-finding-covid19","title":"A simple use case : finding COVID19","text":"<p>Let's try to find mentions of COVID19 and references to patients within a clinical note.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = (\n    \"Motif de prise en charge : probable pneumopathie a COVID19, \"\n    \"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],\n    respiratoire=[\"asthmatique\", \"respiratoire\"],\n)\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.matcher(terms=terms))\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (asthmatique,)\n</code></pre> <p>Let's unpack what happened:</p> <ol> <li>We defined a dictionary of terms to look for, in the form <code>{'label': list of terms}</code>.</li> <li>We declared a spaCy pipeline, and add the <code>eds.matcher</code> component.</li> <li>We applied the pipeline to the texts...</li> <li>... and explored the extracted entities.</li> </ol> <p>This example showcases a limitation of our term dictionary : the phrases <code>COVID19</code> and <code>difficult\u00e9s respiratoires</code> were not detected by the pipeline.</p> <p>To increase recall, we could just add every possible variation :</p> <pre><code>terms = dict(\n-    covid=[\"coronavirus\", \"covid19\"],\n+    covid=[\"coronavirus\", \"covid19\", \"COVID19\"],\n-    respiratoire=[\"asthmatique\", \"respiratoire\"],\n+    respiratoire=[\"asthmatique\", \"respiratoire\", \"respiratoires\"],\n)\n</code></pre> <p>But what if we come across <code>Coronavirus</code>? Surely we can do better!</p>"},{"location":"tutorials/matching-a-terminology/#matching-on-normalised-text","title":"Matching on normalised text","text":"<p>We can modify the matcher's configuration to match on other attributes instead of the verbatim input. You can refer to spaCy's list of available token attributes.</p> <p>Let's focus on two:</p> <ol> <li>The <code>LOWER</code> attribute, which lets you match on a lowercased version of the text.</li> <li>The <code>NORM</code> attribute, which adds some basic normalisation (eg <code>\u0153</code> to <code>oe</code>). EDS-NLP provides a <code>eds.normalizer</code> component that extends the level of cleaning on the <code>NORM</code> attribute.</li> </ol>"},{"location":"tutorials/matching-a-terminology/#the-lower-attribute","title":"The <code>LOWER</code> attribute","text":"<p>Matching on the lowercased version is extremely easy:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = (\n    \"Motif de prise en charge : probable pneumopathie a COVID19, \"\n    \"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],\n    respiratoire=[\"asthmatique\", \"respiratoire\", \"respiratoires\"],\n)\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.matcher(\n        terms=terms,\n        attr=\"LOWER\",  # (1)\n    ),\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (COVID19, respiratoires, asthmatique)\n</code></pre> <ol> <li>The matcher's <code>attr</code> parameter defines the attribute that the matcher will use. It is set to <code>\"TEXT\"</code> by default (ie verbatim text).</li> </ol> <p>This code is complete, and should run as is.</p>"},{"location":"tutorials/matching-a-terminology/#using-the-normalisation-component","title":"Using the normalisation component","text":"<p>EDS-NLP provides its own normalisation component, which modifies the <code>NORM</code> attribute in place. It handles:</p> <ul> <li>removal of accentuated characters;</li> <li>normalisation of quotes and apostrophes;</li> <li>lowercasing, which enabled by default in spaCy \u2013 EDS-NLP lets you disable it;</li> <li>removal of pollution.</li> </ul> <p>Pollution in clinical texts</p> <p>EDS-NLP is meant to be deployed on clinical reports extracted from hospitals information systems. As such, it is often riddled with extraction issues or administrative artifacts that \"pollute\" the report.</p> <p>As a core principle, EDS-NLP never modifies the input text, and <code>nlp(text).text == text</code> is always true. However, we can tag some tokens as pollution elements, and avoid using them for matching the terminology.</p> <p>You can activate it like any other component.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = (\n\"Motif de prise en charge : probable pneumopathie a ===== COVID19, \"  # (1)\n\"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nterms = dict(\ncovid=[\"coronavirus\", \"covid19\", \"pneumopathie \u00e0 covid19\"],  # (2)\nrespiratoire=[\"asthmatique\", \"respiratoire\", \"respiratoires\"],\n)\n\nnlp = edsnlp.blank(\"eds\")\n\n# Add the normalisation component\nnlp.add_pipe(eds.normalizer())  # (3)\nnlp.add_pipe(\n    eds.matcher(\n        terms=terms,\nattr=\"NORM\",  # (4)\nignore_excluded=True,  # (5)\n),\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (pneumopathie a ===== COVID19, respiratoires, asthmatique)\n</code></pre> <ol> <li>We've modified the example to include a simple pollution.</li> <li>We've added <code>pneumopathie \u00e0 covid19</code> to the list of synonyms detected by the pipeline.    Note that in the synonym we provide, we kept the accentuated <code>\u00e0</code>, whereas the example    displays an unaccentuated <code>a</code>.</li> <li>The component can be configured. See the specific documentation for detail.</li> <li>The normalisation lives in the <code>NORM</code> attribute</li> <li>We can tell the matcher to ignore excluded tokens (tokens tagged as pollution by the normalisation component).    This is not an obligation.</li> </ol> <p>Using the normalisation component, you can match on a normalised version of the text, as well as skip pollution tokens during the matching process.</p> <p>Using term matching with the normalisation</p> <p>If you use the term matcher with the normalisation, bear in mind that the examples go through the pipeline. That's how the matcher was able to recover <code>pneumopathie a ===== COVID19</code> despite the fact that we used an accentuated <code>\u00e0</code> in the terminology.</p> <p>The term matcher matches the input text to the provided terminology, using the selected attribute in both cases. The <code>NORM</code> attribute that corresponds to <code>\u00e0</code> and <code>a</code> is the same: <code>a</code>.</p>"},{"location":"tutorials/matching-a-terminology/#preliminary-conclusion","title":"Preliminary conclusion","text":"<p>We have matched all mentions! However, we had to spell out the singular and plural form of <code>respiratoire</code>... And what if we wanted to detect <code>covid 19</code>, or <code>covid-19</code> ? Of course, we could write out every imaginable possibility, but this will quickly become tedious.</p>"},{"location":"tutorials/matching-a-terminology/#using-regular-expressions","title":"Using regular expressions","text":"<p>Let us redefine the pipeline once again, this time using regular expressions. Using regular expressions can help define richer patterns using more compact queries.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = (\n    \"Motif de prise en charge : probable pneumopathie a COVID19, \"\n    \"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nregex = dict(\n    covid=r\"(coronavirus|covid[-\\s]?19)\",\n    respiratoire=r\"respiratoires?\",\n)\nterms = dict(respiratoire=\"asthmatique\")\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.matcher(\n        regex=regex,  # (1)\n        terms=terms,  # (2)\n        attr=\"LOWER\",  # (3)\n    ),\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (COVID19, respiratoires, asthmatique)\n</code></pre> <ol> <li>We can now match using regular expressions.</li> <li>We can mix and match patterns! Here we keep looking for patients using spaCy's term matching.</li> <li>RegExp matching is not limited to the verbatim text! You can choose to use one of spaCy's native attribute, ignore excluded tokens, etc.</li> </ol> <p>To visualize extracted entities, check out the Visualization tutorial.</p> <ol></ol>"},{"location":"tutorials/multiple-texts/","title":"Processing multiple texts","text":"<p>In the previous tutorials, we've seen how to apply a spaCy NLP pipeline to a single text. Once the pipeline is tested and ready to be applied on an entire corpus, we'll want to deploy it efficiently.</p> <p>In this tutorial, we'll cover a few best practices and some caveats to avoid. Then, we'll explore methods that EDS-NLP provides to perform inference on multiple texts.</p> <p>Consider this simple pipeline:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\n\nnlp.add_pipe(\n    eds.matcher(\n        terms=dict(patient=[\"patient\", \"malade\"]),\n        attr=\"NORM\",\n    ),\n)\n\n# Add qualifiers\nnlp.add_pipe(eds.negation())\nnlp.add_pipe(eds.hypothesis())\nnlp.add_pipe(eds.family())\n\n# Add date detection\nnlp.add_pipe(eds.dates())\n</code></pre> <p>Let's deploy it on a large number of documents.</p>"},{"location":"tutorials/multiple-texts/#what-about-a-for-loop","title":"What about a <code>for</code> loop?","text":"<p>Suppose we have a corpus of text:</p> <pre><code>text = (\n    \"Patient admis le 25 septembre 2021 pour suspicion de Covid.\\n\"\n    \"Pas de cas de coronavirus dans ce service.\\n\"\n    \"Le p\u00e8re du patient est atteint du covid.\"\n)\n\ncorpus = [text] * 10000  # (1)\n</code></pre> <ol> <li>This is admittedly ugly. But you get the idea, we have a corpus of 10 000 documents we want to process...</li> </ol> <p>You could just apply the pipeline document by document.</p> <pre><code># \u2191 Omitted code above \u2191\n\ndocs = [nlp(text) for text in corpus]\n</code></pre> <p>Next, you might want to convert these documents to a DataFrame for further analysis or storage. You could do this with a loop like this:</p> <pre><code>import pandas as pd\n\nrows = []\nfor doc in docs:\n    for ent in doc.ents:\n        d = dict(\n            begin=ent.start_char,\n            end=ent.end_char,\n            label=ent.label_,\n            entity_text=ent.text,\n            negation=ent._.negation,\n            hypothesis=ent._.hypothesis,\n            family=ent._.family,\n        )\n        rows.append(d)\n\n    for date in doc.spans.get(\"dates\", []):\n        d = dict(\n            begin=date.start_char,\n            end=date.end_char,\n            label=\"date\",\n            entity_text=date.text,\n            datetime=date._.date.datetime,\n        )\n        rows.append(d)\ndf = pd.DataFrame(rows)\n</code></pre> <p>There are a few issues with this approach:</p> <ul> <li>If our model contains deep learning components (which it does not in this tutorial), we don't benefit from optimized batched matrix operations : ideally, we'd like to process multiple documents at   once.</li> <li>We may have multiple cores available but we don't use them to apply the pipes of our model to multiple documents at the same time.</li> <li>We would also like to perform the Doc -&gt; Dict conversion step in parallel and avoid transferring full Doc instances back and forth between processes.</li> <li>And ideally, being able to switch between input/output formats, or sequential/parallel processing, without changing the code too much.</li> </ul>"},{"location":"tutorials/multiple-texts/#streams-lazy-inference-and-parallelization","title":"Streams, lazy inference and parallelization","text":"<p>To efficiently perform the same operations on multiple documents at once, EDS-NLP uses streams, which record the operations to perform on the documents without actually executing them directly, similar to the way Spark does, or polars with its LazyFrame.</p> <p>This allows EDS-NLP to distribute these operations on multiple cores or machines when it is time to execute them. We can configure how the collection operations are run (how many jobs/workers, how many gpus, whether to use the spark engine) via the stream <code>set_processing()</code> method.</p> <p>For instance,</p> <pre><code>docs = edsnlp.data.from_iterable(corpus)\nprint(docs)  # (1)!\n</code></pre> <ol> <li>Printed version of the stream:     <pre><code>Stream(\n  reader=IterableReader(data=&lt;list object at 0x1084532c0&gt;),\n  ops=[],\n  writer=None)\n</code></pre></li> </ol> <p>as well as any <code>edsnlp.data.read_*</code> or <code>edsnlp.data.from_*</code> return a stream, that we can iterate over or complete with more operations. To apply the model on our collection of documents, we can simply do:</p> <pre><code>docs = docs.map_pipeline(nlp)\n# or \u00e0 la spaCy :\n# docs = nlp.pipe(docs)\nprint(docs)  # (1)!\n</code></pre> <ol> <li>Printed version of the stream:     <pre><code>Stream(\n  reader=IterableReader(data=&lt;list object at 0x1084532c0&gt;),\n  ops=[\n    map(_ensure_doc[&lt;edsnlp.core.pipeline.Pipeline object at 0x14f697f80&gt;]),\n    batchify(size=None, fn=None, sentinel_mode=None),\n    map_batches_op(&lt;edsnlp.pipes.core.sentences.sentences.SentenceSegmenter object at 0x14ac43ce0&gt;),\n    map_batches_op(&lt;edsnlp.pipes.core.normalizer.normalizer.Normalizer object at 0x14c5672c0&gt;),\n    map_batches_op(&lt;edsnlp.pipes.core.matcher.matcher.GenericMatcher object at 0x177013a40&gt;),\n    map_batches_op(&lt;edsnlp.pipes.qualifiers.negation.negation.NegationQualifier object at 0x16a1d1550&gt;),\n    map_batches_op(&lt;edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier object at 0x14ac433b0&gt;),\n    map_batches_op(&lt;edsnlp.pipes.qualifiers.family.family.FamilyContextQualifier object at 0x14f850da0&gt;),\n    map_batches_op(&lt;edsnlp.pipes.misc.dates.dates.DatesMatcher object at 0x1767638c0&gt;),\n    unbatchify()\n  ],\nwriter=None)\n</code></pre></li> </ol> SpaCy vs EDS-NLP <p>SpaCy's <code>nlp.pipe</code> method is not the same as EDS-NLP's <code>nlp.pipe</code> method, and will iterate over anything you pass to it, therefore executing the operations scheduled in our stream.</p> <p>We recommend you instantiate your models using <code>nlp = edsnlp.blank(...)</code> or <code>nlp = edsnlp.load(...)</code>.</p> <p>Otherwise, use the following to apply a spaCy model on a stream <code>docs</code> without triggering its execution:</p> <pre><code>docs = docs.map_pipeline(nlp)\n</code></pre> <p>Finally, we can convert the documents to a DataFrame (or other formats / files) using the <code>edsnlp.data.to_*</code> or <code>edsnlp.data.write_*</code> methods. This triggers the execution of the operations scheduled in the stream and produces the rows of the DataFrame.</p> <p>We can pass our previous Doc to Dict code as a function to the <code>converter</code> parameter of the <code>to_pandas</code> method. Note that this specific conversion is already implemented in EDS-NLP, so you can just pass the string <code>\"ents\"</code> to the <code>converter</code> parameter, and customize the conversion by passing more parameters to the <code>to_pandas</code> method, as described here.</p> <pre><code>def convert_doc_to_rows(doc):\n    entities = []\n\n    for ent in doc.ents:\n        d = dict(\n            start=ent.start_char,\n            end=ent.end_char,\n            label=ent.label_,\n            lexical_variant=ent.text,\n            negation=ent._.negation,\n            hypothesis=ent._.hypothesis,\n            family=ent._.family,\n        )\n        entities.append(d)\n\n    for date in doc.spans.get(\"dates\", []):\n        d = dict(\n            begin=date.start_char,\n            end=date.end_char,\n            label=\"date\",\n            lexical_variant=date.text,\n            datetime=date._.date.datetime,\n        )\n        entities.append(d)\n\n    return entities\n\n\ndf = docs.to_pandas(converter=convert_doc_to_rows)\n# or equivalently:\ndf = docs.to_pandas(\n    converter=\"ents\",\n    span_getter=[\"ents\", \"dates\"],\n    span_attributes=[\n        # span._.*** name: column name\n        \"negation\",\n        \"hypothesis\",\n        \"family\",\n        \"date.datetime\",\n    ],\n)\n</code></pre> <p>We can also iterate over the documents, which also triggers the execution of the operations scheduled in the stream.</p> <pre><code>for doc in docs:\n    # do something with the doc\n    pass\n</code></pre>"},{"location":"tutorials/multiple-texts/#processing-a-dataframe","title":"Processing a DataFrame","text":"<p>Processing text within a pandas DataFrame is a very common use case. In many applications, you'll select a corpus of documents over a distributed cluster, load it in memory and process all texts.</p> <p>The OMOP CDM</p> <p>In every tutorial that mentions distributing EDS-NLP over a corpus of documents, we will expect the data to be organised using a flavour of the OMOP Common Data Model.</p> <p>The OMOP CDM defines two tables of interest to us:</p> <ul> <li>the <code>note</code> table contains the clinical notes</li> <li>the <code>note_nlp</code> table holds the results of   a NLP pipeline applied to the <code>note</code> table.</li> </ul> <p>We can use the <code>converter=\"omop\"</code> argument to the <code>edsnlp.data</code> methods to read data in this format. More information about this converter can be found here.</p> <p>To make sure we can follow along, we propose three recipes for getting the DataFrame: using a dummy dataset like before, loading a CSV or by loading a Spark DataFrame into memory.</p> Dummy exampleLoading data from a CSVLoading data from a Spark DataFrame <pre><code>import pandas as pd\n\ntext = (\n    \"Patient admis le 25 septembre 2021 pour suspicion de Covid.\\n\"\n    \"Pas de cas de coronavirus dans ce service.\\n\"\n    \"Le p\u00e8re du patient est atteint du covid.\"\n)\n\ncorpus = [text] * 1000\n\ndata = pd.DataFrame(dict(note_text=corpus))\ndata[\"note_id\"] = range(len(data))\n</code></pre> <pre><code>import pandas as pd\n\ndata = pd.read_csv(\"note.csv\")\n</code></pre> <pre><code>from pyspark.sql.session import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\ndf = spark.sql(\"SELECT * FROM note\")\ndf = df.select(\"note_id\", \"note_text\")\n\ndata = df.limit(1000).toPandas()  # (1)\n</code></pre> <ol> <li>We limit the size of the DataFrame to make sure we do not overwhelm our machine.</li> </ol> <p>We'll see in what follows how we can efficiently deploy our pipeline on the <code>data</code> object.</p>"},{"location":"tutorials/multiple-texts/#locally-without-parallelization","title":"Locally without parallelization","text":"<pre><code># Read from a dataframe &amp; use the omop converter\ndocs = edsnlp.data.from_pandas(data, converter=\"omop\")\n\n# Add the pipeline to operations that will be run\ndocs = docs.map_pipeline(nlp)\n\n# Convert each doc to a list of dicts (one by entity)\n# and store the result in a pandas DataFrame\nnote_nlp = docs.to_pandas(\n    converter=\"ents\",\n    # Below are the arguments to the converter\n    span_getter=[\"ents\", \"dates\"],\n    span_attributes=[  # (1)\n        # span._.*** name: column name\n        \"negation\",\n        \"hypothesis\",\n        \"family\",\n        \"date.datetime\",\n        # having individual columns for each date part\n        # can be useful for incomplete dates (eg, \"in May\")\n        \"date.day\",\n        \"date.month\",\n        \"date.year\",\n    ],\n)\n</code></pre> <ol> <li>You can just pass a dict if you want to explicitely rename the attributes.</li> </ol> <p>The result on the first note:</p> note_id start end label lexical_variant negation hypothesis family key 0 0 7 patient Patient 0 0 0 ents 0 114 121 patient patient 0 0 1 ents 0 17 34 2021-09-25 25 septembre 2021 nan nan nan dates"},{"location":"tutorials/multiple-texts/#locally-using-multiple-parallel-workers","title":"Locally, using multiple parallel workers","text":"<pre><code># Read from a dataframe &amp; use the omop converter\ndocs = edsnlp.data.from_pandas(data, converter=\"omop\")\n\n# Add the pipeline to operations that will be run\ndocs = docs.map_pipeline(nlp)\n\n# The operations of our stream will be distributed on multiple workers\ndocs = docs.set_processing(backend=\"multiprocessing\")\n# Convert each doc to a list of dicts (one by entity)\n# and store the result in a pandas DataFrame\nnote_nlp = docs.to_pandas(\n    converter=\"ents\",\n    span_getter=[\"ents\", \"dates\"],\n    span_attributes=[\n        \"negation\",\n        \"hypothesis\",\n        \"family\",\n        \"date.datetime\",\n\n        # having individual columns for each date part\n        # can be useful for incomplete dates (eg, \"in May\")\n        \"date.day\",\n        \"date.month\",\n        \"date.year\",\n    ],\n)\n</code></pre> <p>Deterministic processing</p> <p>By default, from version 0.14.0, EDS-NLP dispatches tasks to workers in a round-robin fashion to ensure deterministic processing. This mechanism can be disabled to send documents to workers as soon as they are available, which may result in faster processing but out-of-order results.</p> <p>To disable processing determinism, use <code>set_processing(deterministic=False)</code>. Note that this parameter is only used when using the <code>multiprocessing</code> backend.</p>"},{"location":"tutorials/multiple-texts/#in-a-distributed-fashion-with-spark","title":"In a distributed fashion with spark","text":"<p>To use the Spark engine to distribute the computation, we create our stream from the Spark dataframe directly and write the result to a new Spark dataframe. EDS-NLP will automatically distribute the operations on the cluster (setting <code>backend=\"spark\"</code> behind the scenes), but you can change the backend (for instance to <code>multiprocessing</code> to run locally).</p> <p>Spark backend</p> <p>When processing from AND to a Spark DataFrame, the backend is automatically set to \"spark\".</p> <p>We do NOT recommend using other backend when Spark dataframe are involved, as there may be a discrepancy between the time it takes to process the data locally and the timeout of the spark job.</p> <pre><code># Read from the pyspark dataframe &amp; use the omop converter\ndocs = edsnlp.data.from_spark(df, converter=\"omop\")\n# Add the pipeline to operations that will be run\ndocs = docs.map_pipeline(nlp)\n\n# Backend is set by default to \"spark\"\n# docs = docs.set_processing(backend=\"spark\")\n\n# Convert each doc to a list of dicts (one by entity)\n# and store the result in a pyspark DataFrame\nnote_nlp = docs.to_spark(\nconverter=\"ents\",\n    span_getter=[\"ents\", \"dates\"],\n    span_attributes=[\n        \"negation\",\n        \"hypothesis\",\n        \"family\",\n        \"date.datetime\",\n\n        # having individual columns for each date part\n        # can be useful for incomplete dates (eg, \"in May\")\n        \"date.day\",\n        \"date.month\",\n        \"date.year\",\n    ],\n    dtypes=None,  # (1)\n)\n</code></pre> <ol> <li>If you don't pass a <code>dtypes</code> argument, EDS-NLP will print the inferred schema it such that you can copy-paste it in your code.</li> </ol> <ol></ol>"},{"location":"tutorials/qualifying-entities/","title":"Qualifying entities","text":"<p>In the previous tutorial, we saw how to match a terminology on a text. Using the <code>doc.ents</code> attribute, we can check whether a document mentions a concept of interest to build a cohort or describe patients.</p>"},{"location":"tutorials/qualifying-entities/#the-issue","title":"The issue","text":"<p>However, consider the classical example where we look for the <code>diabetes</code> concept:</p> FrenchEnglish <pre><code>Le patient n'est pas diab\u00e9tique.\nLe patient est peut-\u00eatre diab\u00e9tique.\nLe p\u00e8re du patient est diab\u00e9tique.\n</code></pre> <pre><code>The patient is not diabetic.\nThe patient could be diabetic.\nThe patient's father is diabetic.\n</code></pre> <p>None of these expressions should be used to build a cohort: the detected entity is either negated, speculative, or does not concern the patient themself. That's why we need to qualify the matched entities.</p> <p>Warning</p> <p>We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library.</p>"},{"location":"tutorials/qualifying-entities/#the-solution","title":"The solution","text":"<p>We can use EDS-NLP's qualifier pipes to achieve that. Let's add specific components to our pipeline to detect these three modalities.</p>"},{"location":"tutorials/qualifying-entities/#adding-qualifiers","title":"Adding qualifiers","text":"<p>Adding qualifier pipes is straightforward:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = (\n    \"Motif de prise en charge : probable pneumopathie \u00e0 COVID19, \"\n    \"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nregex = dict(\n    covid=r\"(coronavirus|covid[-\\s]?19)\",\n    respiratoire=r\"respiratoires?\",\n)\nterms = dict(respiratoire=\"asthmatique\")\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.matcher(\n        regex=regex,\n        terms=terms,\n        attr=\"LOWER\",\n    ),\n)\n\nnlp.add_pipe(eds.sentences())  # (1)\nnlp.add_pipe(eds.negation())  # Negation component\nnlp.add_pipe(eds.hypothesis())  # Speculation pipe\nnlp.add_pipe(eds.family())  # Family context detection\n</code></pre> <ol> <li>Qualifiers pipes need sentence boundaries to be set (see the specific documentation for detail).</li> </ol> <p>This code is complete, and should run as is.</p>"},{"location":"tutorials/qualifying-entities/#reading-the-results","title":"Reading the results","text":"<p>Let's output the results as a pandas DataFrame for better readability:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nimport pandas as pd\ntext = (\n    \"Motif de prise en charge : probable pneumopathie \u00e0 COVID19, \"\n    \"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nregex = dict(\n    covid=r\"(coronavirus|covid[-\\s]?19)\",\n    respiratoire=r\"respiratoires?\",\n)\nterms = dict(respiratoire=\"asthmatique\")\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.matcher(\n        regex=regex,\n        terms=terms,\n        attr=\"LOWER\",\n    ),\n)\n\nnlp.add_pipe(eds.sentences())\n\nnlp.add_pipe(eds.negation())  # Negation component\nnlp.add_pipe(eds.hypothesis())  # Speculation pipe\nnlp.add_pipe(eds.family())  # Family context detection\n\ndoc = nlp(text)\n\n# Extraction as a pandas DataFrame\nentities = []\nfor ent in doc.ents:\nd = dict(\nlexical_variant=ent.text,\nlabel=ent.label_,\nnegation=ent._.negation,\nhypothesis=ent._.hypothesis,\nfamily=ent._.family,\n)\nentities.append(d)\ndf = pd.DataFrame.from_records(entities)\n</code></pre> <p>This code is complete, and should run as is.</p> <p>We get the following result:</p> lexical_variant label negation hypothesis family COVID19 covid False True False respiratoires respiratoire True False False asthmatique respiratoire False False True"},{"location":"tutorials/qualifying-entities/#conclusion","title":"Conclusion","text":"<p>The qualifier pipes limits the number of false positives by detecting linguistic modulations such as negations or speculations. Go to the full documentation for a complete presentation of the different pipes, their configuration options and validation performance.</p>"},{"location":"tutorials/reason/","title":"Detecting Reason of Hospitalisation","text":"<p>In this tutorial we will use the pipe <code>eds.reason</code> to :</p> <ul> <li>Identify spans that corresponds to the reason of hospitalisation</li> <li>Check if there are named entities overlapping with my span of 'reason of hospitalisation'</li> <li>Check for all named entities if they are tagged <code>is_reason</code></li> </ul> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018\nMOTIF D'HOSPITALISATION\nMonsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978,\na \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nANT\u00c9C\u00c9DENTS\nAnt\u00e9c\u00e9dents m\u00e9dicaux :\nPremier \u00e9pisode d'asthme en mai 2018.\"\"\"\n\nnlp = edsnlp.blank(\"eds\")\n\n# Extraction d'entit\u00e9s nomm\u00e9es\nnlp.add_pipe(\n    eds.matcher(\n        terms=dict(\n            respiratoire=[\n                \"asthmatique\",\n                \"asthme\",\n                \"toux\",\n            ]\n        ),\n    ),\n)\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sections())\nnlp.add_pipe(eds.reason(use_sections=True))\n\ndoc = nlp(text)\n</code></pre> <p>The pipe <code>reason</code> will add a key of spans called <code>reasons</code>. We check the first item in this list.</p> <pre><code># \u2191 Omitted code above \u2191\n\nreason = doc.spans[\"reasons\"][0]\nreason\n# Out: hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n</code></pre> <p>Naturally, all spans included the <code>reasons</code> key have the attribute <code>reason._.is_reason == True</code>.</p> <pre><code># \u2191 Omitted code above \u2191\n\nreason._.is_reason\n# Out: True\n</code></pre> <pre><code># \u2191 Omitted code above \u2191\n\nfor e in reason._.ents_reason:  # (1)\n    print(\n        \"Entity:\",\n        e.text,\n        \"-- Label:\",\n        e.label_,\n        \"-- is_reason:\",\n        e._.is_reason,\n    )\n# Out: Entity: asthme -- Label: respiratoire -- is_reason: True\n</code></pre> <ol> <li>We check if the span include named entities, their labels and the attribute is_reason</li> </ol> <p>We can verify that named entities that do not overlap with the spans of reason, have their attribute <code>reason._.is_reason == False</code>:</p> <pre><code>for e in doc.ents:\n    print(e.start, e, e._.is_reason)\n# Out: 42 asthme True\n# Out: 54 asthme False\n</code></pre> <ol></ol>"},{"location":"tutorials/spacy101/","title":"SpaCy representations","text":"<p>EDS-NLP uses spaCy to represent documents and their annotations. You will need to familiarise yourself with some key spaCy concepts.</p> <p>Skip if you're familiar with spaCy objects</p> <p>This page is intended as a crash course for the very basic spaCy concepts that are needed to use EDS-NLP. If you've already used spaCy, you should probably skip to the next page.</p>"},{"location":"tutorials/spacy101/#the-doc-object","title":"The <code>Doc</code> object","text":"<p>The <code>doc</code> object carries the result of the entire processing. It's the most important abstraction in spaCy, hence its use in EDS-NLP, and holds a token-based representation of the text along with the results of every pipeline components. It also keeps track of the input text in a non-destructive manner, meaning that <code>doc.text == text</code> is always true.</p> <p>To obtain a doc, run the following code: <pre><code>import edsnlp  # (1)\n\n# Initialize a pipeline\nnlp = edsnlp.blank(\"eds\")  # (2)\n\ntext = \"Michel est un penseur lat\u00e9ral.\"  # (3)\n\n# Apply the pipeline\ndoc = nlp(text)  # (4)\n\ndoc.text\n# Out: 'Michel est un penseur lat\u00e9ral.'\n\n# If you do not want to run the pipeline but only tokenize the text\ndoc = nlp.make_doc(text)\n\n# Text processing in spaCy is non-destructive\ndoc.text == text\n\n# You can access a specific token\ntoken = doc[2]  # (5)\n\n# And create a Span using slices\nspan = doc[:3]  # (6)\n\n# Entities are tracked in the ents attribute\ndoc.ents  # (7)\n# Out: ()\n</code></pre></p> <ol> <li>Import edsnlp...</li> <li>Load a pipeline. The parameter corresponds to the language code and affects the tokenization.</li> <li>Define a text you want to process.</li> <li>Apply the pipeline and get a spaCy <code>Doc</code> object.</li> <li><code>token</code> is a <code>Token</code> object referencing the third token</li> <li><code>span</code> is a <code>Span</code> object referencing the first three tokens.</li> <li>We have not declared any entity recognizer in our pipeline, hence this attribute is empty.</li> </ol> <p>We just created a pipeline and applied it to a sample text. It's that simple.</p>"},{"location":"tutorials/spacy101/#the-span-objects","title":"The <code>Span</code> objects","text":"<p>Span of text are represented by the <code>Span</code> object and represent slices of the <code>Doc</code> object. You can either create a span by slicing a <code>Doc</code> object, or by running a pipeline component that creates spans. There are different types of spans:</p> <ul> <li><code>doc.ents</code> are non-overlapping spans that represent entities</li> <li><code>doc.sents</code> are the sentences of the document</li> <li><code>doc.spans</code> is dict of groups of spans (that can overlap)</li> </ul> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nnlp.add_pipe(eds.sentences())  # (1)\nnlp.add_pipe(eds.dates())  # (2)\n\ntext = \"Le 5 mai 2005, Jimoth\u00e9 a \u00e9t\u00e9 invit\u00e9 \u00e0 une f\u00eate organis\u00e9e par Michel.\"\n\ndoc = nlp(text)\n</code></pre> <ol> <li>Like the name suggests, this pipeline component is declared by EDS-NLP.    <code>eds.sentences</code> is a rule-based sentence boundary prediction.    See its documentation for detail.</li> <li>Like the name suggests, this pipeline component is declared by EDS-NLP.    <code>eds.dates</code> is a date extraction and normalisation component.    See its documentation for detail.</li> </ol> <p>The <code>doc</code> object just became more interesting!</p> <pre><code># \u2191 Omitted code above \u2191\n\n# We can split the document into sentences spans\nlist(doc.sents)  # (1)\n# Out: [Le 5 mai 2005, Jimoth\u00e9 a \u00e9t\u00e9 invit\u00e9 \u00e0 une f\u00eate organis\u00e9e par Michel.]\n\n# And list dates spans\ndoc.spans[\"dates\"]  # (2)\n# Out: [5 mai 2005]\n\nspan = doc.spans[\"dates\"][0]  # (3)\n</code></pre> <ol> <li>In this example, there is only one sentence...</li> <li>The <code>eds.dates</code> adds a key to the <code>doc.spans</code> attribute</li> <li><code>span</code> is a spaCy <code>Span</code> object.</li> </ol>"},{"location":"tutorials/spacy101/#spacy-extensions","title":"SpaCy extensions","text":"<p>We can add custom attributes (or \"extensions\") to spaCy objects via the <code>_</code> attribute. For example, the <code>eds.dates</code> pipeline adds a <code>Span._.date</code> extension to the <code>Span</code> object. The attributes can be any Python object.</p> <pre><code># \u2191 Omitted code above \u2191\n\nspan._.date.to_datetime()  # (1)\n# Out: DateTime(2005, 5, 5, 0, 0, 0, tzinfo=Timezone('Europe/Paris'))\n</code></pre> <ol> <li>We use the <code>to_datetime()</code> method of the extension to get an object that is usable by Python.</li> </ol> <ol></ol>"},{"location":"tutorials/training-ner/","title":"Training a NER model","text":"<p>In this tutorial, we'll see how we can quickly train a deep learning model with EDS-NLP using the <code>edsnlp.train</code> function.</p> <p>Hardware requirements</p> <p>Training modern deep-learning models is compute-intensive. A GPU with \u2265 16 GB VRAM is recommended. Training on CPU is possible but much slower. On macOS, PyTorch\u2019s MPS backend may not support all operations and you'll likely hit <code>NotImplementedError</code> messages : in this case, fall back to CPU using the <code>cpu=True</code> option.</p> <p>This tutorial uses EDS-NLP\u2019s command-line interface, <code>python -m edsnlp.train</code>. If you need fine-grained control over the loop, consider writing your own training script.</p>"},{"location":"tutorials/training-ner/#creating-a-project","title":"Creating a project","text":"<p>If you already have installed <code>edsnlp[ml]</code> and do not want to setup a project, you can skip to the next section.</p> <p>Create a new project:</p> <pre><code>mkdir my_ner_project\ncd my_ner_project\n\ntouch README.md pyproject.toml\nmkdir -p configs data/dataset\n</code></pre> <p>Add a standard <code>pyproject.toml</code> file with the following content. This file will be used to manage the dependencies of the project and its versioning.</p> pyproject.toml<pre><code>[project]\nname = \"my_ner_project\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\n{ name=\"Firstname Lastname\", email=\"firstname.lastname@domain.com\" }\n]\nreadme = \"README.md\"\nrequires-python = \"&gt;3.7.1,&lt;4.0\"\n\ndependencies = [\n\"edsnlp[ml]&gt;=0.16.0\",\n\"sentencepiece&gt;=0.1.96\"\n]\n\n[project.optional-dependencies]\ndev = [\n\"dvc&gt;=2.37.0; python_version &gt;= '3.8'\",\n\"pandas&gt;=1.1.0,&lt;2.0.0; python_version &lt; '3.8'\",\n\"pandas&gt;=1.4.0,&lt;2.0.0; python_version &gt;= '3.8'\",\n\"pre-commit&gt;=2.18.1\",\n\"accelerate&gt;=0.21.0; python_version &gt;= '3.8'\",\n\"rich-logger&gt;=0.3.0\"\n]\n</code></pre> <p>We recommend using a virtual environment (\"venv\") to isolate the dependencies of your project and using uv to install the dependencies:</p> <pre><code>pip install uv\n# skip the next two lines if you do not want a venv\nuv venv .venv\nsource .venv/bin/activate\nuv pip install -e \".[dev]\" -p $(uv python find)\n</code></pre>"},{"location":"tutorials/training-ner/#training-the-model","title":"Training the model","text":"<p>EDS-NLP supports training models either from the command line or from a Python script or notebook, and switching between the two is straightforward thanks to the use of Confit.</p> <p>Visit the <code>edsnlp.train</code> documentation for a list of all the available options.</p> From the command lineFrom a script or a notebook <p>Create a <code>config.yml</code> file in the <code>configs</code> folder with the following content:</p> configs/config.yml<pre><code># Some variables are grouped here for conviency but we could also\n# put their values directly in the config in place of their reference\nvars:\ntrain: './data/dataset/train'\ndev: './data/dataset/test'\n\n# \ud83e\udd16 PIPELINE DEFINITION\nnlp:\n'@core': pipeline  #(1)!\nlang: eds  # Word-level tokenization: use the \"eds\" tokenizer\n\n# Our pipeline will contain a single NER pipe\n# The NER pipe will be a CRF model\ncomponents:\nner:\n'@factory': eds.ner_crf\nmode: 'joint'\ntarget_span_getter: 'gold_spans'\n# Set spans as both to ents and in separate `ent.label` groups\nspan_setter: [ \"ents\", \"*\" ]\ninfer_span_setter: true\n\n# The CRF model will use a CNN to re-contextualize embeddings\nembedding:\n'@factory': eds.text_cnn\nkernel_sizes: [ 3 ]\n\n# The base embeddings will be computed by a transformer\nembedding:\n'@factory': eds.transformer\nmodel: 'camembert-base'\nwindow: 128\nstride: 96\n\n# \ud83d\udcc8 SCORERS\nscorer:\nner:\n'@metrics': eds.ner_exact\nspan_getter: ${ nlp.components.ner.target_span_getter }\n\n# \ud83c\udf9b\ufe0f OPTIMIZER\noptimizer:\n\"@core\": optimizer\noptim: adamw\ngroups:\n# Assign parameters starting with transformer (ie the parameters of the transformer component)\n# to a first group\n- selector: \"ner[.]embedding[.]embedding\"\nlr:\n'@schedules': linear\n\"warmup_rate\": 0.1\n\"start_value\": 0\n\"max_value\": 5e-5\n# And every other parameters to the second group\n- selector: \".*\"\nlr:\n'@schedules': linear\n\"warmup_rate\": 0.1\n\"start_value\": 3e-4\n\"max_value\": 3e-4\nmodule: ${ nlp }\ntotal_steps: ${ train.max_steps }\n\n# \ud83d\udcda DATA\ntrain_data:\n- data:\n# In what kind of files (ie. their extensions) is our\n# training data stored\n'@readers': standoff\npath: ${ vars.train }\nconverter:\n# What schema is used in the data files\n- '@factory': eds.standoff_dict2doc\nspan_setter: 'gold_spans'\n# How to preprocess each doc for training\n- '@factory': eds.split\nnlp: null\nmax_length: 2000\nregex: '\\n\\n+'\nshuffle: dataset\nbatch_size: 4096 tokens  # 32 * 128 tokens\npipe_names: [ \"ner\" ]\n\nval_data:\n'@readers': standoff\npath: ${ vars.dev }\n# What schema is used in the data files\nconverter:\n- '@factory': eds.standoff_dict2doc\nspan_setter: 'gold_spans'\n\nloggers:\n- '@loggers': csv\n- '@loggers': rich\nfields:\nstep: {}\n(.*)loss:\ngoal: lower_is_better\nformat: \"{:.2e}\"\ngoal_wait: 2\nlr:\nformat: \"{:.2e}\"\nspeed/(.*):\nformat: \"{:.2f}\"\nname: \\1\n\"(.*?)/micro/(f|r|p)$\":\ngoal: higher_is_better\nformat: \"{:.2%}\"\ngoal_wait: 1\nname: \\1_\\2\ngrad_norm/__all__:\nformat: \"{:.2e}\"\nname: grad_norm\n# - wandb  # enable if you can and want to track with wandb\n\n# \ud83d\ude80 TRAIN SCRIPT OPTIONS\n# -&gt; python -m edsnlp.train --config configs/config.yml\ntrain:\nnlp: ${ nlp }\noutput_dir: 'artifacts'\ntrain_data: ${ train_data }\nval_data: ${ val_data }\nmax_steps: 2000\nvalidation_interval: ${ train.max_steps//10 }\ngrad_max_norm: 1.0\nscorer: ${ scorer }\noptimizer: ${ optimizer }\nlogger: ${ loggers }\n# Do preprocessing in parallel on 1 worker\nnum_workers: 1\n# Enable on Mac OS X or if you don't want to use available GPUs\n# cpu: true\n\n# \ud83d\udce6 PACKAGE SCRIPT OPTIONS\n# -&gt; python -m edsnlp.package --config configs/config.yml\npackage:\npipeline: ${ train.output_dir }\nname: 'my_ner_model'\n</code></pre> <ol> <li> <p>Why do we use <code>'@core': pipeline</code> here ? Because we need the reference used in <code>optimizer.module = ${ nlp }</code> to be the actual Pipeline and not its keyword arguments : when confit sees <code>'@core': pipeline</code>, it will instantiate the <code>Pipeline</code> class with the arguments provided in the dict.</p> <p>In fact, you could also use <code>'@core': eds.pipeline</code> in every config when you define a pipeline, but sometimes it's more convenient to let Confit infer that the type of the nlp argument based on the function when it's type hinted. Not specifying <code>'@core': pipeline</code> is also more aligned with <code>spacy</code>'s pipeline config API. However, in general, explicit is better than implicit, so feel free to use explicitly write <code>'@core': eds.pipeline</code> when you define a pipeline.</p> </li> </ol> <p>To train the model, you can use the following command:</p> <pre><code>python -m edsnlp.train --config configs/config.yml --seed 42\n</code></pre> <p>Any option can also be set either via the CLI or in <code>config.yml</code> under <code>[train]</code>.</p> <p>Create a notebook, with the following content:</p> <pre><code>import edsnlp\nfrom edsnlp.training import train, ScheduledOptimizer, TrainingData\nfrom edsnlp.metrics.ner import NerExactMetric\nfrom edsnlp.training.loggers import CSVLogger, RichLogger, WandbLogger\nimport edsnlp.pipes as eds\nimport torch\n\n# \ud83e\udd16 PIPELINE DEFINITION\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    # The NER pipe will be a CRF model\n    eds.ner_crf(\n        mode=\"joint\",\n        target_span_getter=\"gold_spans\",\n        # Set spans as both to ents and in separate `ent.label` groups\n        span_setter=[\"ents\", \"*\"],\n        infer_span_setter=True,\n        # The CRF model will use a CNN to re-contextualize embeddings\n        embedding=eds.text_cnn(\n            kernel_sizes=[3],\n            # The base embeddings will be computed by a transformer\n            embedding=eds.transformer(\n                model=\"camembert-base\",\n                window=128,\n                stride=96,\n            ),\n        ),\n    )\n)\n\n# \ud83d\udcc8 SCORERS\nner_metric = NerExactMetric(span_getter=\"gold_spans\")\n\n# \ud83d\udcda DATA\ntrain_data = (\n    edsnlp.data\n    .read_standoff(\"./data/dataset/train\", span_setter=\"gold_spans\")\n    .map(eds.split(nlp=None, max_length=2000, regex=\"\\n\\n+\"))\n)\nval_data = (\n    edsnlp.data\n    .read_standoff(\"./data/dataset/test\", span_setter=\"gold_spans\")\n)\n\n# \ud83c\udf9b\ufe0f OPTIMIZER\nmax_steps = 2000\noptimizer = ScheduledOptimizer(\n    optim=torch.optim.Adam,\n    module=nlp,\n    total_steps=max_steps,\n    groups={\n        \"^transformer\": {\n            \"lr\": {\"@schedules\": \"linear\", \"warmup_rate\": 0.1, \"start_value\": 0 \"max_value\": 5e-5,},\n        },\n        \"\": {\n            \"lr\": {\"@schedules\": \"linear\", \"warmup_rate\": 0.1, \"start_value\": 3e-4 \"max_value\": 3e-4,},\n        },\n    },\n)\n\n#\nloggers = [\n    CSVLogger(),\n    RichLogger(\n        fields={\n            \"step\": {},\n            \"(.*)loss\": {\"goal\": \"lower_is_better\", \"format\": \"{:.2e}\", \"goal_wait\": 2},\n            \"lr\": {\"format\": \"{:.2e}\"},\n            \"speed/(.*)\": {\"format\": \"{:.2f}\", \"name\": \"\\\\1\"},\n            \"(.*?)/micro/(f|r|p)$\": {\"goal\": \"higher_is_better\", \"format\": \"{:.2%}\", \"goal_wait\": 1, \"name\": \"\\\\1_\\\\2\"},\n            \"grad_norm/__all__\": {\"format\": \"{:.2e}\", \"name\": \"grad_norm\"},\n        }\n    ),\n    # WandBLogger(),  #  if you can and want to track with Weights &amp; Biases\n]\n\n# \ud83d\ude80 TRAIN\ntrain(\n    nlp=nlp,\n    max_steps=max_steps,\n    validation_interval=max_steps // 10,\n    train_data=TrainingData(\n        data=train_data,\n        batch_size=\"4096 tokens\",  # 32 * 128 tokens\n        pipe_names=[\"ner\"],\n        shuffle=\"dataset\",\n    ),\n    val_data=val_data,\n    scorer={\"ner\": ner_metric},\n    optimizer=optimizer,\n    grad_max_norm=1.0,\n    output_dir=\"artifacts\",\n    logger=loggers,\n    # Do preprocessing in parallel on 1 worker\n    num_workers=1,\n    # Enable on Mac OS X or if you don't want to use available GPUs\n    # cpu=True,\n)\n</code></pre> <p>or use the config file:</p> <pre><code>from edsnlp.train import train\nimport edsnlp\nimport confit\n\ncfg = confit.Config.from_disk(\n    \"configs/config.yml\", resolve=True, registry=edsnlp.registry\n)\nnlp = train(**cfg[\"train\"])\n</code></pre>"},{"location":"tutorials/training-ner/#use-the-model","title":"Use the model","text":"<p>You can now load the model and use it to process some text:</p> <pre><code>import edsnlp\n\nnlp = edsnlp.load(\"artifacts/model-last\")\ndoc = nlp(\"Some sample text\")\nfor ent in doc.ents:\n    print(ent, ent.label_)\n</code></pre>"},{"location":"tutorials/training-ner/#packaging-the-model","title":"Packaging the model","text":"<p>To package the model and share it with friends or family (if the model does not contain sensitive data), you can use the following command:</p> <pre><code>python -m edsnlp.package --pipeline artifacts/model-last/ --name my_ner_model --distributions sdist\n</code></pre> <p>Parametrize either via the CLI or in <code>config.yml</code> under <code>[package]</code>.</p> <p>Tthe model saved at the train script output path (<code>artifacts/model-last</code>) will be named <code>my_ner_model</code> and will be saved in the <code>dist</code> folder. You can upload it to a package registry or install it directly with</p> <pre><code>pip install dist/my_ner_model-0.1.0.tar.gz\n</code></pre>"},{"location":"tutorials/training-span-classifier/","title":"Training a span classifier","text":"<p>In this tutorial, we\u2019ll train a hybrid biopsy date extraction model with EDS-NLP using the <code>edsnlp.train</code> API. Our goal will be to distinguish biopsy dates from other dates. We\u2019ll use a small, annotated dataset of dates to train the model, and then apply the model to the date candidates extracted by the rule-based <code>eds.dates</code> component.</p> <p>Hardware requirements</p> <p>Training modern deep-learning models is compute-intensive. A GPU with \u2265 16 GB VRAM is recommended. Training on CPU is possible but much slower. On macOS, PyTorch\u2019s MPS backend may not support all operations and you'll likely hit <code>NotImplementedError</code> messages : in this case, fall back to CPU using the <code>cpu=True</code> option.</p> <p>This tutorial uses EDS-NLP\u2019s command-line interface, <code>python -m edsnlp.train</code>. If you need fine-grained control over the loop, consider writing your own training script.</p>"},{"location":"tutorials/training-span-classifier/#creating-a-project","title":"Creating a project","text":"<p>If you already have <code>edsnlp[ml]</code> installed, skip to the next section</p> <p>Create a new project:</p> <pre><code>mkdir my_span_classification_project\ncd my_span_classification_project\n\ntouch README.md pyproject.toml\nmkdir -p configs\n</code></pre> <p>Add a <code>pyproject.toml</code>:</p> pyproject.toml<pre><code>[project]\nname = \"my_span_classification_project\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\n{ name = \"Firstname Lastname\", email = \"firstname.lastname@domain.com\" }\n]\nreadme = \"README.md\"\nrequires-python = \"&gt;3.7.1,&lt;4.0\"\n\ndependencies = [\n\"edsnlp[ml]&gt;=0.16.0\",\n\"sentencepiece&gt;=0.1.96\"\n]\n\n[project.optional-dependencies]\ndev = [\n\"dvc&gt;=2.37.0; python_version &gt;= '3.8'\",\n\"pandas&gt;=1.4.0,&lt;2.0.0; python_version &gt;= '3.8'\",\n\"pre-commit&gt;=2.18.1\",\n\"accelerate&gt;=0.21.0; python_version &gt;= '3.8'\",\n\"rich-logger&gt;=0.3.0\"\n]\n</code></pre> <p>We recommend using a virtual environment and uv:</p> <pre><code>pip install uv\nuv venv .venv\nsource .venv/bin/activate\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"tutorials/training-span-classifier/#creating-the-dataset","title":"Creating the dataset","text":"<p>We'll use a small dataset of annotated biopsy dates. The dataset is in the standoff format. You can use Brat to visualize and edit the annotations. The dataset is available under <code>tests/training/dataset_2</code> directory of EDS-NLP's repository.</p> <p>To use it, download and copy it into a local <code>dataset</code> directory:</p> <ul> <li>You can clone the repository and copy it yourself, or</li> <li>Use this direct downloader link and unzip the downloaded archive.</li> </ul>"},{"location":"tutorials/training-span-classifier/#training-the-model","title":"Training the model","text":"<p>You can train the model either from the command line or from a script or a notebook. Visit the <code>edsnlp.train</code> documentation for a list of all the available options.</p> From the command lineFrom a script or a notebook <p>Create a config file:</p> configs/config.yml<pre><code>vars:\ntrain: './dataset/train'\ndev: './dataset/dev'\n\n# \ud83e\udd16 PIPELINE DEFINITION\nnlp:\n'@core': pipeline\nlang: eds\n\ncomponents:\nnormalizer:\n'@factory': eds.normalizer\n\n# When we encounter a new document, first we extract the dates from the text\ndates:\n'@factory': eds.dates\nspan_setter: 'ents'  # (1)!\n\n# Then for each date, we classify it as a biopsy date or not\nbiopsy_classifier:\n'@factory': eds.span_classifier\nattributes: [ \"is_biopsy_date\" ]\nspan_getter: [ \"ents\", \"gold_spans\" ]\n# ...using a context of 20 words before and after the date\ncontext_getter: words[-20:20]\n# ...embedded by pooling the embeddings\nembedding:\n'@factory': eds.span_pooler\n# ...of a transformer model\nembedding:\n'@factory': eds.transformer\nmodel: 'almanach/camembert-bio-base'\nwindow: 128\nstride: 96\n\n# \ud83d\udcc8 SCORER\nscorer:\nbiopsy_date:\n'@metrics': eds.span_attribute\nspan_getter: ${nlp.components.biopsy_classifier.span_getter}\nqualifiers: ${nlp.components.biopsy_classifier.attributes}\n\n# \ud83c\udf9b\ufe0f OPTIMIZER\noptimizer:\n\"@core\": optimizer !draft  # (2)!\noptim: torch.optim.AdamW\ngroups:\n# Small learning rate for the pretrained transformer model\n- selector: 'biopsy_classifier[.]embedding[.]embedding'\nlr:\n'@schedules': linear\nwarmup_rate: 0.1\nstart_value: 0.\nmax_value: 5e-5\n# Larger learning rate for the rest of the model\n- selector: '.*'\nlr:\n'@schedules': linear\nwarmup_rate: 0.1\nstart_value: 3e-4\nmax_value: 3e-4\n\n# \ud83d\udcda DATA\ntrain_data:\n- data:\n# Load the training data from standoff (BRAT) files\n'@readers': standoff\npath: ${vars.train}\nconverter:\n# Convert a standoff file to a Doc\n- '@factory': eds.standoff_dict2doc\nspan_setter: 'gold_spans'\nspan_attributes: [ 'is_biopsy_date' ]\nbool_attributes: [ 'is_biopsy_date' ]\n# Split each doc into replicas, each with exactly one\n# span from the `gold_spans` group to improve mixing\n- '@factory': eds.explode\nspan_getter: 'gold_spans'\nshuffle: dataset\nbatch_size: 8 spans\npipe_names: [ \"biopsy_classifier\" ]\n\nval_data:\n'@readers': standoff\npath: ${vars.dev}\nconverter:\n- '@factory': eds.standoff_dict2doc\nspan_setter: 'gold_spans'\nspan_attributes: [ 'is_biopsy_date' ]\nbool_attributes: [ 'is_biopsy_date' ]\n\n# \ud83d\ude80 TRAIN SCRIPT OPTIONS\ntrain:\nnlp: ${nlp}\ntrain_data: ${train_data}\nval_data: ${val_data}\nmax_steps: 250\nvalidation_interval: 50\nmax_grad_norm: 1.0\nscorer: ${scorer}\nnum_workers: 1\noutput_dir: 'artifacts'\n</code></pre> <ol> <li>Put entities extracted by <code>eds.dates</code> in <code>doc.ents</code>, instead of <code>doc.spans['dates']</code>.</li> <li>Wait, what's does \"draft\" mean here ? The rationale is this: we don't want to instantiate the optimizer now, because the nlp object hasn't been <code>post_init</code>'ed yet : <code>post_init</code> is the operation that looks at some data, finds how many labels the model must learn, and updates the model weights to have as many heads as there are labels. This function will be called by <code>train</code>, so the optimizer should be defined after, when the model parameter tensors are final. To do that, instead of instantiating the optimizer, we create a \"Draft\", which will be instantiated inside the <code>train</code> function, once all the required parameters are set.</li> </ol> <p>And train the model:</p> <pre><code>python -m edsnlp.train --config configs/config.yml --seed 42\n</code></pre> <pre><code>import edsnlp\nfrom edsnlp.training import train, ScheduledOptimizer, TrainingData\nfrom edsnlp.metrics.span_attribute import SpanAttributeMetric\nimport edsnlp.pipes as eds\nimport torch\n\n# \ud83e\udd16 PIPELINE DEFINITION\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\n# When we encounter a new document, first we extract the dates from the text\nnlp.add_pipe(eds.dates(span_setter=\"ents\"))  # (1)!\n# Then for each data, we classify the dates as biopsy dates or not\nnlp.add_pipe(\n    eds.span_classifier(\n        attributes=[\"is_biopsy_date\"],\n        span_getter=[\n            \"ents\",  # used at inference time\n            \"gold_spans\",  # used at training time\n        ],\n        # ...using a context of 20 words before and after the date\n        context_getter=\"words[-20:20]\",\n        # ...embedded by pooling the embeddings\n        embedding=eds.span_pooler(\n            # ...of a transformer model\n            embedding=eds.transformer(\n                model=\"almanach/camembert-bio-base\",\n                window=128,\n                stride=96,\n            ),\n        ),\n    ),\n    name=\"biopsy_classifier\",\n)\n\n# \ud83d\udcc8 SCORER\nmetric = SpanAttributeMetric(\n    span_getter=nlp.pipes.biopsy_classifier.span_getter,\n    qualifiers=nlp.pipes.biopsy_classifier.attributes,\n)\n\n# \ud83d\udcda DATA\ntrain_docs = (\n    edsnlp.data\n    # Load and convert standoff files to Doc objects\n    .read_standoff(\n        \"./dataset/train\",\n        span_setter=\"gold_spans\",\n        span_attributes=[\"is_biopsy_date\"],\n        bool_attributes=[\"is_biopsy_date\"],\n    )\n    # Split each doc into replicas, each with exactly one\n    # span from the `gold_spans` group to improve mixing\n    .map(eds.explode(span_getter=\"gold_spans\"))\n)\nval_docs = edsnlp.data.read_standoff(\n    \"./dataset/dev\",\n    span_setter=\"gold_spans\",\n    span_attributes=[\"is_biopsy_date\"],\n    bool_attributes=[\"is_biopsy_date\"],\n)\n\n# \ud83c\udf9b\ufe0f OPTIMIZER (here it will be the same as thedefault one)\noptimizer = ScheduledOptimizer.draft(  # (2)!\n    optim=torch.optim.AdamW,\n    groups={\n        \"biopsy_classifier[.]embedding\": {\n            \"lr\": {\n                \"@schedules\": \"linear\",\n                \"warmup_rate\": 0.1,\n                \"start_value\": 0.,\n                \"max_value\": 5e-5,\n            },\n        },\n        \".*\": {\n            \"lr\": {\n                \"@schedules\": \"linear\",\n                \"warmup_rate\": 0.1,\n                \"start_value\": 3e-4,\n                \"max_value\": 3e-4,\n            },\n        },\n    }\n)\n\n# \ud83d\ude80 TRAIN\ntrain(\n    nlp=nlp,\n    train_data=TrainingData(\n        data=train_docs,\n        batch_size=\"8 spans\",\n        pipe_names=[\"biopsy_classifier\"],\n        shuffle=\"dataset\",\n    ),\n    val_data=val_docs,\n    scorer={\"biopsy_date\": metric},\n    optimizer=optimizer,\n    max_steps=250,\n    validation_interval=50,\n    grad_max_norm=1.0,\n    num_workers=0,\n    output_dir=\"artifacts\",\n    # cpu=True,  # (optional) use CPU instead of GPU/MPS\n)\n</code></pre> <ol> <li>Put entities extracted by <code>eds.dates</code> in <code>doc.ents</code>, instead of <code>doc.spans['dates']</code>.</li> <li>Wait, what's does \"draft\" mean here ? The rationale is this: we don't want to instantiate the optimizer now, because the nlp object hasn't been <code>post_init</code>'ed yet : <code>post_init</code> is the operation that looks at some data, finds how many label the model must learn, and updates the model weights to have as many heads as there are labels. This function will be called by <code>train</code>, so the optimizer should be defined after, when the model parameter tensors are final. To do that, instead of instantiating the optimizer, we create a \"Draft\", which will be instantiated inside the <code>train</code> function, once all the required parameters are set.</li> </ol> <p>Upstream annotations at training vs inference time</p> <p>In this example, the pipeline contains the <code>eds.dates</code> component but this component is not applied to documents during the training.</p> <p>Actually, this is not specific to this example: in EDS-NLP, the documents used in a training are never modified by the pipeline components, and are instead kept intact, as yielded by the <code>training_data</code> parameter object of train(...).</p> <p>This is intended: only the spans in <code>gold_spans</code> were actually annotated with the <code>is_biopsy_date</code> attribute. If instead <code>eds.dates</code> had modified the documents, the predicted dates would not necessarily had contained the annotated attribute.</p> <p>In general, there is no need to apply upstream pipe to the documents when training a given trainable pipe.</p>"},{"location":"tutorials/training-span-classifier/#use-the-model","title":"Use the model","text":"<p>You can now load the trained pipeline and extract the biopsy dates it predicts:</p> <pre><code>import edsnlp\n\nnlp = edsnlp.load(\"artifacts/model-last\")\n\ndocs = edsnlp.data.from_iterable([\n    \"Le 15/07/2023, un pr\u00e9l\u00e8vement a \u00e9t\u00e9 r\u00e9alis\u00e9.\"\n])\ndocs = docs.map_pipeline(nlp)\ndocs.to_pandas(\n    converter=\"ents\",\n    # by default, will list doc.ents\n    span_attributes=[\"is_biopsy_date\"],\n)\n</code></pre>"},{"location":"tutorials/tuning/","title":"Hyperparameter Tuning","text":"<p>In this tutorial, we'll see how we can quickly tune hyperparameters of a deep learning model with EDS-NLP using the <code>edsnlp.tune</code> function.</p> <p>Tuning refers to the process of optimizing the hyperparameters of a machine learning model to achieve the best performance. These hyperparameters include factors like learning rate, batch size, dropout rates, and model architecture parameters. Tuning is crucial because the right combination of hyperparameters can significantly improve model accuracy and efficiency, while poor choices can lead to overfitting, underfitting, or unnecessary computational costs. By systematically searching for the best hyperparameters, we ensure the model is both effective and efficient before the final training phase.</p> <p>We strongly suggest you read the previous \"Training API tutorial\" to understand how to train a deep learning model using a config file with EDS-NLP.</p>"},{"location":"tutorials/tuning/#1-creating-a-project","title":"1. Creating a project","text":"<p>If you already have installed <code>edsnlp[ml]</code> and do not want to setup a project, you can skip to the next section.</p> <p>Create a new project:</p> <pre><code>mkdir my_ner_project\ncd my_ner_project\n\ntouch README.md pyproject.toml\nmkdir -p configs data/dataset\n</code></pre> <p>Add a standard <code>pyproject.toml</code> file with the following content. This file will be used to manage the dependencies of the project and its versioning.</p> pyproject.toml<pre><code>[project]\nname = \"my_ner_project\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\n{ name=\"Firstname Lastname\", email=\"firstname.lastname@domain.com\" }\n]\nreadme = \"README.md\"\nrequires-python = \"&gt;3.7.1,&lt;4.0\"\n\ndependencies = [\n\"edsnlp[ml]&gt;=0.16.0\",\n\"sentencepiece&gt;=0.1.96\",\n\"optuna&gt;=4.0.0\",\n\"plotly&gt;=5.18.0\",\n\"ruamel.yaml&gt;=0.18.0\",\n\"configobj&gt;=5.0.9\",\n]\n\n[project.optional-dependencies]\ndev = [\n\"dvc&gt;=2.37.0; python_version &gt;= '3.8'\",\n\"pandas&gt;=1.1.0,&lt;2.0.0; python_version &lt; '3.8'\",\n\"pandas&gt;=1.4.0,&lt;2.0.0; python_version &gt;= '3.8'\",\n\"pre-commit&gt;=2.18.1\",\n\"accelerate&gt;=0.21.0; python_version &gt;= '3.8'\",\n\"rich-logger&gt;=0.3.0\"\n]\n</code></pre> <p>We recommend using a virtual environment (\"venv\") to isolate the dependencies of your project and using uv to install the dependencies:</p> <pre><code>pip install uv\n# skip the next two lines if you do not want a venv\nuv venv .venv\nsource .venv/bin/activate\nuv pip install -e \".[dev]\" -p $(uv python find)\n</code></pre>"},{"location":"tutorials/tuning/#2-tuning-a-model","title":"2. Tuning a model","text":""},{"location":"tutorials/tuning/#21-tuning-section-in-configyml-file","title":"2.1. Tuning Section in <code>config.yml</code> file","text":"<p>If you followed the \"Training API tutorial\", you should already have a <code>configs/config.yml</code> file for training parameters.</p> <p>To enable hyperparameter tuning, add the following <code>tuning</code> section to your <code>config.yml</code> file:</p> configs/config.yml<pre><code>tuning:\n# Output directory for tuning results.\noutput_dir: 'results'\n# Checkpoint directory\ncheckpoint_dir: 'checkpoint'\n# Number of gpu hours allowed for tuning.\ngpu_hours: 1.0\n# Number of fixed trials to tune hyperparameters (override gpu_hours).\nn_trials: 4\n# Enable two-phase tuning. In the first phase, the script will tune all hyperparameters.\n# In the second phase, it will focus only on the top 50% most important hyperparameters.\ntwo_phase_tuning: True\n# Metric used to evaluate trials.\nmetric: \"ner.micro.f\"\n# Hyperparameters to tune.\nhyperparameters:\n</code></pre> <p>Let's detail the new parameters:</p> <ul> <li><code>output_dir</code>: Directory where tuning results, visualizations, and best parameters will be saved.</li> <li><code>checkpoint_dir</code>: Directory where the tuning checkpoint <code>study.pkl</code> will be saved each trial. This enables automatic resumption of tuning in case of a crash. To disable resumption, simply delete the <code>study.pkl</code> file.</li> <li><code>gpu_hours</code>: Estimated total GPU time available for tuning, in hours. Given this time, the script will automatically compute for how many training trials we can tune hyperparameters. By default, <code>gpu_hours</code> is set to 1.</li> <li><code>n_trials</code>: Number of training trials for tuning. If provided, it will override <code>gpu_hours</code> and tune the model for exactly <code>n_trial</code> trials.</li> <li><code>two_phase_tuning</code>: If True, performs a two-phase tuning. In the first phase, all hyperparameters are tuned, and in the second phase, the top half (based on importance) are fine-tuned while freezing others. By default, <code>two_phase_tuning</code> is False.</li> <li><code>metric</code>: Metric used to evaluate trials. It corresponds to a path in the scorer results (depending on the scorer used in the config). By default <code>metric</code> is set to \"ner.micro.f\".</li> <li><code>hyperparameters</code>: The list of hyperparameters to tune and details about their tunings. We will discuss how it work in the following section.</li> </ul>"},{"location":"tutorials/tuning/#22-add-hyperparameters-to-tune","title":"2.2. Add hyperparameters to tune","text":"<p>In the <code>config.yml</code> file, the <code>tuning.hyperparameters</code> section defines the hyperparameters to optimize. Each hyperparameter can be specified with its type, range, and additional properties. To add a hyperparameter, follow this syntax:</p> configs/config.yml<pre><code>tuning:\nhyperparameters:\n# Hyperparameter path in `config.yml`.\n\"nlp.components.ner.embedding.embedding.classifier_dropout\":\n# Alias name. If not specified, full path will be the name.\nalias: \"classifier_dropout\"\n# Type of the hyperparameter: 'int', 'float', or 'categorical'.\ntype: \"float\"\n# Lower bound for tuning.\nlow: 0.\n# Upper bound for tuning.\nhigh: 0.3\n# Step for discretization (optional).\nstep: 0.05\n</code></pre> <p>Since <code>edsnlp.tune</code> leverages the Optuna framework, we recommend reviewing the following Optuna functions to understand the properties you can specify for hyperparameter sampling:</p> <ul> <li>suggest_float \u2013 For sampling floating-point hyperparameters.</li> <li>suggest_int \u2013 For sampling integer hyperparameters.</li> <li>suggest_categorical \u2013 For sampling categorical hyperparameters.</li> </ul> <p>These resources provide detailed guidance on defining the sampling ranges, distributions, and additional properties for each type of hyperparameter.</p>"},{"location":"tutorials/tuning/#23-complete-example","title":"2.3. Complete Example","text":"<p>Now, let's look at a complete example. Assume that we want to perform a two-phase tuning, for 40 gpu hours, on the following hyperparameters:</p> <ul> <li><code>hidden_dropout_prob</code>: Dropout probability for hidden layers.</li> <li><code>attention_dropout_prob</code>: Dropout probability for attention layers.</li> <li><code>classifier_dropout</code>: Dropout probability for the classifier layer.</li> <li><code>transformer_start_value</code>: Learning rate start value for the transformer.</li> <li><code>transformer_max_value</code>: Maximum learning rate for the transformer.</li> <li><code>transformer_warmup_rate</code>: Warmup rate for the transformer learning rate scheduler.</li> <li><code>transformer_weight_decay</code>: Weight decay for the transformer optimizer.</li> <li><code>other_start_value</code>: Learning rate start value for other components.</li> <li><code>other_max_value</code>: Maximum learning rate for other components.</li> <li><code>other_warmup_rate</code>: Warmup rate for the learning rate scheduler of other components.</li> <li><code>other_weight_decay</code>: Weight decay for the optimizer of other components.</li> </ul> <p>Then the full <code>config.yml</code> will be:</p> configs/config.yml<pre><code>vars:\ntrain: './data/dataset/train'\ndev: './data/dataset/test'\n\n# \ud83e\udd16 PIPELINE DEFINITION\nnlp:\n'@core': pipeline\nlang: eds  # Word-level tokenization: use the \"eds\" tokenizer\ncomponents:\nner:\n'@factory': eds.ner_crf\nmode: 'joint'\ntarget_span_getter: 'gold_spans'\nspan_setter: [ \"ents\", \"*\" ]\ninfer_span_setter: true\nembedding:\n'@factory': eds.text_cnn\nkernel_sizes: [ 3 ]\nembedding:\n'@factory': eds.transformer\nmodel: prajjwal1/bert-tiny\nignore_mismatched_sizes: True\nwindow: 128\nstride: 96\n# Dropout parameters passed to the underlying transformer object.\nhidden_dropout_prob: 0.1\nattention_probs_dropout_prob: 0.1\nclassifier_dropout: 0.1\n\n# \ud83d\udcc8 SCORERS\nscorer:\nner:\n'@metrics': eds.ner_token\nspan_getter: ${ nlp.components.ner.target_span_getter }\n\n# \ud83c\udf9b\ufe0f OPTIMIZER\noptimizer:\n\"@core\": optimizer\noptim: adamw\ngroups:\n\"^transformer\":\nweight_decay: 1e-3\nlr:\n'@schedules': linear\n\"warmup_rate\": 0.1\n\"start_value\": 1e-5\n\"max_value\": 8e-5\n\".*\":\nweight_decay: 1e-3\nlr:\n'@schedules': linear\n\"warmup_rate\": 0.1\n\"start_value\": 1e-5\n\"max_value\": 8e-5\nmodule: ${ nlp }\ntotal_steps: ${ train.max_steps }\n\n# \ud83d\udcda DATA\ntrain_data:\n- data:\n'@readers': standoff\npath: ${ vars.train }\nconverter:\n- '@factory': eds.standoff_dict2doc\nspan_setter: 'gold_spans'\n- '@factory': eds.split\nnlp: null\nmax_length: 256\nregex: '\\n\\n+'\nshuffle: dataset\nbatch_size: 32 * 128 tokens\npipe_names: [ \"ner\" ]\n\nval_data:\n'@readers': standoff\npath: ${ vars.dev }\nconverter:\n- '@factory': eds.standoff_dict2doc\nspan_setter: 'gold_spans'\n\n# \ud83d\ude80 TRAIN SCRIPT OPTIONS\n# -&gt; python -m edsnlp.train --config configs/config.yml\ntrain:\nnlp: ${ nlp }\nlogger: True\noutput_dir: 'artifacts'\ntrain_data: ${ train_data }\nval_data: ${ val_data }\nmax_steps: 400\nvalidation_interval: ${ train.max_steps//2 }\ngrad_max_norm: 1.0\nscorer: ${ scorer }\noptimizer: ${ optimizer }\nnum_workers: 2\n\n# \ud83d\udce6 PACKAGE SCRIPT OPTIONS\n# -&gt; python -m edsnlp.package --config configs/config.yml\npackage:\npipeline: ${ train.output_dir }\nname: 'my_ner_model'\n\n# \u2699\ufe0f TUNE SCRIPT OPTIONS\n# -&gt; python -m edsnlp.tune --config configs/config.yml\ntuning:\noutput_dir: 'results'\ncheckpoint_dir: 'checkpoint'\ngpu_hours: 40.0\ntwo_phase_tuning: True\nmetric: \"ner.micro.f\"\nhyperparameters:\n\"nlp.components.ner.embedding.embedding.hidden_dropout_prob\":\nalias: \"hidden_dropout\"\ntype: \"float\"\nlow: 0.\nhigh: 0.3\nstep: 0.05\n\"nlp.components.ner.embedding.embedding.attention_probs_dropout_prob\":\nalias: \"attention_dropout\"\ntype: \"float\"\nlow: 0.\nhigh: 0.3\nstep: 0.05\n\"nlp.components.ner.embedding.embedding.classifier_dropout\":\nalias: \"classifier_dropout\"\ntype: \"float\"\nlow: 0.\nhigh: 0.3\nstep: 0.05\n\"optimizer.groups.^transformer.lr.start_value\":\nalias: \"transformer_start_value\"\ntype: \"float\"\nlow: 1e-6\nhigh: 1e-3\nlog: True\n\"optimizer.groups.^transformer.lr.max_value\":\nalias: \"transformer_max_value\"\ntype: \"float\"\nlow: 1e-6\nhigh: 1e-3\nlog: True\n\"optimizer.groups.^transformer.lr.warmup_rate\":\nalias: \"transformer_warmup_rate\"\ntype: \"float\"\nlow: 0.\nhigh: 0.3\nstep: 0.05\n\"optimizer.groups.^transformer.weight_decay\":\nalias: \"transformer_weight_decay\"\ntype: \"float\"\nlow: 1e-4\nhigh: 1e-2\nlog: True\n\"optimizer.groups.'.*'.lr.warmup_rate\":\nalias: \"other_warmup_rate\"\ntype: \"float\"\nlow: 0.\nhigh: 0.3\nstep: 0.05\n\"optimizer.groups.'.*'.lr.start_value\":\nalias: \"other_start_value\"\ntype: \"float\"\nlow: 1e-6\nhigh: 1e-3\nlog: True\n\"optimizer.groups.'.*'.lr.max_value\":\nalias: \"other_max_value\"\ntype: \"float\"\nlow: 1e-6\nhigh: 1e-3\nlog: True\n\"optimizer.groups.'.*'.weight_decay\":\nalias: \"other_weight_decay\"\ntype: \"float\"\nlow: 1e-4\nhigh: 1e-2\nlog: True\n</code></pre> <p>Finally, to lauch the tuning process, use the following command:</p> <pre><code>python -m edsnlp.tune --config configs/config.yml --seed 42\n</code></pre>"},{"location":"tutorials/tuning/#3-results","title":"3. Results","text":"<p>At the end of the tuning process, <code>edsnlp.tune</code> generates various results and saves them in the <code>output_dir</code> specified in the <code>config.yml</code> file:</p> <ul> <li>Tuning Summary: <code>result_summary.txt</code>, a summary file containing details about the best training trial, the best overall metric, the optimal hyperparameter values, and the average importance of each hyperparameter across all trials.</li> <li>Optimal Configuration: <code>config.yml</code>, containing the best hyperparameter values.</li> <li>Graphs and Visualizations: Various graphics illustrating the tuning process, such as:</li> <li>Optimization History plot: A line graph showing the performance of each trial over time, illustrating the optimization process and how the model's performance improves with each iteration.</li> <li>Empirical Distribution Function (EDF) plot: A graph showing the cumulative distribution of the results, helping you understand the distribution of performance scores and providing insights into the variability and robustness of the tuning process.</li> <li>Contour plot: A 2D plot that shows the relationship between two hyperparameters and their combined effect on the objective metric, providing a clear view of the optimal parameter regions.</li> <li>Parallel Coordinate plot: A multi-dimensional plot where each hyperparameter is represented as a vertical axis, and each trial is displayed as a line connecting the hyperparameter values, helping you analyze correlations and patterns across hyperparameters and their impact on performance.</li> <li>Timeline plot: A 2D plot that displays all trials and their statuses (\"completed,\" \"pruned,\" or \"failed\") over time, providing a clear overview of the progress and outcomes of the tuning process.</li> </ul> <p>These outputs offer a comprehensive view of the tuning results, enabling you to better understand the optimization process and easily deploy the best configuration.</p> <p>Note: If you enabled two-phase tuning, the <code>output_dir</code> will contain two subdirectories, <code>phase_1</code> and <code>phase_2</code>, each with their own result files as described earlier. This separation allows you to analyze the results from each phase individually.</p>"},{"location":"tutorials/tuning/#4-final-training","title":"4. Final Training","text":"<p>Now that the hyperparameters have been tuned, you can update your final <code>config.yml</code> with the best-performing hyperparameters and proceed to launch the final training using the \"Training API\".</p>"},{"location":"tutorials/visualization/","title":"Visualization","text":"<p>Let's see how to display the output of a pipeline on a single text.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.covid())\nnlp.add_pipe(eds.negation())\nnlp.add_pipe(eds.hypothesis())\nnlp.add_pipe(eds.family())\n\ntxt = \"Le patient a le covid.\"\n</code></pre>"},{"location":"tutorials/visualization/#visualize-entities-in-a-document","title":"Visualize entities in a document","text":"<p>To print a text and highlight the entities in it, you can use <code>spacy.displacy</code>.</p> <pre><code>from spacy import displacy\n\ndoc = nlp(txt)\noptions = {\n    # Optional colors\n\n    # \"colors\": {\n    #     \"covid\": \"orange\",\n    #     \"respiratoire\": \"steelblue\",\n    # },\n}\ndisplacy.render(doc, style=\"ent\", options=options)\n</code></pre> <p>will render like this:</p> Le patient a le      covid     covid  ."},{"location":"tutorials/visualization/#visualize-entities-as-a-table","title":"Visualize entities as a table","text":"<p>To quickly visualize the output of a pipeline on a document, including the annotated extensions/qualifiers, you can convert the output to a DataFrame and display it.</p> <pre><code>nlp.pipe([txt]).to_pandas(\n    converter=\"ents\",\n    # Add any extension you want to display\n    span_attributes=[\"negation\", \"hypothesis\", \"family\"],\n    # Shows the entities in doc.ents by default\n    # span_getter=[\"ents\"]\n)\n</code></pre> note_id start end label lexical_variant span_type negation hypothesis family None 16 21 covid covid ents False False False"},{"location":"utilities/","title":"Utilities","text":"<p>EDS-NLP provides a few utilities to deploy pipelines, process RegExps, etc.</p>"},{"location":"utilities/evaluation/","title":"Pipeline evaluation","text":""},{"location":"utilities/matchers/","title":"Matchers","text":"<p>We implemented three pattern matchers that are fit to clinical documents:</p> <ul> <li>the <code>EDSPhraseMatcher</code></li> <li>the <code>RegexMatcher</code></li> <li>the <code>SimstringMatcher</code></li> </ul> <p>However, note that for most use-cases, you should instead use the <code>eds.matcher</code> pipe that wraps these classes to annotate documents.</p>"},{"location":"utilities/matchers/#edsphrasematcher","title":"EDSPhraseMatcher","text":"<p>The EDSPhraseMatcher lets you efficiently match large terminology lists, by comparing tokenx against a given attribute. This matcher differs from the <code>spacy.PhraseMatcher</code> in that it allows to skip pollution tokens. To make it efficient, we have reimplemented the matching algorithm in Cython, like the original <code>spacy.PhraseMatcher</code>.</p> <p>You can use it as described in the code below.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom edsnlp.matchers.phrase import EDSPhraseMatcher\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\ndoc = nlp(\"On ne rel\u00e8ve pas de signe du Corona =============== virus.\")\n\nmatcher = EDSPhraseMatcher(nlp.vocab, attr=\"NORM\")\nmatcher.build_patterns(\n    nlp,\n    {\n        \"covid\": [\"corona virus\", \"coronavirus\", \"covid\"],\n        \"diabete\": [\"diabete\", \"diabetique\"],\n    },\n)\n\nlist(matcher(doc, as_spans=True))[0].text\n# Out: Corona =============== virus\n</code></pre>"},{"location":"utilities/matchers/#regexmatcher","title":"RegexMatcher","text":"<p>The <code>RegexMatcher</code> performs full-text regex matching. It is especially useful to handle spelling variations like <code>mammo-?graphies?</code>. Like the <code>EDSPhraseMatcher</code>, this class allows to skip pollution tokens. Note that this class is significantly slower than the <code>EDSPhraseMatcher</code>: if you can, try enumerating lexical variations of the target phrases and feed them to the <code>PhraseMatcher</code> instead.</p> <p>You can use it as described in the code below.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom edsnlp.matchers.regex import RegexMatcher\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\ndoc = nlp(\"On ne rel\u00e8ve pas de signe du Corona =============== virus.\")\n\nmatcher = RegexMatcher(attr=\"NORM\", ignore_excluded=True)\nmatcher.build_patterns(\n    {\n        \"covid\": [\"corona[ ]*virus\", \"covid\"],\n        \"diabete\": [\"diabete\", \"diabetique\"],\n    },\n)\n\nlist(matcher(doc, as_spans=True))[0].text\n# Out: Corona =============== virus\n</code></pre>"},{"location":"utilities/matchers/#simstringmatcher","title":"SimstringMatcher","text":"<p>The <code>SimstringMatcher</code> performs fuzzy term matching by comparing spans of text with a similarity metric. It is especially useful to handle spelling variations like <code>paracetomol</code> (instead of <code>paracetamol</code>).</p> <p>The <code>simstring</code> algorithm compares two strings by enumerating their char trigrams and measuring the overlap between the two sets. In the previous example: - <code>paracetomol</code> becomes <code>##p #pa par ara rac ace cet eto tom omo mol ol# l##</code> - <code>paracetamol</code> becomes <code>##p #pa par ara rac ace cet eta tam amo mol ol# l##</code> and the Dice (or F1) similarity between the two sets is 0.75.</p> <p>Like the <code>EDSPhraseMatcher</code>, this class allows to skip pollution tokens. Just like the <code>RegexMatcher</code>, this class is significantly slower than the <code>EDSPhraseMatcher</code>: if you can, try enumerating lexical variations of the target phrases and feed them to the <code>PhraseMatcher</code> instead.</p> <p>You can use it as described in the code below.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\nfrom edsnlp.matchers.simstring import SimstringMatcher\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\ndoc = nlp(\n    \"On ne rel\u00e8ve pas de signe du corona-virus. Historique d'un hepatocellulaire carcinome.\"\n)\n\nmatcher = SimstringMatcher(\n    nlp.vocab,\n    attr=\"NORM\",\n    ignore_excluded=True,\n    measure=\"dice\",\n    threshold=0.75,\n    windows=5,\n)\nmatcher.build_patterns(\n    nlp,\n    {\n        \"covid\": [\"coronavirus\", \"covid\"],\n        \"carcinome\": [\"carcinome hepatocellulaire\"],\n    },\n)\n\nlist(matcher(doc, as_spans=True))[0].text\n# Out: corona-virus\n\nlist(matcher(doc, as_spans=True))[1].text\n# Out: hepatocellulaire carcinome\n</code></pre>"},{"location":"utilities/regex/","title":"Work with RegExp","text":""},{"location":"utilities/connectors/brat/","title":"BRAT Connector","text":"<p>BRAT is currently the only supported in-text annotation editor at EDS. BRAT annotations are in the standoff format. Consider the following document:</p> <pre><code>Le patient est admis pour une pneumopathie au coronavirus.\nOn lui prescrit du parac\u00e9tamol.\n</code></pre> <p>It could be annotated as follows :</p> <pre><code>T1  Patient 4 11    patient\nT2  Disease 31 58   pneumopathie au coronavirus\nT3  Drug 79 90  parac\u00e9tamol\n</code></pre> <p>The point of the BRAT connector is to go from the standoff annotation format to an annotated spaCy document :</p> <pre><code>import edsnlp\nfrom edsnlp.connectors.brat import BratConnector\n\n# Instantiate the connector\nbrat = BratConnector(\"path/to/brat\")\n\n# Instantiate the spacy pipeline\nnlp = edsnlp.blank(\"eds\")\n\n# Convert all BRAT files to a list of documents\ndocs = brat.brat2docs(nlp)\ndoc = docs[0]\n\ndoc.ents\n# Out: [patient, pneumopathie au coronavirus, parac\u00e9tamol]\n\ndoc.ents[0].label_\n# Out: Patient\n</code></pre> <p>The connector can also go the other way around, enabling pre-annotations and an ersatz of active learning.</p>"},{"location":"utilities/connectors/labeltool/","title":"LabelTool Connector","text":"<p>LabelTool is an in-house module enabling rapid annotation of pre-extracted entities.</p> <p>We provide a ready-to-use function that converts a list of annotated spaCy documents into a <code>pandas</code> DataFrame that is readable to LabelTool.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nfrom edsnlp.connectors.labeltool import docs2labeltool\n\ncorpus = [\n    \"Ceci est un document m\u00e9dical.\",\n    \"Le patient n'est pas malade.\",\n]\n\n# Instantiate the spacy pipeline\nnlp = edsnlp.blank(\"fr\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.matcher(terms=dict(medical=\"m\u00e9dical\", malade=\"malade\")))\nnlp.add_pipe(eds.negation())\n\n# Convert all BRAT files to a list of documents\ndocs = nlp.pipe(corpus)\n\ndf = docs2labeltool(docs, extensions=[\"negation\"])\n</code></pre> <p>The results:</p> note_id note_text start end label lexical_variant negation 0 Ceci est un document m\u00e9dical. 21 28 medical m\u00e9dical False 1 Le patient n'est pas malade. 21 27 malade malade True"},{"location":"utilities/connectors/omop/","title":"OMOP Connector","text":"<p>We provide a connector between OMOP-formatted dataframes and spaCy documents.</p>"},{"location":"utilities/connectors/omop/#omop-style-dataframes","title":"OMOP-style dataframes","text":"<p>Consider a corpus of just one document:</p> <pre><code>Le patient est admis pour une pneumopathie au coronavirus.\nOn lui prescrit du parac\u00e9tamol.\n</code></pre> <p>And its OMOP-style representation, separated in two tables <code>note</code> and <code>note_nlp</code> (here with selected columns) :</p> <p><code>note</code>:</p> note_id note_text note_datetime 0 Le patient est admis pour une pneumopathie... 2021-10-23 <p><code>note_nlp</code>:</p> note_nlp_id note_id start_char end_char note_nlp_source_value lexical_variant 0 0 46 57 disease coronavirus 1 0 77 88 drug parac\u00e9tamol"},{"location":"utilities/connectors/omop/#using-the-connector","title":"Using the connector","text":"<p>The following snippet expects the tables <code>note</code> and <code>note_nlp</code> to be already defined (eg through PySpark's <code>toPandas()</code> method).</p> <pre><code>import spacy\nfrom edsnlp.connectors.omop import OmopConnector\n\n# Instantiate a spacy pipeline\nnlp = spacy.blank(\"eds\")\n\n# Instantiate the connector\nconnector = OmopConnector(nlp)\n\n# Convert OMOP tables (note and note_nlp) to a list of documents\ndocs = connector.omop2docs(note, note_nlp)\ndoc = docs[0]\n\ndoc.ents\n# Out: [coronavirus, parac\u00e9tamol]\n\ndoc.ents[0].label_\n# Out: 'disease'\n\ndoc.text == note.loc[0].note_text\n# Out: True\n</code></pre> <p>The object <code>docs</code> now contains a list of documents that reflects the information contained in the OMOP-formatted dataframes.</p>"},{"location":"utilities/connectors/overview/","title":"Overview of connectors","text":"<p>EDS-NLP provides a series of connectors apt to convert back and forth from different formats into spaCy representation.</p> <p>We provide the following connectors:</p> <ul> <li>BRAT</li> <li>OMOP</li> </ul>"},{"location":"utilities/tests/","title":"Tests Utilities","text":"<p>We provide a few testing utilities that simplify the process of:</p> <ul> <li>creating testing examples for NLP pipelines;</li> <li>testing documentation code blocs.</li> </ul>"},{"location":"utilities/tests/blocs/","title":"Testing Code Blocs","text":"<p>We created a utility that scans through the documentation, extracts code blocs and executes them to check that everything is indeed functional.</p> <p>There is more! Whenever the utility comes across an example (denoted by <code># Out:</code>, see example below), an <code>assert</code> statement is dynamically added to the snippet to check that the output matches.</p> <p>For instance:</p> <pre><code>a = 1\n\na\n# Out: 1\n</code></pre> <p>Is transformed into:</p> <pre><code>a = 1\n\nv = a\nassert repr(v) == \"1\"\n</code></pre> <p>We can disable code checking for a specific code bloc by adding a <code>.no-check</code> class to the code bloc:</p> <pre><code>```python { .no-check }\ntest = undeclared_function(42)\n```\n</code></pre> <p>Visit the source code of test_docs.py for more information.</p>"},{"location":"utilities/tests/examples/","title":"Creating Examples","text":"<p>Testing a NER/qualifier pipeline can be a hassle. We created a utility to simplify that process.</p> <p>Using the <code>parse_example</code> method, you can define a full example in a human-readable way:</p> <pre><code>from edsnlp.utils.examples import parse_example\n\nexample = \"Absence d'&lt;ent negated=true&gt;image osseuse d'allure \u00e9volutive&lt;/ent&gt;.\"\n\ntext, entities = parse_example(example)\n\ntext\n# Out: \"Absence d'image osseuse d'allure \u00e9volutive.\"\n\nentities\n# Out: [Entity(start_char=10, end_char=42, modifiers=[Modifier(key='negated', value=True)])]\n</code></pre> <p>Entities are defined using the <code>&lt;ent&gt;</code> tag. You can encode complexe information by adding keys into the tag (see example above). The <code>parse_example</code> method strips the text of the tags, and outputs a list of <code>Entity</code> objects that contain:</p> <ul> <li>the character indices of the entity ;</li> <li>custom user-defined \"modifiers\".</li> </ul> <p>See the dedicated reference page for more information.</p>"},{"location":"reference/edsnlp/","title":"<code>edsnlp</code>","text":"<p>EDS-NLP</p>"},{"location":"reference/edsnlp/conjugator/","title":"<code>edsnlp.conjugator</code>","text":""},{"location":"reference/edsnlp/conjugator/#edsnlp.conjugator.conjugate_verb","title":"<code>conjugate_verb</code>","text":"<p>Conjugates the verb using an instance of mlconjug3, and formats the results in a pandas <code>DataFrame</code>.</p>"},{"location":"reference/edsnlp/conjugator/#edsnlp.conjugator.conjugate_verb--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>verb</code> <p>Verb to conjugate.</p> <p> TYPE: <code>str</code> </p> <code>conjugator</code> <p>mlconjug3 instance for conjugating.</p> <p> TYPE: <code>Conjugator</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Normalized dataframe containing all conjugated forms for the verb.</p>"},{"location":"reference/edsnlp/conjugator/#edsnlp.conjugator.conjugate","title":"<code>conjugate</code>","text":"<p>Conjugate a list of verbs.</p>"},{"location":"reference/edsnlp/conjugator/#edsnlp.conjugator.conjugate--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>verbs</code> <p>List of verbs to conjugate</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>language</code> <p>Language to conjugate. Defaults to French (<code>fr</code>).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'fr'</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Dataframe containing the conjugations for the provided verbs. Columns: <code>verb</code>, <code>mode</code>, <code>tense</code>, <code>person</code>, <code>term</code></p>"},{"location":"reference/edsnlp/conjugator/#edsnlp.conjugator.get_conjugated_verbs","title":"<code>get_conjugated_verbs</code>","text":"<p>Get a list of conjugated verbs.</p>"},{"location":"reference/edsnlp/conjugator/#edsnlp.conjugator.get_conjugated_verbs--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>verbs</code> <p>List of verbs to conjugate.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>matches</code> <p>List of dictionary describing the mode/tense/persons to keep.</p> <p> TYPE: <code>Union[List[Dict[str, str]], Dict[str, str]]</code> </p> <code>language</code> <p>[description], by default \"fr\" (French)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'fr'</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>List of terms to look for.</p>"},{"location":"reference/edsnlp/conjugator/#edsnlp.conjugator.get_conjugated_verbs--examples","title":"Examples","text":"<p>get_conjugated_verbs(         \"aimer\",         dict(mode=\"Indicatif\", tense=\"Pr\u00e9sent\", person=\"1p\"),     ) ['aimons']</p>"},{"location":"reference/edsnlp/connectors/","title":"<code>edsnlp.connectors</code>","text":""},{"location":"reference/edsnlp/connectors/brat/","title":"<code>edsnlp.connectors.brat</code>","text":""},{"location":"reference/edsnlp/connectors/brat/#edsnlp.connectors.brat.BratConnector","title":"<code>BratConnector</code>","text":"<p>           Bases: <code>object</code></p> <p>Deprecated. Use <code>edsnlp.data.read_standoff</code> and <code>edsnlp.data.write_standoff</code> instead. Two-way connector with BRAT. Supports entities only.</p>"},{"location":"reference/edsnlp/connectors/brat/#edsnlp.connectors.brat.BratConnector--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>directory</code> <p>Directory containing the BRAT files.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>n_jobs</code> <p>Number of jobs for multiprocessing, by default 1</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>attributes</code> <p>Mapping from BRAT attributes to spaCy Span extensions. Extensions / attributes that are not in the mapping are not imported or exported If left to None, the mapping is filled with all BRAT attributes.</p> <p> TYPE: <code>Optional[AttributesMappingArg]</code> DEFAULT: <code>None</code> </p> <code>span_groups</code> <p>Additional span groups to look for entities in spaCy documents when exporting. Missing label (resp. span group) names are not imported (resp. exported) If left to None, the sequence is filled with all BRAT entity labels.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>['ents', '*']</code> </p>"},{"location":"reference/edsnlp/connectors/brat/#edsnlp.connectors.brat.BratConnector.docs2brat","title":"<code>docs2brat</code>","text":"<p>Writes a list of spaCy documents to file.</p>"},{"location":"reference/edsnlp/connectors/labeltool/","title":"<code>edsnlp.connectors.labeltool</code>","text":""},{"location":"reference/edsnlp/connectors/labeltool/#edsnlp.connectors.labeltool.docs2labeltool","title":"<code>docs2labeltool</code>","text":"<p>Returns a labeltool-ready dataframe from a list of annotated document.</p>"},{"location":"reference/edsnlp/connectors/labeltool/#edsnlp.connectors.labeltool.docs2labeltool--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>List of annotated spacy docs.</p> <p> TYPE: <code>List[Doc]</code> </p> <code>extensions</code> <p>List of extensions to use by labeltool.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>df</code> <p>DataFrame tailored for labeltool.</p> <p> TYPE: <code>DataFrame</code> </p>"},{"location":"reference/edsnlp/connectors/omop/","title":"<code>edsnlp.connectors.omop</code>","text":""},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector","title":"<code>OmopConnector</code>","text":"<p>           Bases: <code>object</code></p> <p>[summary]</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline instance</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>start_char</code> <p>Name of the column containing the start character index of the entity, by default \"start_char\"</p> <p> TYPE: <code>str</code> DEFAULT: <code>'start_char'</code> </p> <code>end_char</code> <p>Name of the column containing the end character index of the entity, by default \"end_char\"</p> <p> TYPE: <code>str</code> DEFAULT: <code>'end_char'</code> </p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector.preprocess","title":"<code>preprocess</code>","text":"<p>Preprocess the input OMOP tables: modification of the column names.</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector.preprocess--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>DataFrame</code> </p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector.postprocess","title":"<code>postprocess</code>","text":"<p>Postprocess the input OMOP tables: modification of the column names.</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector.postprocess--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>DataFrame</code> </p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector.omop2docs","title":"<code>omop2docs</code>","text":"<p>Transforms OMOP tables to a list of spaCy documents.</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector.omop2docs--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> <code>extensions</code> <p>Extensions to keep, by default None</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[Doc]</code> <p>List of spaCy documents.</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector.docs2omop","title":"<code>docs2omop</code>","text":"<p>Transforms a list of spaCy documents to a pair of OMOP tables.</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.OmopConnector.docs2omop--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>List of spaCy documents.</p> <p> TYPE: <code>List[Doc]</code> </p> <code>extensions</code> <p>Extensions to keep, by default None</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>DataFrame</code> </p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.omop2docs","title":"<code>omop2docs</code>","text":"<p>Transforms an OMOP-formatted pair of dataframes into a list of documents.</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.omop2docs--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>note</code> <p>The OMOP <code>note</code> table.</p> <p> TYPE: <code>DataFrame</code> </p> <code>note_nlp</code> <p>The OMOP <code>note_nlp</code> table</p> <p> TYPE: <code>DataFrame</code> </p> <code>nlp</code> <p>The pipeline instance</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>extensions</code> <p>Extensions to keep, by default None</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[Doc] :</code> <p>List of spaCy documents</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.docs2omop","title":"<code>docs2omop</code>","text":"<p>Transforms a list of spaCy docs to a pair of OMOP tables.</p>"},{"location":"reference/edsnlp/connectors/omop/#edsnlp.connectors.omop.docs2omop--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>List of documents to transform.</p> <p> TYPE: <code>List[Doc]</code> </p> <code>extensions</code> <p>Extensions to keep, by default None</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tuple[DataFrame, DataFrame]</code> <p>Pair of OMOP tables (<code>note</code> and <code>note_nlp</code>)</p>"},{"location":"reference/edsnlp/core/","title":"<code>edsnlp.core</code>","text":""},{"location":"reference/edsnlp/core/pipeline/","title":"<code>edsnlp.core.pipeline</code>","text":""},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>           Bases: <code>Validated</code></p> <p>New pipeline to use as a drop-in replacement for spaCy's pipeline. It uses PyTorch as the deep-learning backend and allows components to share subcomponents.</p> <p>See the documentation for more details.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>lang</code> <p>Language code</p> <p> TYPE: <code>str</code> </p> <code>create_tokenizer</code> <p>Function that creates a tokenizer for the pipeline</p> <p> TYPE: <code>Optional[Callable[[Self], Tokenizer]]</code> DEFAULT: <code>None</code> </p> <code>vocab</code> <p>Whether to create a new vocab or use an existing one</p> <p> TYPE: <code>Union[bool, Vocab]</code> DEFAULT: <code>True</code> </p> <code>vocab_config</code> <p>Configuration for the vocab</p> <p> TYPE: <code>Optional[Type[BaseDefaults]]</code> DEFAULT: <code>None</code> </p> <code>meta</code> <p>Meta information about the pipeline</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.disabled","title":"<code>disabled</code>  <code>property</code>","text":"<p>The names of the disabled components</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.cfg","title":"<code>cfg: Config</code>  <code>property</code>","text":"<p>Returns the config of the pipeline, including the config of all components. Updated from spacy to allow references between components.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.get_pipe","title":"<code>get_pipe</code>","text":"<p>Get a component by its name.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.get_pipe--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>name</code> <p>The name of the component to get.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Pipe</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.has_pipe","title":"<code>has_pipe</code>","text":"<p>Check if a component exists in the pipeline.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.has_pipe--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>name</code> <p>The name of the component to check.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.create_pipe","title":"<code>create_pipe</code>","text":"<p>Create a component from a factory name.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.create_pipe--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>factory</code> <p>The name of the factory to use</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> </p> <code>config</code> <p>The config to pass to the factory</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Pipe</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.add_pipe","title":"<code>add_pipe</code>","text":"<p>Add a component to the pipeline.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.add_pipe--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>factory</code> <p>The name of the component to add or the component itself</p> <p> TYPE: <code>Union[str, Pipe]</code> </p> <code>name</code> <p>The name of the component. If not provided, the name of the component will be used if it has one (.name), otherwise the factory name will be used.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>first</code> <p>Whether to add the component to the beginning of the pipeline. This argument is mutually exclusive with <code>before</code> and <code>after</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>before</code> <p>The name of the component to add the new component before. This argument is mutually exclusive with <code>after</code> and <code>first</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>after</code> <p>The name of the component to add the new component after. This argument is mutually exclusive with <code>before</code> and <code>first</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>The arguments to pass to the component factory.</p> <p>Note that instead of replacing arguments with the same keys, the config will be merged with the default config of the component. This means that you can override specific nested arguments without having to specify the entire config.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Pipe</code> <p>The component that was added to the pipeline.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.get_pipe_meta","title":"<code>get_pipe_meta</code>","text":"<p>Get the meta information for a component.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.get_pipe_meta--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>name</code> <p>The name of the component to get the meta for.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.make_doc","title":"<code>make_doc</code>","text":"<p>Create a Doc from text.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.make_doc--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>text</code> <p>The text to create the Doc from.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Doc</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.__call__","title":"<code>__call__</code>","text":"<p>Apply each component successively on a document.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>text</code> <p>The text to create the Doc from, or a Doc.</p> <p> TYPE: <code>Union[str, Doc]</code> </p> RETURNS DESCRIPTION <code>Doc</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.pipe","title":"<code>pipe</code>","text":"<p>Process a stream of documents by applying each component successively on batches of documents.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.pipe--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>inputs</code> <p>The inputs to create the Docs from, or Docs directly.</p> <p> TYPE: <code>Union[Iterable, Stream]</code> </p> <code>n_process</code> <p>Deprecated. Use the \".set_processing(num_cpu_workers=n_process)\" method on the returned data stream instead. The number of parallel workers to use. If 0, the operations will be executed sequentially.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.cache","title":"<code>cache</code>","text":"<p>Enable caching for all (trainable) components in the pipeline</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.torch_components","title":"<code>torch_components</code>","text":"<p>Yields components that are PyTorch modules.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.torch_components--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>disable</code> <p>The names of disabled components, which will be skipped.</p> <p> TYPE: <code>Container[str]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Iterable[Tuple[str, TorchComponent]]</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.connected_pipes_names","title":"<code>connected_pipes_names</code>","text":"<p>Returns a list of lists of connected components in the pipeline, i.e. components that share at least one parameter.</p> RETURNS DESCRIPTION <code>List[List[str]]</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.post_init","title":"<code>post_init</code>","text":"<p>Completes the initialization of the pipeline by calling the post_init method of all components that have one. This is useful for components that need to see some data to build their vocabulary, for instance.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.post_init--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The documents to use for initialization. Each component will not necessarily see all the data.</p> <p> TYPE: <code>Iterable[Doc]</code> </p> <code>exclude</code> <p>Components to exclude from post initialization on data</p> <p> TYPE: <code>Optional[Set]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.from_config","title":"<code>from_config</code>  <code>classmethod</code>","text":"<p>Create a pipeline from a config object</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.from_config--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>config</code> <p>The config to use</p> <p> TYPE: <code>Union[Dict[str, Any], Pipeline]</code> DEFAULT: <code>{}</code> </p> <code>vocab</code> <p>The spaCy vocab to use. If True, a new vocab will be created</p> <p> TYPE: <code>Union[Vocab, bool]</code> DEFAULT: <code>True</code> </p> <code>disable</code> <p>Components to disable</p> <p> TYPE: <code>Union[str, Iterable[str]]</code> DEFAULT: <code>EMPTY_LIST</code> </p> <code>enable</code> <p>Components to enable</p> <p> TYPE: <code>Union[str, Iterable[str]]</code> DEFAULT: <code>EMPTY_LIST</code> </p> <code>exclude</code> <p>Components to exclude</p> <p> TYPE: <code>Union[str, Iterable[str]]</code> DEFAULT: <code>EMPTY_LIST</code> </p> <code>meta</code> <p>Metadata to add to the pipeline</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>FrozenDict()</code> </p> RETURNS DESCRIPTION <code>Pipeline</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.__get_validators__","title":"<code>__get_validators__</code>  <code>classmethod</code>","text":"<p>Pydantic validators generator</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.validate","title":"<code>validate</code>  <code>classmethod</code>","text":"<p>Pydantic validator, used in the <code>validate_arguments</code> decorated functions</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.preprocess_many","title":"<code>preprocess_many</code>","text":"<p>Runs the preprocessing methods of each component in the pipeline on a collection of documents and returns an iterable of dictionaries containing the results, with the component names as keys.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.preprocess_many--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p> TYPE: <code>Iterable[Doc]</code> </p> <code>compress</code> <p>Whether to deduplicate identical preprocessing outputs of the results if multiple documents share identical subcomponents. This step is required to enable the cache mechanism when training or running the pipeline over a tabular datasets such as pyarrow tables that do not store referential equality information.</p> <p> DEFAULT: <code>True</code> </p> <code>supervision</code> <p>Whether to include supervision information in the preprocessing</p> <p> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.collate","title":"<code>collate</code>","text":"<p>Collates a batch of preprocessed samples into a single (maybe nested) dictionary of tensors by calling the collate method of each component.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.collate--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <p>The batch of preprocessed samples</p> <p> TYPE: <code>Union[Iterable[Dict[str, Any]], Dict[str, Any]]</code> </p> <code>device</code> <p>Should we move the tensors to a device, if so, which one?</p> <p> TYPE: <code>Optional[Union[str, device]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>The collated batch</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.parameters","title":"<code>parameters</code>","text":"<p>Returns an iterator over the Pytorch parameters of the components in the pipeline</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.named_parameters","title":"<code>named_parameters</code>","text":"<p>Returns an iterator over the Pytorch parameters of the components in the pipeline</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.to","title":"<code>to</code>","text":"<p>Moves the pipeline to a given device</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.train","title":"<code>train</code>","text":"<p>Enables training mode on pytorch modules</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.train--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>mode</code> <p>Whether to enable training or not</p> <p> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.to_disk","title":"<code>to_disk</code>","text":"<p>Save the pipeline to a directory.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.to_disk--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>The path to the directory to save the pipeline to. Every component will be saved to separated subdirectories of this directory, except for tensors that will be saved to a shared files depending on the references between the components.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>exclude</code> <p>The names of the components, or attributes to exclude from the saving process. By default, the vocabulary is excluded since it may contain personal identifiers and can be rebuilt during inference.</p> <p> TYPE: <code>Optional[Set[str]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.from_disk","title":"<code>from_disk</code>","text":"<p>Load the pipeline from a directory. Components will be updated in-place.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.from_disk--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>The path to the directory to load the pipeline from</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>exclude</code> <p>The names of the components, or attributes to exclude from the loading process.</p> <p> TYPE: <code>Optional[Union[str, Sequence[str]]]</code> DEFAULT: <code>None</code> </p> <code>device</code> <p>Device to use when loading the tensors</p> <p> TYPE: <code>Optional[Union[str, device]]</code> DEFAULT: <code>'cpu'</code> </p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.select_pipes","title":"<code>select_pipes</code>","text":"<p>Temporarily disable and enable components in the pipeline.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline.select_pipes--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>disable</code> <p>The name of the component to disable, or a list of names.</p> <p> TYPE: <code>Optional[Union[str, Iterable[str]]]</code> DEFAULT: <code>None</code> </p> <code>enable</code> <p>The name of the component to enable, or a list of names.</p> <p> TYPE: <code>Optional[Union[str, Iterable[str]]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.blank","title":"<code>blank</code>","text":"<p>Loads an empty EDS-NLP Pipeline, similarly to <code>spacy.blank</code>. In addition to standard components, this pipeline supports EDS-NLP trainable torch components.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.blank--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.covid())\n</code></pre>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.blank--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>lang</code> <p>Language ID, e.g. \"en\", \"fr\", \"eds\", etc.</p> <p> TYPE: <code>str</code> </p> <code>config</code> <p>The config to use for the pipeline</p> <p> TYPE: <code>Union[Dict[str, Any], Config]</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Pipeline</code> <p>The new empty pipeline instance.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.load","title":"<code>load</code>","text":"<p>Load a pipeline from a config file or a directory.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.load--examples","title":"Examples","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.load(\n    \"path/to/config.cfg\",\n    overrides={\"components\": {\"my_component\": {\"arg\": \"value\"}}},\n)\n</code></pre>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.load--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>model</code> <p>The config to use for the pipeline, or the path to a config file or a directory.</p> <p> TYPE: <code>Union[str, Path, Config]</code> </p> <code>overrides</code> <p>Overrides to apply to the config when loading the pipeline. These are the same parameters as the ones used when initializing the pipeline.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>exclude</code> <p>The names of the components, or attributes to exclude from the loading process.  The <code>exclude</code> argument will be mutated in place.</p> <p> TYPE: <code>Optional[AsList[str]]</code> DEFAULT: <code>EMPTY_LIST</code> </p> <code>auto_update</code> <p>When installing a pipeline from the Hugging Face Hub, whether to automatically try to update the model, even if a local version is found. Only applies when loading from the Hugging Face Hub.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>install_dependencies</code> <p>When installing a pipeline from the Hugging Face Hub, whether to install the dependencies of the model if they are not already installed. Only applies when loading from the Hugging Face Hub.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Pipeline</code>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.load_from_huggingface","title":"<code>load_from_huggingface</code>","text":"<p>Load a model from the Hugging Face Hub.</p>"},{"location":"reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.load_from_huggingface--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>repo_id</code> <p>The repository ID of the model to load (e.g. \"username/repo_name\").</p> <p> TYPE: <code>str</code> </p> <code>auto_update</code> <p>Whether to automatically try to update the model, even if a local version is found.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>install_dependencies</code> <p>Whether to install the dependencies of the model if they are not already installed.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>token</code> <p>The Hugging Face Hub API token to use.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>revision</code> <p>The revision of the model to load.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the model's <code>load</code> method.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/core/registries/","title":"<code>edsnlp.core.registries</code>","text":""},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.DraftPipe","title":"<code>DraftPipe</code>","text":"<p>           Bases: <code>Draft[T]</code></p>"},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.DraftPipe.maybe_nlp","title":"<code>maybe_nlp</code>","text":"<p>If the factory requires an nlp argument and the user has explicitly provided it (this is unusual, we usually expect the factory to be instantiated via add_pipe, or a config), then we should instantiate it.</p> RETURNS DESCRIPTION <code>Union[PartialFactory, Any]</code>"},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.DraftPipe.instantiate","title":"<code>instantiate</code>","text":"<p>To ensure compatibility with spaCy's API, we need to support passing in the nlp object and name to factories. Since they can be nested, we need to add them to every factory in the config.</p>"},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.FactoryRegistry","title":"<code>FactoryRegistry</code>","text":"<p>           Bases: <code>Registry</code></p> <p>A registry that validates the input arguments of the registered functions.</p>"},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.FactoryRegistry.get","title":"<code>get</code>","text":"<p>Get the registered function for a given name. Since we want to be able to get functions registered under spacy namespace, and functions defined in memory but not accessible via entry points (spacy internal namespace), we need to check multiple registries.</p> <p>The strategy is the following:</p> <ol> <li>If the function exists in the edsnlp_factories namespace, return it as a    curried function.</li> <li>Otherwise, check spacy namespaces and re-register it under edsnlp_factories    namespace, then return it as a curried function.</li> <li>Otherwise, search in edsnlp's entry points and redo steps 1 &amp; 2</li> <li>Otherwise, search in spacy's points and redo steps 1 &amp; 2</li> <li>Fail</li> </ol> <p>name (str): The name. RETURNS (Any): The registered function.</p>"},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.FactoryRegistry.register","title":"<code>register</code>","text":"<p>This is a convenience wrapper around <code>confit.Registry.register</code>, that curries the function to be registered, allowing to instantiate the class later once <code>nlp</code> and <code>name</code> are known.</p>"},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.FactoryRegistry.register--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>name</code> <p> TYPE: <code>str</code> </p> <code>func</code> <p> TYPE: <code>Optional[InFunc]</code> DEFAULT: <code>None</code> </p> <code>default_config</code> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>FrozenDict()</code> </p> <code>assigns</code> <p> TYPE: <code>Iterable[str]</code> DEFAULT: <code>FrozenList()</code> </p> <code>requires</code> <p> TYPE: <code>Iterable[str]</code> DEFAULT: <code>FrozenList()</code> </p> <code>retokenizes</code> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default_score_weights</code> <p> TYPE: <code>Dict[str, Optional[float]]</code> DEFAULT: <code>FrozenDict()</code> </p> <code>invoker</code> <p> TYPE: <code>Callable</code> DEFAULT: <code>None</code> </p> <code>deprecated</code> <p> TYPE: <code>Sequence[str]</code> DEFAULT: <code>()</code> </p> <code>spacy_compatible</code> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Callable[[InFunc], InFunc]</code>"},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.accepted_arguments","title":"<code>accepted_arguments</code>","text":"<p>Checks that a function accepts a list of keyword arguments</p>"},{"location":"reference/edsnlp/core/registries/#edsnlp.core.registries.accepted_arguments--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>func</code> <p>Function to check</p> <p> TYPE: <code>Callable</code> </p> <code>args</code> <p>Argument or list of arguments to check</p> <p> TYPE: <code>Sequence[str]</code> </p> RETURNS DESCRIPTION <code>List[str]</code>"},{"location":"reference/edsnlp/core/stream/","title":"<code>edsnlp.core.stream</code>","text":""},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream","title":"<code>Stream</code>","text":""},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.set_processing","title":"<code>set_processing</code>","text":""},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.set_processing--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch_size</code> <p>The batch size. Can also be a batching expression like \"32 docs\", \"1024 words\", \"dataset\", \"fragment\", etc.</p> <p> TYPE: <code>Optional[Union[int, float, str]]</code> DEFAULT: <code>None</code> </p> <code>batch_by</code> <p>Function to compute the batches. If set, it should take an iterable of documents and return an iterable of batches. You can also set it to \"docs\", \"words\" or \"padded_words\" to use predefined batching functions. Defaults to \"docs\".</p> <p> TYPE: <code>BatchBy</code> DEFAULT: <code>None</code> </p> <code>num_cpu_workers</code> <p>Number of CPU workers. A CPU worker handles the non deep-learning components and the preprocessing, collating and postprocessing of deep-learning components. If no GPU workers are used, the CPU workers also handle the forward call of the deep-learning components.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>num_gpu_workers</code> <p>Number of GPU workers. A GPU worker handles the forward call of the deep-learning components. Only used with \"multiprocessing\" backend.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>disable_implicit_parallelism</code> <p>Whether to disable OpenMP and Huggingface tokenizers implicit parallelism in multiprocessing mode. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>backend</code> <p>The backend to use for parallel processing. If not set, the backend is automatically selected based on the input data and the number of workers.</p> <ul> <li>\"simple\" is the default backend and is used when <code>num_cpu_workers</code> is 1     and <code>num_gpu_workers</code> is 0.</li> <li>\"multiprocessing\" is used when <code>num_cpu_workers</code> is greater than 1 or     <code>num_gpu_workers</code> is greater than 0.</li> <li>\"spark\" is used when the input data is a Spark dataframe and the output     writer is a Spark writer.</li> </ul> <p> TYPE: <code>Optional[Literal['simple', 'multiprocessing', 'mp', 'spark']]</code> DEFAULT: <code>None</code> </p> <code>autocast</code> <p>Whether to use automatic mixed precision (AMP) for the forward pass of the deep-learning components. If True (by default), AMP will be used with the default settings. If False, AMP will not be used. If a dtype is provided, it will be passed to the <code>torch.autocast</code> context manager.</p> <p> TYPE: <code>Union[bool, Any]</code> DEFAULT: <code>None</code> </p> <code>show_progress</code> <p>Whether to show progress bars (only applicable with \"simple\" and \"multiprocessing\" backends).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>gpu_pipe_names</code> <p>List of pipe names to accelerate on a GPUWorker, defaults to all pipes that inherit from TorchComponent. Only used with \"multiprocessing\" backend. Inferred from the pipeline if not set.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>process_start_method</code> <p>Whether to use \"fork\" or \"spawn\" as the start method for the multiprocessing backend. The default is \"fork\" on Unix systems and \"spawn\" on Windows.</p> <ul> <li>\"fork\" is the default start method on Unix systems and is the fastest     start method, but it is not available on Windows, can cause issues     with CUDA and is not safe when using multiple threads.</li> <li>\"spawn\" is the default start method on Windows and is the safest start     method, but it is not available on Unix systems and is slower than     \"fork\".</li> </ul> <p> TYPE: <code>Optional[Literal['fork', 'spawn']]</code> DEFAULT: <code>None</code> </p> <code>gpu_worker_devices</code> <p>List of GPU devices to use for the GPU workers. Defaults to all available devices, one worker per device. Only used with \"multiprocessing\" backend.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>cpu_worker_devices</code> <p>List of GPU devices to use for the CPU workers. Used for debugging purposes.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>deterministic</code> <p>Whether to try and preserve the order of the documents in \"multiprocessing\" mode. If set to <code>False</code>, workers will process documents whenever they are available in a dynamic fashion, which may result in out-of-order but usually faster processing. If set to true, tasks will be distributed in a static, round-robin fashion to workers. Defaults to <code>True</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.map","title":"<code>map</code>","text":"<p>Maps a callable to the documents. It takes a callable as input and an optional dictionary of keyword arguments. The function will be applied to each element of the collection. If the callable is a generator function, each element will be yielded to the stream as is.</p>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.map--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>pipe</code> <p>The callable to map to the documents.</p> <p> </p> <code>kwargs</code> <p>The keyword arguments to pass to the callable.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.flatten","title":"<code>flatten</code>","text":"<p>Flattens the stream.</p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.map_batches","title":"<code>map_batches</code>","text":"<p>Maps a callable to a batch of documents. The callable should take a list of inputs. The output of the callable will be flattened if it is a list or a generator, or yielded to the stream as is if it is a single output (tuple or any other type).</p>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.map_batches--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>pipe</code> <p>The callable to map to the documents.</p> <p> </p> <code>kwargs</code> <p>The keyword arguments to pass to the callable.</p> <p> DEFAULT: <code>{}</code> </p> <code>batch_size</code> <p>The batch size. Can also be a batching expression like \"32 docs\", \"1024 words\", \"dataset\", \"fragment\", etc.</p> <p> TYPE: <code>Optional[Union[int, float, str]]</code> DEFAULT: <code>None</code> </p> <code>batch_by</code> <p>Function to compute the batches. If set, it should take an iterable of documents and return an iterable of batches. You can also set it to \"docs\", \"words\" or \"padded_words\" to use predefined batching functions. Defaults to \"docs\".</p> <p> TYPE: <code>BatchBy</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.batchify","title":"<code>batchify</code>","text":"<p>Accumulates the documents into batches and yield each batch to the stream.</p>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.batchify--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch_size</code> <p>The batch size. Can also be a batching expression like \"32 docs\", \"1024 words\", \"dataset\", \"fragment\", etc.</p> <p> TYPE: <code>Optional[Union[int, float, str]]</code> DEFAULT: <code>None</code> </p> <code>batch_by</code> <p>Function to compute the batches. If set, it should take an iterable of documents and return an iterable of batches. You can also set it to \"docs\", \"words\" or \"padded_words\" to use predefined batching functions. Defaults to \"docs\".</p> <p> TYPE: <code>BatchBy</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.map_gpu","title":"<code>map_gpu</code>","text":"<p>Maps a deep learning operation to a batch of documents, on a GPU worker.</p>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.map_gpu--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>prepare_batch</code> <p>A callable that takes a list of documents and a device and returns a batch of tensors (or anything that can be passed to the <code>forward</code> callable). This will be called on a CPU-bound worker, and may be parallelized.</p> <p> TYPE: <code>Callable[[List, Union[str, device]], Any]</code> </p> <code>forward</code> <p>A callable that takes the output of <code>prepare_batch</code> and returns the output of the deep learning operation. This will be called on a GPU-bound worker.</p> <p> TYPE: <code>Callable[[Any], Any]</code> </p> <code>postprocess</code> <p>An optional callable that takes the list of documents and the output of the deep learning operation, and returns the final output. This will be called on the same CPU-bound worker that called the <code>prepare_batch</code> function.</p> <p> TYPE: <code>Optional[Callable[[List, Any], Any]]</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>The batch size. Can also be a batching expression like \"32 docs\", \"1024 words\", \"dataset\", \"fragment\", etc.</p> <p> TYPE: <code>Optional[Union[int, float, str]]</code> DEFAULT: <code>None</code> </p> <code>batch_by</code> <p>Function to compute the batches. If set, it should take an iterable of documents and return an iterable of batches. You can also set it to \"docs\", \"words\" or \"padded_words\" to use predefined batching functions. Defaults to \"docs\".</p> <p> TYPE: <code>BatchBy</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.map_pipeline","title":"<code>map_pipeline</code>","text":"<p>Maps a pipeline to the documents, i.e. adds each component of the pipeline to the stream operations. This function is called under the hood by <code>nlp.pipe()</code></p>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.map_pipeline--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>model</code> <p>The pipeline to map to the documents.</p> <p> TYPE: <code>Pipeline</code> </p> <code>batch_size</code> <p>The batch size. Can also be a batching expression like \"32 docs\", \"1024 words\", \"dataset\", \"fragment\", etc.</p> <p> TYPE: <code>Optional[Union[int, float, str]]</code> DEFAULT: <code>None</code> </p> <code>batch_by</code> <p>Function to compute the batches. If set, it should take an iterable of documents and return an iterable of batches. You can also set it to \"docs\", \"words\" or \"padded_words\" to use predefined batching functions. Defaults to \"docs\".</p> <p> TYPE: <code>BatchBy</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.shuffle","title":"<code>shuffle</code>","text":"<p>Shuffles the stream by accumulating the documents into batches and shuffling the batches. We try to optimize and avoid the accumulation by shuffling items directly in the reader, but if some upstream operations are not elementwise or if the reader is not compatible with the batching mode, we have to accumulate the documents into batches and shuffle the batches.</p> <p>For instance, imagine a reading from list of 2 very large documents and applying an operation to split the documents into sentences. Shuffling only in the reader, then applying the split operation would not shuffle the sentences across documents and may lead to a lack of randomness when training a model. Think of this as having lumps after mixing your data. In our case, we detect that the split op is not elementwise and trigger the accumulation of sentences into batches after their generation before shuffling the batches.</p>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.shuffle--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch_size</code> <p>The batch size. Can also be a batching expression like \"32 docs\", \"1024 words\", \"dataset\", \"fragment\", etc.</p> <p> TYPE: <code>Optional[Union[int, float, str]]</code> DEFAULT: <code>None</code> </p> <code>batch_by</code> <p>Function to compute the batches. If set, it should take an iterable of documents and return an iterable of batches. You can also set it to \"docs\", \"words\" or \"padded_words\" to use predefined batching functions. Defaults to \"docs\".</p> <p> TYPE: <code>Optional[str, BatchFn]</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>shuffle_reader</code> <p>Whether to shuffle the reader. Defaults to True if the reader is compatible with the batch_by mode, False otherwise.</p> <p> TYPE: <code>Optional[Union[bool, str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.loop","title":"<code>loop</code>","text":"<p>Loops over the stream indefinitely.</p> <p>Note that we cycle over items produced by the reader, not the items produced by the stream operations. This means that the stream operations will be applied to the same items multiple times, and may produce different results if they are non-deterministic. This also mean that calling this function will have the same effect regardless of the operations applied to the stream before calling it, ie:</p> <pre><code>stream.loop().map(...)\n# is equivalent to\nstream.map(...).loop()\n</code></pre> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.torch_components","title":"<code>torch_components</code>","text":"<p>Yields components that are PyTorch modules.</p> RETURNS DESCRIPTION <code>Iterable['edsnlp.core.torch_component.TorchComponent']</code>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.train","title":"<code>train</code>","text":"<p>Enables training mode on pytorch modules</p>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.train--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>mode</code> <p>Whether to enable training or not</p> <p> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/core/stream/#edsnlp.core.stream.Stream.eval","title":"<code>eval</code>","text":"<p>Enables evaluation mode on pytorch modules</p>"},{"location":"reference/edsnlp/core/torch_component/","title":"<code>edsnlp.core.torch_component</code>","text":""},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent","title":"<code>TorchComponent</code>","text":"<p>           Bases: <code>BaseComponent</code>, <code>Module</code>, <code>Generic[BatchOutput, BatchInput]</code></p> <p>A TorchComponent is a Component that can be trained and inherits <code>torch.nn.Module</code>. You can use it either as a torch module inside a more complex neural network, or as a standalone component in a Pipeline.</p> <p>In addition to the methods of a torch module, a TorchComponent adds a few methods to handle preprocessing and collating features, as well as caching intermediate results for components that share a common subcomponent.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.post_init","title":"<code>post_init</code>","text":"<p>This method completes the attributes of the component, by looking at some documents. It is especially useful to build vocabularies or detect the labels of a classification task.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.post_init--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>gold_data</code> <p>The documents to use for initialization.</p> <p> TYPE: <code>Iterable[Doc]</code> </p> <code>exclude</code> <p>The names of components to exclude from initialization. This argument will be gradually updated  with the names of initialized components</p> <p> TYPE: <code>Set[str]</code> </p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.preprocess","title":"<code>preprocess</code>","text":"<p>Preprocess the document to extract features that will be used by the neural network and its subcomponents on to perform its predictions.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.preprocess--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>Document to preprocess</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>Dictionary (optionally nested) containing the features extracted from the document.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.collate","title":"<code>collate</code>","text":"<p>Collate the batch of features into a single batch of tensors that can be used by the forward method of the component.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.collate--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <p>Batch of features</p> <p> TYPE: <code>Dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>BatchInput</code> <p>Dictionary (optionally nested) containing the collated tensors</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.batch_to_device","title":"<code>batch_to_device</code>","text":"<p>Move the batch of tensors to the specified device.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.batch_to_device--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <p>Batch of tensors</p> <p> TYPE: <code>BatchInput</code> </p> <code>device</code> <p>Device to move the tensors to</p> <p> TYPE: <code>Optional[Union[str, device]]</code> </p> RETURNS DESCRIPTION <code>BatchInput</code>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.forward","title":"<code>forward</code>","text":"<p>Perform the forward pass of the neural network.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.forward--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <p>Batch of tensors (nested dictionary) computed by the collate method</p> <p> TYPE: <code>BatchInput</code> </p> RETURNS DESCRIPTION <code>BatchOutput</code>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.compute_training_metrics","title":"<code>compute_training_metrics</code>","text":"<p>Compute post-gather metrics on the batch output. This is a no-op by default. This is useful to compute averages when doing multi-gpu training or mini-batch accumulation since full denominators are not known during the forward pass.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.module_forward","title":"<code>module_forward</code>","text":"<p>This is a wrapper around <code>torch.nn.Module.__call__</code> to avoid conflict with the components <code>__call__</code> method.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.prepare_batch","title":"<code>prepare_batch</code>","text":"<p>Convenience method to preprocess a batch of documents and collate them Features corresponding to the same path are grouped together in a list, under the same key.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.prepare_batch--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>Batch of documents</p> <p> TYPE: <code>Sequence[Doc]</code> </p> <code>supervision</code> <p>Whether to extract supervision features or not</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>device</code> <p>Device to move the tensors to</p> <p> TYPE: <code>Optional[Union[str, device]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dict[str, Sequence[Any]]</code>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.batch_process","title":"<code>batch_process</code>","text":"<p>Process a batch of documents using the neural network. This differs from the <code>pipe</code> method in that it does not return an iterator, but executes the component on the whole batch at once.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.batch_process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>Batch of documents</p> <p> TYPE: <code>Sequence[Doc]</code> </p> RETURNS DESCRIPTION <code>Sequence[Doc]</code> <p>Batch of updated documents</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.postprocess","title":"<code>postprocess</code>","text":"<p>Update the documents with the predictions of the neural network. By default, this is a no-op.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.postprocess--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>List of documents to update</p> <p> TYPE: <code>Sequence[Doc]</code> </p> <code>results</code> <p>Batch of predictions, as returned by the forward method</p> <p> TYPE: <code>BatchOutput</code> </p> <code>inputs</code> <p>List of preprocessed features, as returned by the preprocess method</p> <p> TYPE: <code>List[Dict[str, Any]]</code> </p> RETURNS DESCRIPTION <code>Sequence[Doc]</code>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.preprocess_supervised","title":"<code>preprocess_supervised</code>","text":"<p>Preprocess the document to extract features that will be used by the neural network to perform its training. By default, this returns the same features as the <code>preprocess</code> method.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.preprocess_supervised--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>Document to preprocess</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>Dictionary (optionally nested) containing the features extracted from the document.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.pipe","title":"<code>pipe</code>","text":"<p>Applies the component on a collection of documents. It is recommended to use the <code>Pipeline.pipe</code> method instead of this one to apply a pipeline on a collection of documents, to benefit from the caching of intermediate results.</p>"},{"location":"reference/edsnlp/core/torch_component/#edsnlp.core.torch_component.TorchComponent.pipe--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>Input docs</p> <p> TYPE: <code>Iterable[Doc]</code> </p> <code>batch_size</code> <p>Batch size to use when making batched to be process at once</p> <p> DEFAULT: <code>1</code> </p>"},{"location":"reference/edsnlp/data/","title":"<code>edsnlp.data</code>","text":""},{"location":"reference/edsnlp/data/base/","title":"<code>edsnlp.data.base</code>","text":""},{"location":"reference/edsnlp/data/base/#edsnlp.data.base.from_iterable","title":"<code>from_iterable</code>","text":"<p>The IterableReader (or <code>edsnlp.data.from_iterable</code>) reads a list of Python objects ( texts, dictionaries, ...) and yields documents by passing them through the <code>converter</code> if given, or returns them as is.</p>"},{"location":"reference/edsnlp/data/base/#edsnlp.data.base.from_iterable--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.from_iterable([{...}], nlp=nlp, converter=...)\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.from_iterable</code> returns a Stream. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.from_iterable([{...}], converter=...)\n</code></pre>"},{"location":"reference/edsnlp/data/base/#edsnlp.data.base.from_iterable--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to read</p> <p> TYPE: <code>Any</code> </p> <code>converter</code> <p>Converters to use to convert the JSON rows of the data source to Doc objects</p> <p> TYPE: <code>Optional[AsList[Union[str, Callable]]]</code> DEFAULT: <code>None</code> </p> <code>read_in_worker</code> <p>In multiprocessing mode, whether to read the data in the worker processes. If <code>True</code>, the data will be read in the worker processes, requires pickling the input iterable: this is mostly useful if the pickled iterable is smaller than the data itself (eg, an infinite generator of synthetic data). If <code>False</code>, the data will be read in the main process and distributed to the workers.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> <code>shuffle</code> <p>Whether to shuffle the data. If \"dataset\", the whole dataset will be shuffled before starting iterating on it (at the start of every epoch if looping).</p> <p> TYPE: <code>Literal['dataset', False]</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>loop</code> <p>Whether to loop over the data indefinitely.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/data/base/#edsnlp.data.base.to_iterable","title":"<code>to_iterable</code>","text":"<p><code>edsnlp.data.to_iterable</code> returns an iterator of documents, as converted by the <code>converter</code>. In comparison to just iterating over a Stream, this will also apply the <code>converter</code> to the documents, which can lower the data transfer overhead when using multiprocessing.</p>"},{"location":"reference/edsnlp/data/base/#edsnlp.data.base.to_iterable--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.to_iterable([doc], converter=\"omop\")\n</code></pre>"},{"location":"reference/edsnlp/data/base/#edsnlp.data.base.to_iterable--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a Stream).</p> <p> TYPE: <code>Union[Any, Stream]</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments passed to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/brat/","title":"<code>edsnlp.data.brat</code>","text":""},{"location":"reference/edsnlp/data/conll/","title":"<code>edsnlp.data.conll</code>","text":""},{"location":"reference/edsnlp/data/conll/#edsnlp.data.conll.parse_conll","title":"<code>parse_conll</code>","text":"<p>Load a .conll file and return a dictionary with the text, words, and entities. This expects the file to contain multiple sentences, split into words, each one described in a line. Each sentence is separated by an empty line.</p> <p>If possible, looks for a <code>#global.columns</code> comment at the start of the file to extract the column names.</p> <p>Examples:</p> <pre><code>...\n11  jeune   jeune   ADJ     _       Number=Sing     12      amod    _       _\n12  fille   fille   NOUN    _       Gender=Fem|Number=Sing  5       obj     _       _\n13  qui     qui     PRON    _       PronType=Rel    14      nsubj   _       _\n...\n</code></pre>"},{"location":"reference/edsnlp/data/conll/#edsnlp.data.conll.parse_conll--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path or glob path of the brat text file (.txt, not .ann)</p> <p> TYPE: <code>str</code> </p> <code>cols</code> <p>List of column names to use. If None, the first line of the file will be used</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>fs</code> <p>Filesystem to use</p> <p> TYPE: <code>FileSystem</code> DEFAULT: <code>LOCAL_FS</code> </p> RETURNS DESCRIPTION <code>Iterator[Dict]</code>"},{"location":"reference/edsnlp/data/conll/#edsnlp.data.conll.read_conll","title":"<code>read_conll</code>","text":"<p>The ConllReader (or <code>edsnlp.data.read_conll</code>) reads a file or directory of CoNLL files and yields documents.</p> <p>The raw output (i.e., by setting <code>converter=None</code>) will be in the following form for a single doc:</p> <pre><code>{\n    \"words\": [\n        {\"ID\": \"1\", \"FORM\": ...},\n        ...\n    ],\n}\n</code></pre>"},{"location":"reference/edsnlp/data/conll/#edsnlp.data.conll.read_conll--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.read_conll(\"path/to/conll/file/or/directory\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.read_conll</code> returns a Stream. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list :</p> <pre><code>docs = list(edsnlp.data.read_conll(\"path/to/conll/file/or/directory\"))\n</code></pre>"},{"location":"reference/edsnlp/data/conll/#edsnlp.data.conll.read_conll--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path to the directory containing the CoNLL files (will recursively look for files in subdirectories).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>columns</code> <p>List of column names to use. If None, will try to extract to look for a <code>#global.columns</code> comment at the start of the file to extract the column names.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>shuffle</code> <p>Whether to shuffle the data. If \"dataset\", the whole dataset will be shuffled before starting iterating on it (at the start of every epoch if looping).</p> <p> TYPE: <code>Literal['dataset', False]</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>loop</code> <p>Whether to loop over the data indefinitely.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>nlp</code> <p>The pipeline object (optional and likely not needed, prefer to use the <code>tokenizer</code> directly argument instead).</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>tokenizer</code> <p>The tokenizer instance used to tokenize the documents. Likely not needed since by default it uses the current context tokenizer :</p> <ul> <li>the tokenizer of the next pipeline run by <code>.map_pipeline</code> in a   Stream.</li> <li>or the <code>eds</code> tokenizer by default.</li> </ul> <p> TYPE: <code>Optional[Tokenizer]</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects.</p> <p> TYPE: <code>Optional[AsList[Union[str, Callable]]]</code> DEFAULT: <code>['conll']</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/data/converters/","title":"<code>edsnlp.data.converters</code>","text":"<p>Converters are used to convert documents between python dictionaries and Doc objects. There are two types of converters: readers and writers. Readers convert dictionaries to Doc objects, and writers convert Doc objects to dictionaries.</p>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.AttributesMappingArg","title":"<code>AttributesMappingArg</code>","text":"<p>           Bases: <code>Validated</code></p> <p>A span attribute mapping (can be a list too to keep the same names).</p> <p>For instance:</p> <ul> <li><code>doc_attributes=\"note_datetime\"</code> will map the <code>note_datetime</code> JSON attribute to   the <code>note_datetime</code> extension.</li> <li><code>span_attributes=[\"negation\", \"family\"]</code> will map the <code>negation</code> and <code>family</code> JSON   attributes to the <code>negation</code> and <code>family</code> extensions.</li> </ul>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.StandoffDict2DocConverter","title":"<code>StandoffDict2DocConverter</code>","text":"<p>Why does BRAT/Standoff need a converter ?</p> <p>You may wonder : why do I need a converter ? Since BRAT is already a NLP oriented format, it should be straightforward to convert it to a Doc object.</p> <p>Indeed, we do provide a default converter for the BRAT standoff format, but we also acknowledge that there may be more than one way to convert a standoff document to a Doc object. For instance, an annotated span may be used to represent a relation between two smaller included entities, or another entity scope, etc.</p> <p>In such cases, we recommend you use a custom converter as described here.</p>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.StandoffDict2DocConverter--examples","title":"Examples","text":"<pre><code># Any kind of reader (`edsnlp.data.read/from_...`) can be used here\ndocs = edsnlp.data.read_standoff(\n    \"path/to/standoff\",\n    converter=\"standoff\",  # set by default\n\n    # Optional parameters\n    tokenizer=tokenizer,\n    span_setter={\"ents\": True, \"*\": True},\n    span_attributes={\"negation\": \"negated\"},\n    keep_raw_attribute_values=False,\n    default_attributes={\"negated\": False, \"temporality\": \"present\"},\n)\n</code></pre>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.StandoffDict2DocConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object (optional and likely not needed, prefer to use the <code>tokenizer</code> directly argument instead).</p> <p> </p> <code>tokenizer</code> <p>The tokenizer instance used to tokenize the documents. Likely not needed since by default it uses the current context tokenizer :</p> <ul> <li>the tokenizer of the next pipeline run by <code>.map_pipeline</code> in a   Stream.</li> <li>or the <code>eds</code> tokenizer by default.</li> </ul> <p> TYPE: <code>Optional[Tokenizer]</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use when setting the spans in the documents. Defaults to setting the spans in the <code>ents</code> attribute, and creates a new span group for each JSON entity label.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, '*': True}</code> </p> <code>span_attributes</code> <p>Mapping from BRAT attributes to Span extensions (can be a list too). By default, all attributes are imported as Span extensions with the same name.</p> <p> TYPE: <code>Optional[AttributesMappingArg]</code> DEFAULT: <code>None</code> </p> <code>keep_raw_attribute_values</code> <p>Whether to keep the raw attribute values (as strings) or to convert them to Python objects (e.g. booleans).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default_attributes</code> <p>How to set attributes on spans for which no attribute value was found in the input format. This is especially useful for negation, or frequent attributes values (e.g. \"negated\" is often False, \"temporal\" is often \"present\"), that annotators may not want to annotate every time.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p> <code>notes_as_span_attribute</code> <p>If set, the AnnotatorNote annotations will be concatenated and stored in a span attribute with this name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>split_fragments</code> <p>Whether to split the fragments into separate spans or not. If set to False, the fragments will be concatenated into a single span.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.StandoffDoc2DictConverter","title":"<code>StandoffDoc2DictConverter</code>","text":""},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.StandoffDoc2DictConverter--examples","title":"Examples","text":"<pre><code># Any kind of writer (`edsnlp.data.read/from_...`) can be used here\nedsnlp.data.write_standoff(\n    docs,\n    converter=\"standoff\",  # set by default\n\n    # Optional parameters\n    span_getter={\"ents\": True},\n    span_attributes=[\"negation\"],\n)\n# or docs.to_standoff(...) if it's already a\n# [Stream][edsnlp.core.stream.Stream]\n</code></pre>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.StandoffDoc2DictConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use when getting the spans from the documents. Defaults to getting the spans in the <code>ents</code> attribute.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>{'ents': True}</code> </p> <code>span_attributes</code> <p>Mapping from Span extensions to JSON attributes (can be a list too). By default, no attribute is exported, except <code>note_id</code>.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.ConllDict2DocConverter","title":"<code>ConllDict2DocConverter</code>","text":"<p>TODO</p>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.OmopDict2DocConverter","title":"<code>OmopDict2DocConverter</code>","text":""},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.OmopDict2DocConverter--examples","title":"Examples","text":"<pre><code># Any kind of reader (`edsnlp.data.read/from_...`) can be used here\ndocs = edsnlp.data.from_pandas(\n    df,\n    converter=\"omop\",\n\n    # Optional parameters\n    tokenizer=tokenizer,\n    doc_attributes=[\"note_datetime\"],\n\n    # Parameters below should only matter if you plan to import entities\n    # from the dataframe. If the data doesn't contain pre-annotated\n    # entities, you can ignore these.\n    span_setter={\"ents\": True, \"*\": True},\n    span_attributes={\"negation\": \"negated\"},\n    default_attributes={\"negated\": False, \"temporality\": \"present\"},\n)\n</code></pre>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.OmopDict2DocConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object (optional and likely not needed, prefer to use the <code>tokenizer</code> directly argument instead).</p> <p> </p> <code>tokenizer</code> <p>The tokenizer instance used to tokenize the documents. Likely not needed since by default it uses the current context tokenizer :</p> <ul> <li>the tokenizer of the next pipeline run by <code>.map_pipeline</code> in a   Stream.</li> <li>or the <code>eds</code> tokenizer by default.</li> </ul> <p> TYPE: <code>Optional[Tokenizer]</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use when setting the spans in the documents. Defaults to setting the spans in the <code>ents</code> attribute, and creates a new span group for each JSON entity label.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, '*': True}</code> </p> <code>doc_attributes</code> <p>Mapping from JSON attributes to additional Span extensions (can be a list too). By default, all attributes are imported as Doc extensions with the same name.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{'note_datetime': 'note_datetime'}</code> </p> <code>span_attributes</code> <p>Mapping from JSON attributes to Span extensions (can be a list too). By default, all attributes are imported as Span extensions with the same name.</p> <p> TYPE: <code>Optional[AttributesMappingArg]</code> DEFAULT: <code>None</code> </p> <code>default_attributes</code> <p>How to set attributes on spans for which no attribute value was found in the input format. This is especially useful for negation, or frequent attributes values (e.g. \"negated\" is often False, \"temporal\" is often \"present\"), that annotators may not want to annotate every time.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.OmopDoc2DictConverter","title":"<code>OmopDoc2DictConverter</code>","text":""},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.OmopDoc2DictConverter--examples","title":"Examples","text":"<pre><code># Any kind of writer (`edsnlp.data.write/to_...`) can be used here\ndf = edsnlp.data.to_pandas(\n    docs,\n    converter=\"omop\",\n\n    # Optional parameters\n    span_getter={\"ents\": True},\n    doc_attributes=[\"note_datetime\"],\n    span_attributes=[\"negation\", \"family\"],\n)\n# or docs.to_pandas(...) if it's already a\n# [Stream][edsnlp.core.stream.Stream]\n</code></pre>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.OmopDoc2DictConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use when getting the spans from the documents. Defaults to getting the spans in the <code>ents</code> attribute.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>doc_attributes</code> <p>Mapping from Doc extensions to JSON attributes (can be a list too). By default, no doc attribute is exported, except <code>note_id</code>.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p> <code>span_attributes</code> <p>Mapping from Span extensions to JSON attributes (can be a list too). By default, no attribute is exported.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.EntsDoc2DictConverter","title":"<code>EntsDoc2DictConverter</code>","text":""},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.EntsDoc2DictConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use when getting the spans from the documents. Defaults to getting the spans in the <code>ents</code> attribute.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>doc_attributes</code> <p>Mapping from Doc extensions to JSON attributes (can be a list too). By default, no doc attribute is exported, except <code>note_id</code>.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p> <code>span_attributes</code> <p>Mapping from Span extensions to JSON attributes (can be a list too). By default, no attribute is exported.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.MarkupToDocConverter","title":"<code>MarkupToDocConverter</code>","text":""},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.MarkupToDocConverter--examples","title":"Examples","text":"<pre><code>import edsnlp\n\n# Any kind of reader (`edsnlp.data.read/from_...`) can be used here\n# If input items are dicts, the converter expects a \"text\" key/column.\ndocs = list(\n    edsnlp.data.from_iterable(\n        [\n            \"This [is](VERB negation=True) not a [test](NOUN).\",\n            \"This is another [test](NOUN).\",\n        ],\n        converter=\"markup\",\n        span_setter=\"entities\",\n    ),\n)\nprint(docs[0].spans[\"entities\"])\n# Out: [is, test]\n</code></pre> <p>You can also use it directly on a string:</p> <pre><code>from edsnlp.data.converters import MarkupToDocConverter\n\nconverter = MarkupToDocConverter(\n    span_setter={\"verb\": \"VERB\", \"noun\": \"NOUN\"},\n    preset=\"xml\",\n)\ndoc = converter(\"This &lt;VERB negation=True&gt;is&lt;/VERB&gt; not a &lt;NOUN&gt;test&lt;/NOUN&gt;.\")\nprint(doc.spans[\"verb\"])\n# Out: [is]\nprint(doc.spans[\"verb\"][0]._.negation)\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/data/converters/#edsnlp.data.converters.MarkupToDocConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>preset</code> <p>The preset to use for the markup format. Defaults to \"md\" (Markdown-like syntax). Use \"xml\" for XML-like syntax.</p> <p> TYPE: <code>Literal['md', 'xml']</code> DEFAULT: <code>'md'</code> </p> <code>opener</code> <p>The regex pattern to match the opening tag of the markup. Defaults to the preset's opener.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>closer</code> <p>The regex pattern to match the closing tag of the markup. Defaults to the preset's closer.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>tokenizer</code> <p>The tokenizer instance used to tokenize the documents. Likely not needed since by default it uses the current context tokenizer :</p> <ul> <li>the tokenizer of the next pipeline run by <code>.map_pipeline</code> in a   Stream.</li> <li>or the <code>eds</code> tokenizer by default.</li> </ul> <p> TYPE: <code>Optional[Tokenizer]</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use when setting the spans in the documents. Defaults to setting the spans in the <code>ents</code> attribute and creates a new span group for each JSON entity label.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, '*': True}</code> </p> <code>span_attributes</code> <p>Mapping from markup attributes to Span extensions (can be a list too). By default, all attributes are imported as Span extensions with the same name.</p> <p> TYPE: <code>Optional[AttributesMappingArg]</code> DEFAULT: <code>None</code> </p> <code>keep_raw_attribute_values</code> <p>Whether to keep the raw attribute values (as strings) or to convert them to Python objects (e.g. booleans).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default_attributes</code> <p>How to set attributes on spans for which no attribute value was found in the input format. This is especially useful for negation, or frequent attributes values (e.g. \"negated\" is often False, \"temporal\" is often \"present\"), that annotators may not want to annotate every time.</p> <p> TYPE: <code>AttributesMappingArg</code> DEFAULT: <code>{}</code> </p> <code>bool_attributes</code> <p>List of boolean attributes to set to False by default. This is useful for attributes that are often not annotated, but you want to have a default value for them.</p> <p> TYPE: <code>AsList[str]</code> DEFAULT: <code>[]</code> </p>"},{"location":"reference/edsnlp/data/json/","title":"<code>edsnlp.data.json</code>","text":""},{"location":"reference/edsnlp/data/json/#edsnlp.data.json.read_json","title":"<code>read_json</code>","text":"<p>The JsonReader (or <code>edsnlp.data.read_json</code>) reads a directory of JSON files and yields documents. At the moment, only entities and attributes are loaded.</p>"},{"location":"reference/edsnlp/data/json/#edsnlp.data.json.read_json--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.read_json(\"path/to/json/dir\", converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.read_json</code> returns a Stream. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.read_json(\"path/to/json/dir\", converter=\"omop\")\n</code></pre>"},{"location":"reference/edsnlp/data/json/#edsnlp.data.json.read_json--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path to the directory containing the JSON files (will recursively look for files in subdirectories).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>keep_ipynb_checkpoints</code> <p>Whether to keep the files have \".ipynb_checkpoints\" in their path.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>shuffle</code> <p>Whether to shuffle the data. If \"dataset\", the whole dataset will be shuffled before starting iterating on it (at the start of every epoch if looping).</p> <p> TYPE: <code>Literal['dataset', False]</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>loop</code> <p>Whether to loop over the data indefinitely.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converters to use to convert the JSON objects to Doc objects. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[AsList[Union[str, Callable]]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/data/json/#edsnlp.data.json.write_json","title":"<code>write_json</code>","text":"<p><code>edsnlp.data.write_json</code> writes a list of documents using the JSON format in a directory. If <code>lines</code> is false, each document will be stored in its own JSON file, named after the FILENAME field returned by the converter (commonly the <code>note_id</code> attribute of the documents), and subdirectories will be created if the name contains <code>/</code> characters.</p>"},{"location":"reference/edsnlp/data/json/#edsnlp.data.json.write_json--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.write_json([doc], \"path/to/json/file\", converter=\"omop\", lines=True)\n# or to write a directory of JSON files, ensure that each doc has a doc._.note_id\n# attribute, since this will be used as a filename:\nedsnlp.data.write_json([doc], \"path/to/json/dir\", converter=\"omop\", lines=False)\n</code></pre> <p>Overwriting files</p> <p>By default, <code>write_json</code> will raise an error if the directory already exists and contains files with <code>.a*</code> or <code>.txt</code> suffixes. This is to avoid overwriting existing annotations. To allow overwriting existing files, use <code>overwrite=True</code>.</p>"},{"location":"reference/edsnlp/data/json/#edsnlp.data.json.write_json--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a Stream).</p> <p> TYPE: <code>Union[Any, Stream]</code> </p> <code>path</code> <p>Path to either - a file if <code>lines</code> is true : this will write the documents as a JSONL file - a directory if <code>lines</code> is false: this will write one JSON file per document   using the FILENAME field returned by the converter (commonly the <code>note_id</code>   attribute of the documents) as the filename.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>lines</code> <p>Whether to write the documents as a JSONL file or as a directory of JSON files. By default, this is inferred from the path: if the path is a file, lines is assumed to be true, otherwise it is assumed to be false.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>overwrite</code> <p>Whether to overwrite existing directories.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>execute</code> <p>Whether to execute the writing operation immediately or to return a stream</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before writing them. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/pandas/","title":"<code>edsnlp.data.pandas</code>","text":""},{"location":"reference/edsnlp/data/pandas/#edsnlp.data.pandas.from_pandas","title":"<code>from_pandas</code>","text":"<p>The PandasReader (or <code>edsnlp.data.from_pandas</code>) handles reading from a table and yields documents. At the moment, only entities and attributes are loaded. Relations and events are not supported.</p>"},{"location":"reference/edsnlp/data/pandas/#edsnlp.data.pandas.from_pandas--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.from_pandas(df, nlp=nlp, converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.from_pandas</code> returns a Stream. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.from_pandas(df, converter=\"omop\"))\n</code></pre>"},{"location":"reference/edsnlp/data/pandas/#edsnlp.data.pandas.from_pandas--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>Pandas object</p> <p> </p> <code>shuffle</code> <p>Whether to shuffle the data. If \"dataset\", the whole dataset will be shuffled before starting iterating on it (at the start of every epoch if looping).</p> <p> TYPE: <code>Literal['dataset', False]</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>loop</code> <p>Whether to loop over the data indefinitely.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converters to use to convert the rows of the DataFrame (represented as dicts) to Doc objects. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[AsList[Union[str, Callable]]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/data/pandas/#edsnlp.data.pandas.to_pandas","title":"<code>to_pandas</code>","text":"<p><code>edsnlp.data.to_pandas</code> writes a list of documents as a pandas table.</p>"},{"location":"reference/edsnlp/data/pandas/#edsnlp.data.pandas.to_pandas--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.to_pandas([doc], converter=\"omop\")\n</code></pre>"},{"location":"reference/edsnlp/data/pandas/#edsnlp.data.pandas.to_pandas--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a Stream).</p> <p> TYPE: <code>Union[Any, Stream]</code> </p> <code>dtypes</code> <p>Dictionary of column names to dtypes. This is passed to <code>pd.DataFrame.astype</code>.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>execute</code> <p>Whether to execute the writing operation immediately or to return a stream</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before storing them in the dataframe. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/parquet/","title":"<code>edsnlp.data.parquet</code>","text":""},{"location":"reference/edsnlp/data/parquet/#edsnlp.data.parquet.read_parquet","title":"<code>read_parquet</code>","text":"<p>The ParquetReader (or <code>edsnlp.data.read_parquet</code>) reads a directory of parquet files (or a single file) and yields documents.</p>"},{"location":"reference/edsnlp/data/parquet/#edsnlp.data.parquet.read_parquet--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.read_parquet(\"path/to/parquet\", converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.read_parquet</code> returns a Stream. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.read_parquet(\"path/to/parquet\", converter=\"omop\"))\n</code></pre>"},{"location":"reference/edsnlp/data/parquet/#edsnlp.data.parquet.read_parquet--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path to the directory containing the parquet files (will recursively look for files in subdirectories). Supports any filesystem supported by pyarrow.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>shuffle</code> <p>Whether to shuffle the data. If \"dataset\", the whole dataset will be shuffled before starting iterating on it (at the start of every epoch if looping). If \"fragment\", shuffling will occur between and inside the parquet files, but not across them.</p> <p>Dataset shuffling</p> <p>Shuffling the dataset can be expensive, especially for large datasets, since it requires reading the entire dataset into memory. If you have a large dataset, consider shuffling at the \"fragment\" level.</p> <p> TYPE: <code>Literal['dataset', 'fragment', False]</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>loop</code> <p>Whether to loop over the data indefinitely.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>work_unit</code> <p>Only affects the multiprocessing mode. If \"record\", every worker will start to read the same parquet file and yield each every num_workers-th record, starting at an offset each. For instance, if num_workers=2, the first worker will read the 1st, 3rd, 5th, ... records, while the second worker will read the 2nd, 4th, 6th, ... records of the first parquet file.</p> <p>If \"fragment\", each worker will read a different parquet file. For instance, the first worker will every record of the 1st parquet file, the second worker will read every record of the 2nd parquet file, and so on. This way, no record is \"wasted\" and every record loaded in memory is yielded.</p> <p> TYPE: <code>Literal['record', 'fragment']</code> DEFAULT: <code>'record'</code> </p> <p>converter: Optional[AsList[Union[str, Callable]]]     Converters to use to convert the parquet rows of the data source to Doc objects     These are documented on the Converters page. kwargs:     Additional keyword arguments to pass to the converter. These are documented on     the Converters page.</p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/data/parquet/#edsnlp.data.parquet.write_parquet","title":"<code>write_parquet</code>","text":"<p><code>edsnlp.data.write_parquet</code> writes a list of documents as a parquet dataset.</p>"},{"location":"reference/edsnlp/data/parquet/#edsnlp.data.parquet.write_parquet--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.write_parquet([doc], \"path/to/parquet\")\n</code></pre> <p>Overwriting files</p> <p>By default, <code>write_parquet</code> will raise an error if the directory already exists and contains parquet files. This is to avoid overwriting existing annotations. To allow overwriting existing files, use <code>overwrite=True</code>.</p>"},{"location":"reference/edsnlp/data/parquet/#edsnlp.data.parquet.write_parquet--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a Stream).</p> <p> TYPE: <code>Union[Any, Stream]</code> </p> <code>path</code> <p>Path to the directory containing the parquet files (will recursively look for files in subdirectories). Supports any filesystem supported by pyarrow.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>batch_size</code> <p>The maximum number of documents to write in each parquet file.</p> <p> TYPE: <code>Optional[Union[int, str]]</code> DEFAULT: <code>None</code> </p> <code>batch_by</code> <p>The method to batch the documents. If \"docs\", the batch size is the number of documents. If \"fragment\", each batch corresponds to a parquet file fragment from the input data.</p> <p> TYPE: <code>BatchBy</code> DEFAULT: <code>None</code> </p> <code>write_in_worker</code> <p>In multiprocessing or spark mode, whether to batch and write the documents in the workers or in the main process.</p> <p>For instance, a worker may read the 1st, 3rd, 5th, ... documents, while another reads the 2nd, 4th, 6th, ... documents.</p> <p>If <code>write_in_worker</code> is False, <code>deterministic</code> is True (default) and no operation adds or remove document from the stream (e.g., no <code>map_batches</code>), the original order of the documents will be recovered in the main process, and batching there can produce fragments that respect the original order.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>overwrite</code> <p>Whether to overwrite existing directories.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>pyarrow_write_kwargs</code> <p>Additional keyword arguments to pass to the <code>pyarrow.parquet.write_to_dataset</code></p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>execute</code> <p>Whether to execute the writing operation immediately or to return a stream</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before writing them as Parquet rows. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/polars/","title":"<code>edsnlp.data.polars</code>","text":""},{"location":"reference/edsnlp/data/polars/#edsnlp.data.polars.from_polars","title":"<code>from_polars</code>","text":"<p>The PolarsReader (or <code>edsnlp.data.from_polars</code>) handles reading from a table and yields documents. At the moment, only entities and attributes are loaded. Relations and events are not supported.</p>"},{"location":"reference/edsnlp/data/polars/#edsnlp.data.polars.from_polars--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.from_polars(df, nlp=nlp, converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.from_polars</code> returns a Stream. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.from_polars(df, converter=\"omop\"))\n</code></pre>"},{"location":"reference/edsnlp/data/polars/#edsnlp.data.polars.from_polars--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>Polars object</p> <p> TYPE: <code>Union[DataFrame, LazyFrame]</code> </p> <code>shuffle</code> <p>Whether to shuffle the data. If \"dataset\", the whole dataset will be shuffled at the beginning (of every epoch if looping).</p> <p> TYPE: <code>Literal['dataset', False]</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>loop</code> <p>Whether to loop over the data indefinitely.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converters to use to convert the rows of the DataFrame (represented as dicts) to Doc objects. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[AsList[Union[str, Callable]]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/data/polars/#edsnlp.data.polars.to_polars","title":"<code>to_polars</code>","text":"<p><code>edsnlp.data.to_polars</code> writes a list of documents as a polars dataframe.</p>"},{"location":"reference/edsnlp/data/polars/#edsnlp.data.polars.to_polars--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.to_polars([doc], converter=\"omop\")\n</code></pre>"},{"location":"reference/edsnlp/data/polars/#edsnlp.data.polars.to_polars--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a Stream).</p> <p> TYPE: <code>Union[Any, Stream]</code> </p> <code>dtypes</code> <p>Dictionary of column names to dtypes. This is passed to the schema parameter of <code>pl.from_dicts</code>.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before storing them in the dataframe. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>execute</code> <p>Whether to execute the writing operation immediately or to return a stream</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/spark/","title":"<code>edsnlp.data.spark</code>","text":""},{"location":"reference/edsnlp/data/spark/#edsnlp.data.spark.from_spark","title":"<code>from_spark</code>","text":"<p>The SparkReader (or <code>edsnlp.data.from_spark</code>) reads a pyspark (or koalas) DataFrame and yields documents. At the moment, only entities and span attributes are loaded.</p>"},{"location":"reference/edsnlp/data/spark/#edsnlp.data.spark.from_spark--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.from_spark(note_df, converter=\"omop\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.from_spark</code> returns a Stream To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list</p> <pre><code>docs = list(edsnlp.data.from_spark(note_df, converter=\"omop\"))\n</code></pre>"},{"location":"reference/edsnlp/data/spark/#edsnlp.data.spark.from_spark--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The DataFrame to read.</p> <p> </p> <code>shuffle</code> <p>Whether to shuffle the data. If \"dataset\", the whole dataset will be shuffled before starting iterating on it (at the start of every epoch if looping).</p> <p> TYPE: <code>Literal['dataset', False]</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>loop</code> <p>Whether to loop over the data indefinitely.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converters to use to convert the rows of the DataFrame to Doc objects. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[AsList[Union[str, Callable]]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/data/spark/#edsnlp.data.spark.to_spark","title":"<code>to_spark</code>","text":"<p><code>edsnlp.data.to_spark</code> converts a list of documents into a Spark DataFrame, usually one row per document, unless the converter returns a list in which case each entry of the resulting list will be stored in its own row.</p>"},{"location":"reference/edsnlp/data/spark/#edsnlp.data.spark.to_spark--example","title":"Example","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.covid())\n\nnote_df = sql('''\n    select note_id, note_text from note\n    where note_text is not null\n    limit 500\n''')\n\ndocs = edsnlp.data.from_spark(note_df, converter=\"omop\")\n\ndocs = nlp.pipe(docs)\n\nres = edsnlp.data.to_spark(docs, converter=\"omop\")\n\nres.show()\n</code></pre> <p>Mac OS X</p> <p>If you are using Mac OS X, you may need to set the following environment variable (see this thread) to run pyspark:</p> <pre><code>import os\nos.environ[\"OBJC_DISABLE_INITIALIZE_FORK_SAFETY\"] = \"YES\"\n</code></pre>"},{"location":"reference/edsnlp/data/spark/#edsnlp.data.spark.to_spark--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a Stream).</p> <p> TYPE: <code>Union[Any, Stream]</code> </p> <code>dtypes</code> <p>The schema to use for the DataFrame.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>show_dtypes</code> <p>Whether to print the inferred schema (only if <code>dtypes</code> is None).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>execute</code> <p>Whether to execute the writing operation immediately or to return a stream</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects before storing them in the dataframe. These are documented on the Converters page.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the converter. These are documented on the Converters page.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/data/standoff/","title":"<code>edsnlp.data.standoff</code>","text":""},{"location":"reference/edsnlp/data/standoff/#edsnlp.data.standoff.parse_standoff_file","title":"<code>parse_standoff_file</code>","text":"<p>Load a brat file</p> <p>Adapted from https://github.com/percevalw/nlstruct/blob/master/nlstruct/datasets/brat.py</p>"},{"location":"reference/edsnlp/data/standoff/#edsnlp.data.standoff.parse_standoff_file--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path or glob path of the brat text file (.txt, not .ann)</p> <p> </p> <code>merge_spaced_fragments</code> <p>Merge fragments of an entity that was split by brat because it overlapped an end of line</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>fs</code> <p>Filesystem to use</p> <p> TYPE: <code>FileSystem</code> DEFAULT: <code>LOCAL_FS</code> </p> RETURNS DESCRIPTION <code>Iterator[Dict]</code>"},{"location":"reference/edsnlp/data/standoff/#edsnlp.data.standoff.read_standoff","title":"<code>read_standoff</code>","text":"<p>The BratReader (or <code>edsnlp.data.read_standoff</code>) reads a directory of BRAT files and yields documents. At the moment, only entities and attributes are loaded. Relations  and events are not supported.</p>"},{"location":"reference/edsnlp/data/standoff/#edsnlp.data.standoff.read_standoff--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\ndoc_iterator = edsnlp.data.read_standoff(\"path/to/brat/directory\")\nannotated_docs = nlp.pipe(doc_iterator)\n</code></pre> <p>Generator vs list</p> <p><code>edsnlp.data.read_standoff</code> returns a Stream. To iterate over the documents multiple times efficiently or to access them by index, you must convert it to a list :</p> <pre><code>docs = list(edsnlp.data.read_standoff(\"path/to/brat/directory\"))\n</code></pre> <p>True/False attributes</p> <p>Boolean values are not supported by the BRAT editor, and are stored as empty (key: empty value) if true, and not stored otherwise. This means that False values will not be assigned to attributes by default, which can be problematic when deciding if an entity is negated or not : is the entity not negated, or has the negation attribute not been annotated ?</p> <p>To avoid this issue, you can use the <code>bool_attributes</code> argument to specify which attributes should be considered as boolean when reading a BRAT dataset. These attributes will be assigned a value of <code>True</code> if they are present, and <code>False</code> otherwise.</p> <pre><code>doc_iterator = edsnlp.data.read_standoff(\n    \"path/to/brat/directory\",\n    span_attributes=[\"negation\", \"family\"],\n    bool_attributes=[\"negation\"],  # Missing values will be set to False\n)\n</code></pre>"},{"location":"reference/edsnlp/data/standoff/#edsnlp.data.standoff.read_standoff--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path to the directory containing the BRAT files (will recursively look for files in subdirectories).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>shuffle</code> <p>Whether to shuffle the data. If \"dataset\", the whole dataset will be shuffled before starting iterating on it (at the start of every epoch if looping).</p> <p> TYPE: <code>Literal['dataset', False]</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for shuffling.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>loop</code> <p>Whether to loop over the data indefinitely.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>nlp</code> <p>The pipeline object (optional and likely not needed, prefer to use the <code>tokenizer</code> directly argument instead).</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>tokenizer</code> <p>The tokenizer instance used to tokenize the documents. Likely not needed since by default it uses the current context tokenizer :</p> <ul> <li>the tokenizer of the next pipeline run by <code>.map_pipeline</code> in a   Stream.</li> <li>or the <code>eds</code> tokenizer by default.</li> </ul> <p> TYPE: <code>Optional[Tokenizer]</code> </p> <code>span_setter</code> <p>The span setter to use when setting the spans in the documents. Defaults to setting the spans in the <code>ents</code> attribute, and creates a new span group for each JSON entity label.</p> <p> TYPE: <code>SpanSetterArg</code> </p> <code>span_attributes</code> <p>Mapping from BRAT attributes to Span extensions (can be a list too). By default, all attributes are imported as Span extensions with the same name.</p> <p> TYPE: <code>Optional[AttributesMappingArg]</code> </p> <code>keep_raw_attribute_values</code> <p>Whether to keep the raw attribute values (as strings) or to convert them to Python objects (e.g. booleans).</p> <p> TYPE: <code>bool</code> </p> <code>default_attributes</code> <p>How to set attributes on spans for which no attribute value was found in the input format. This is especially useful for negation, or frequent attributes values (e.g. \"negated\" is often False, \"temporal\" is often \"present\"), that annotators may not want to annotate every time.</p> <p> TYPE: <code>AttributesMappingArg</code> </p> <code>notes_as_span_attribute</code> <p>If set, the AnnotatorNote annotations will be concatenated and stored in a span attribute with this name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>split_fragments</code> <p>Whether to split the fragments into separate spans or not. If set to False, the fragments will be concatenated into a single span.</p> <p> TYPE: <code>bool</code> </p> <code>keep_ipynb_checkpoints</code> <p>Whether to keep the files that are in the <code>.ipynb_checkpoints</code> directory.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>keep_txt_only_docs</code> <p>Whether to keep the <code>.txt</code> files that do not have corresponding <code>.ann</code> files.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects.</p> <p> TYPE: <code>Optional[AsList[Union[str, Callable]]]</code> DEFAULT: <code>['standoff']</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Stream</code>"},{"location":"reference/edsnlp/data/standoff/#edsnlp.data.standoff.write_standoff","title":"<code>write_standoff</code>","text":"<p><code>edsnlp.data.write_standoff</code> writes a list of documents using the BRAT/Standoff format in a directory. The BRAT files will be named after the <code>note_id</code> attribute of the documents, and subdirectories will be created if the name contains <code>/</code> characters.</p>"},{"location":"reference/edsnlp/data/standoff/#edsnlp.data.standoff.write_standoff--example","title":"Example","text":"<pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(...)\n\ndoc = nlp(\"My document with entities\")\n\nedsnlp.data.write_standoff([doc], \"path/to/brat/directory\")\n</code></pre> <p>Overwriting files</p> <p>By default, <code>write_standoff</code> will raise an error if the directory already exists and contains files with <code>.a*</code> or <code>.txt</code> suffixes. This is to avoid overwriting existing annotations. To allow overwriting existing files, use <code>overwrite=True</code>.</p>"},{"location":"reference/edsnlp/data/standoff/#edsnlp.data.standoff.write_standoff--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The data to write (either a list of documents or a Stream).</p> <p> TYPE: <code>Union[Any, Stream]</code> </p> <code>path</code> <p>Path to the directory containing the BRAT files (will recursively look for files in subdirectories).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>span_getter</code> <p>The span getter to use when listing the spans that will be exported as BRAT entities. Defaults to getting the spans in the <code>ents</code> attribute.</p> <p> </p> <code>span_attributes</code> <p>Mapping from BRAT attributes to Span extension. By default, no attribute will be exported.</p> <p> </p> <code>overwrite</code> <p>Whether to overwrite existing directories.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>filesystem</code> <p>The filesystem to use to write the files. If None, the filesystem will be inferred from the path (e.g. <code>s3://</code> will use S3).</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>execute</code> <p>Whether to execute the writing operation immediately or to return a stream</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>converter</code> <p>Converter to use to convert the documents to dictionary objects. Defaults to the \"standoff\" format converter.</p> <p> TYPE: <code>Optional[Union[str, Callable]]</code> DEFAULT: <code>'standoff'</code> </p>"},{"location":"reference/edsnlp/evaluate/","title":"<code>edsnlp.evaluate</code>","text":""},{"location":"reference/edsnlp/evaluate/#edsnlp.evaluate.evaluate","title":"<code>evaluate</code>","text":"<p>Evaluate a model on a dataset. This function can be called from the command line or from a script.</p> <p>By default, the model is loaded from <code>artifacts/model-last</code>, and the results are stored both in <code>artifacts/test_metrics.json</code> and in the model's <code>artifacts/model-last/meta.json</code> file.</p>"},{"location":"reference/edsnlp/evaluate/#edsnlp.evaluate.evaluate--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>A function that generates samples for evaluation</p> <p> TYPE: <code>SampleGenerator</code> </p> <code>model_path</code> <p>The path to the model to evaluate</p> <p> TYPE: <code>Path</code> DEFAULT: <code>'artifacts/model-last'</code> </p> <code>scorer</code> <p>A function that computes metrics on the model. You can also pass a dict:</p> <pre><code>scorer = {\n    \"ner\": NerExactMetric(...),\n    ...\n}\n</code></pre> <p> TYPE: <code>GenericScorer</code> </p> <code>task_metadata</code> <p>Metadata about the evaluation task. This will be stored in the model's meta.json file and is primarily meant to be parsed by the Hugging Face Hub, e.g., but also to remove previous results for the same dataset.</p> <pre><code>task_metadata={\n    \"task\": {\"type\": \"token-classification\"},\n    \"dataset\": {\n        \"name\": \"my_dataset\",\n        \"type\": \"private\",\n    },\n}\n</code></pre> <p> TYPE: <code>Dict</code> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/extensions/","title":"<code>edsnlp.extensions</code>","text":""},{"location":"reference/edsnlp/language/","title":"<code>edsnlp.language</code>","text":""},{"location":"reference/edsnlp/language/#edsnlp.language.EDSDefaults","title":"<code>EDSDefaults</code>","text":"<p>           Bases: <code>FrenchDefaults</code></p> <p>Defaults for the EDSLanguage class Mostly identical to the FrenchDefaults, but without tokenization info</p>"},{"location":"reference/edsnlp/language/#edsnlp.language.EDSLanguage","title":"<code>EDSLanguage</code>","text":"<p>           Bases: <code>French</code></p> <p>French clinical language. It is shipped with the <code>EDSTokenizer</code> tokenizer that better handles tokenization for French clinical documents</p>"},{"location":"reference/edsnlp/language/#edsnlp.language.EDSTokenizer","title":"<code>EDSTokenizer</code>","text":"<p>           Bases: <code>Tokenizer</code></p> <pre><code>    Tokenizer class for French clinical documents.\n    It better handles tokenization around:\n    - numbers: \"ACR5\" -&gt; [\"ACR\", \"5\"] instead of [\"ACR5\"]\n    - newlines: \"\n</code></pre> <p>\" -&gt; [\" \", \" \", \" \"] instead of [\"</p> <p>\"]         and should be around 5-6 times faster than its standard French counterpart.         Parameters         ----------         vocab: Vocab             The spacy vocabulary</p>"},{"location":"reference/edsnlp/language/#edsnlp.language.EDSTokenizer.__call__","title":"<code>__call__</code>","text":"<p>Tokenizes the text using the EDSTokenizer</p>"},{"location":"reference/edsnlp/language/#edsnlp.language.EDSTokenizer.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>text</code> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Doc</code>"},{"location":"reference/edsnlp/language/#edsnlp.language.create_eds_tokenizer","title":"<code>create_eds_tokenizer</code>","text":"<p>Creates a factory that returns new EDSTokenizer instances</p> RETURNS DESCRIPTION <code>EDSTokenizer</code>"},{"location":"reference/edsnlp/matchers/","title":"<code>edsnlp.matchers</code>","text":""},{"location":"reference/edsnlp/matchers/regex/","title":"<code>edsnlp.matchers.regex</code>","text":""},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher","title":"<code>RegexMatcher</code>","text":"<p>           Bases: <code>object</code></p> <p>Simple RegExp matcher.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>alignment_mode</code> <p>How spans should be aligned with tokens. Possible values are <code>strict</code> (character indices must be aligned with token boundaries), \"contract\" (span of all tokens completely within the character span), \"expand\" (span of all tokens at least partially covered by the character span). Defaults to <code>expand</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'expand'</code> </p> <code>attr</code> <p>Default attribute to match on, by default \"TEXT\". Can be overiden in the <code>add</code> method.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'TEXT'</code> </p> <code>flags</code> <p>Additional flags provided to the <code>re</code> module. Can be overiden in the <code>add</code> method.</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>ignore_excluded</code> <p>Whether to skip exclusions</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p>You won't be able to match on newlines if this is enabled and the \"spaces\"/\"newline\" option of <code>eds.normalizer</code> is enabled (by default).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>span_from_group</code> <p>If set to <code>False</code>, will create spans basede on the regex's full match. If set to <code>True</code>, will use the first matching capturing group as a span (and fall back to using the full match if no capturing group is matching)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.build_patterns","title":"<code>build_patterns</code>","text":"<p>Build patterns and adds them for matching. Helper function for pipelines using this matcher.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.build_patterns--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>regex</code> <p>Dictionary of label/terms, or label/dictionary of terms/attribute.</p> <p> TYPE: <code>Patterns</code> </p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.add","title":"<code>add</code>","text":"<p>Add a pattern to the registry.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.add--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>key</code> <p>Key of the new/updated pattern.</p> <p> TYPE: <code>str</code> </p> <code>patterns</code> <p>List of patterns to add.</p> <p> TYPE: <code>List[str]</code> </p> <code>attr</code> <p>Attribute to use for matching. By default, uses the <code>default_attr</code> attribute</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p>You won't be able to match on newlines if this is enabled and the \"spaces\"/\"newline\" option of <code>eds.normalizer</code> is enabled (by default).</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <p>alignment_mode : Optional[str]     Overwrite alignment mode.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.remove","title":"<code>remove</code>","text":"<p>Remove a pattern for the registry.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.remove--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>key</code> <p>key of the pattern to remove.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the key is not present in the registered patterns.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.match","title":"<code>match</code>","text":"<p>Iterates on the matches.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.match--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p>spaCy Doc or Span object to match on.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> YIELDS DESCRIPTION <code>span</code> <p>A match.</p> <p> TYPE:: <code>Tuple[Span, Match]</code> </p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.match_with_groupdict_as_spans","title":"<code>match_with_groupdict_as_spans</code>","text":"<p>Iterates on the matches.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.match_with_groupdict_as_spans--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p>spaCy Doc or Span object to match on.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> YIELDS DESCRIPTION <code>span</code> <p>A match.</p> <p> TYPE:: <code>Tuple[Span, Dict[str, Span]]</code> </p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.__call__","title":"<code>__call__</code>","text":"<p>Performs matching. Yields matches.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p>spaCy Doc or Span object.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>as_spans</code> <p>Returns matches as spans.</p> <p> DEFAULT: <code>False</code> </p> YIELDS DESCRIPTION <code>span</code> <p>A match.</p> <p> TYPE:: <code>Union[Span, Tuple[Span, Dict[str, Any]]]</code> </p> <code>groupdict</code> <p>Additional information coming from the named patterns in the regular expression.</p> <p> TYPE:: <code>Union[Span, Tuple[Span, Dict[str, Any]]]</code> </p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.spans_generator","title":"<code>spans_generator</code>","text":"<p>Iterates over every group, and then yields the full match</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.spans_generator--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>match</code> <p>A match object</p> <p> TYPE: <code>Match</code> </p> YIELDS DESCRIPTION <code>Tuple[int, int]</code> <p>A tuple containing the start and end of the group or match</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.span_from_match","title":"<code>span_from_match</code>","text":"<p>Return the span (as a (start, end) tuple) of the first matching group. If <code>span_from_group=True</code>, returns the full match instead.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.span_from_match--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>match</code> <p>The Match object</p> <p> TYPE: <code>Match</code> </p> <code>span_from_group</code> <p>Whether to work on groups or on the full match</p> <p> TYPE: <code>bool</code> </p> RETURNS DESCRIPTION <code>Tuple[int, int]</code> <p>A tuple containing the start and end of the group or match</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.create_span","title":"<code>create_span</code>","text":"<p>spaCy only allows strict alignment mode for char_span on Spans. This method circumvents this.</p>"},{"location":"reference/edsnlp/matchers/regex/#edsnlp.matchers.regex.create_span--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p><code>Doc</code> or <code>Span</code>.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>start_char</code> <p>Character index within the Doc-like object.</p> <p> TYPE: <code>int</code> </p> <code>end_char</code> <p>Character index of the end, within the Doc-like object.</p> <p> TYPE: <code>int</code> </p> <code>key</code> <p>The key used to match.</p> <p> TYPE: <code>str</code> </p> <code>alignment_mode</code> <p>The alignment mode.</p> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens.</p> <p> TYPE: <code>bool</code> </p> RETURNS DESCRIPTION <code>span</code> <p>A span matched on the Doc-like object.</p> <p> TYPE: <code>Optional[Span]</code> </p>"},{"location":"reference/edsnlp/matchers/simstring/","title":"<code>edsnlp.matchers.simstring</code>","text":""},{"location":"reference/edsnlp/matchers/simstring/#edsnlp.matchers.simstring.SimstringWriter","title":"<code>SimstringWriter</code>","text":"<p>A context class to write a simstring database</p>"},{"location":"reference/edsnlp/matchers/simstring/#edsnlp.matchers.simstring.SimstringWriter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>Path to database</p> <p> TYPE: <code>Union[str, Path]</code> </p>"},{"location":"reference/edsnlp/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher","title":"<code>SimstringMatcher</code>","text":"<p>PhraseMatcher that allows to skip excluded tokens. Heavily inspired by https://github.com/Georgetown-IR-Lab/QuickUMLS</p>"},{"location":"reference/edsnlp/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>vocab</code> <p>spaCy vocabulary to match on.</p> <p> TYPE: <code>Vocab</code> </p> <code>path</code> <p>Path where we will store the precomputed patterns</p> <p> TYPE: <code>Optional[Union[Path, str]]</code> DEFAULT: <code>None</code> </p> <code>measure</code> <p>Name of the similarity measure. One of [jaccard, dice, overlap, cosine]</p> <p> TYPE: <code>SimilarityMeasure</code> DEFAULT: <code>dice</code> </p> <code>windows</code> <p>Maximum number of words in a candidate span</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>threshold</code> <p>Minimum similarity value to match a concept's synonym</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.75</code> </p> <code>ignore_excluded</code> <p>Whether to exclude tokens that have an EXCLUDED tag, by default False</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to exclude tokens that have a \"SPACE\" tag, by default False</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>Default attribute to match on, by default \"TEXT\". Can be overridden in the <code>add</code> method. To match on a custom attribute, prepend the attribute name with <code>_</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p>"},{"location":"reference/edsnlp/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.build_patterns","title":"<code>build_patterns</code>","text":"<p>Build patterns and adds them for matching.</p>"},{"location":"reference/edsnlp/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.build_patterns--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The instance of the spaCy language class.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>terms</code> <p>Dictionary of label/terms, or label/dictionary of terms/attribute.</p> <p> TYPE: <code>Patterns</code> </p> <code>progress</code> <p>Whether to track progress when preprocessing terms</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/matchers/simstring/#edsnlp.matchers.simstring.get_text_and_offsets","title":"<code>get_text_and_offsets</code>  <code>cached</code>","text":"<p>Align different representations of a <code>Doc</code> or <code>Span</code> object.</p>"},{"location":"reference/edsnlp/matchers/simstring/#edsnlp.matchers.simstring.get_text_and_offsets--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p>spaCy <code>Doc</code> or <code>Span</code> object</p> <p> TYPE: <code>Doc</code> </p> <code>attr</code> <p>Attribute to use, by default <code>\"TEXT\"</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>'TEXT'</code> </p> <code>ignore_excluded</code> <p>Whether to remove excluded tokens, by default True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ignore_space_tokens</code> <p>Whether to remove space tokens, by default False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Tuple[str, List[Tuple[int, int, int, int]]]</code> <p>The new clean text and offset tuples for each word giving the begin char indice of the word in the new text, the end char indice of its preceding word and the begin / end indices of the word in the original document</p>"},{"location":"reference/edsnlp/matchers/utils/","title":"<code>edsnlp.matchers.utils</code>","text":""},{"location":"reference/edsnlp/matchers/utils/offset/","title":"<code>edsnlp.matchers.utils.offset</code>","text":""},{"location":"reference/edsnlp/matchers/utils/text/","title":"<code>edsnlp.matchers.utils.text</code>","text":""},{"location":"reference/edsnlp/metrics/","title":"<code>edsnlp.metrics</code>","text":""},{"location":"reference/edsnlp/metrics/dep_parsing/","title":"<code>edsnlp.metrics.dep_parsing</code>","text":""},{"location":"reference/edsnlp/metrics/dep_parsing/#edsnlp.metrics.dep_parsing.dependency_parsing_metric","title":"<code>dependency_parsing_metric</code>","text":"<p>Compute the UAS and LAS scores for dependency parsing.</p>"},{"location":"reference/edsnlp/metrics/dep_parsing/#edsnlp.metrics.dep_parsing.dependency_parsing_metric--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>examples</code> <p>The examples to score, either a tuple of (golds, preds) or a list of spacy.training.Example objects</p> <p> TYPE: <code>Examples</code> </p> <code>filter_expr</code> <p>The filter expression to use to filter the documents</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dict[str, float]</code>"},{"location":"reference/edsnlp/metrics/ner/","title":"<code>edsnlp.metrics.ner</code>","text":"<p>We provide several metrics to evaluate the performance of Named Entity Recognition (NER) components. Let's look at an example and see how they differ. We'll use the following two documents: a reference document (ref) and a document with predicted entities (pred).</p> <p>pred</p> <p>ref</p> <p>La patiente a une fi\u00e8vre aig\u00fce</p> <p>La patiente a une fi\u00e8vre aig\u00fce.</p> <p>Let's create matching documents in EDS-NLP using the following code snippet:</p> <pre><code>from edsnlp.data.converters import MarkupToDocConverter\n\nconv = MarkupToDocConverter(preset=\"md\", span_setter=\"entities\")\n\npred = conv(\"[La](PER) [patiente](PER) a une [fi\u00e8vre aigu\u00eb](DIS).\")\nref = conv(\"La [patiente](PER) a [une fi\u00e8vre](DIS) aigu\u00eb.\")\n</code></pre>"},{"location":"reference/edsnlp/metrics/ner/#edsnlp.metrics.ner.NerExactMetric","title":"<code>NerExactMetric</code>","text":"<p>           Bases: <code>NerMetric</code></p> <p>The <code>eds.ner_exact</code> metric scores the extracted entities (that may be overlapping or nested) by looking in the spans returned by a given SpanGetter object and comparing predicted spans to gold spans for exact boundary and label matches.</p> <p>Let's view these elements as collections of (span \u2192 label) and count how many of the predicted spans match the gold spans exactly (and vice versa):</p> <p>pred</p> <p>ref</p> <p>La patiente fi\u00e8vre aigu\u00eb</p> <p>patiente une fi\u00e8vre </p> <p>Precision, Recall and F1 (micro-average and per\u2010label) are computed as follows:</p> <ul> <li>Precision: <code>p = |matched items of pred| / |pred|</code></li> <li>Recall: <code>r = |matched items of ref| / |ref|</code></li> <li>F1: <code>f = 2 / (1/p + 1/f)</code></li> </ul>"},{"location":"reference/edsnlp/metrics/ner/#edsnlp.metrics.ner.NerExactMetric--examples","title":"Examples","text":"<pre><code>from edsnlp.metrics.ner import NerExactMetric\n\nmetric = NerExactMetric(span_getter=conv.span_setter, micro_key=\"micro\")\nmetric([ref], [pred])\n# Out: {\n#   'micro': {'f': 0.4, 'p': 0.33, 'r': 0.5, 'tp': 1, 'support': 2, 'positives': 3},\n#   'PER': {'f': 0.67, 'p': 0.5, 'r': 1, 'tp': 1, 'support': 1, 'positives': 2},\n#   'DIS': {'f': 0.0, 'p': 0.0, 'r': 0.0, 'tp': 0, 'support': 1, 'positives': 1},\n# }\n</code></pre>"},{"location":"reference/edsnlp/metrics/ner/#edsnlp.metrics.ner.NerExactMetric--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use to extract the spans from the document</p> <p> TYPE: <code>SpanGetterArg</code> </p> <code>micro_key</code> <p>The key to use to store the micro-averaged results for spans of all types</p> <p> TYPE: <code>str</code> DEFAULT: <code>'micro'</code> </p> <code>filter_expr</code> <p>The filter expression to use to filter the documents. Evaluated with <code>doc</code> as the variable.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/metrics/ner/#edsnlp.metrics.ner.NerTokenMetric","title":"<code>NerTokenMetric</code>","text":"<p>           Bases: <code>NerMetric</code></p> <p>The <code>eds.ner_token</code> metric scores the extracted entities that may be overlapping or nested by looking in <code>doc.ents</code>, and <code>doc.spans</code>, and comparing the predicted and gold entities at the token level.</p> <p>Assuming we use the <code>eds</code> (or <code>fr</code> or <code>en</code>) tokenizer, in the above example, there are 3 annotated tokens in the reference, and 4 annotated tokens in the prediction. Let's view these elements as sets of (token, label) and count how many of the predicted tokens match the gold tokens exactly (and vice versa):</p> <p>pred</p> <p>ref</p> <p>La patiente fi\u00e8vre aigu\u00eb</p> <p>patiente une fi\u00e8vre </p> <p>Precision, Recall and F1 (micro-average and per\u2010label) are computed as follows:</p> <ul> <li>Precision: <code>p = |matched items of pred| / |pred|</code></li> <li>Recall: <code>r = |matched items of ref| / |ref|</code></li> <li>F1: <code>f = 2 / (1/p + 1/f)</code></li> </ul>"},{"location":"reference/edsnlp/metrics/ner/#edsnlp.metrics.ner.NerTokenMetric--examples","title":"Examples","text":"<pre><code>from edsnlp.metrics.ner import NerTokenMetric\n\nmetric = NerTokenMetric(span_getter=conv.span_setter, micro_key=\"micro\")\nmetric([ref], [pred])\n# Out: {\n#   'micro': {'f': 0.57, 'p': 0.5, 'r': 0.67, 'tp': 2, 'support': 3, 'positives': 4},\n#   'PER': {'f': 0.67, 'p': 0.5, 'r': 1, 'tp': 1, 'support': 1, 'positives': 2},\n#   'DIS': {'f': 0.5, 'p': 0.5, 'r': 0.5, 'tp': 1, 'support': 2, 'positives': 2}\n# }\n</code></pre>"},{"location":"reference/edsnlp/metrics/ner/#edsnlp.metrics.ner.NerTokenMetric--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use to extract the spans from the document</p> <p> TYPE: <code>SpanGetterArg</code> </p> <code>micro_key</code> <p>The key to use to store the micro-averaged results for spans of all types</p> <p> TYPE: <code>str</code> DEFAULT: <code>'micro'</code> </p> <code>filter_expr</code> <p>The filter expression to use to filter the documents. Will be evaluated with <code>doc</code> as the variable name, so you can use <code>doc.ents</code>, <code>doc.spans</code>, etc.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/metrics/ner/#edsnlp.metrics.ner.NerOverlapMetric","title":"<code>NerOverlapMetric</code>","text":"<p>           Bases: <code>NerMetric</code></p> <p>The <code>eds.ner_overlap</code> metric scores the extracted entities that may be overlapping or nested by looking in the spans returned by a given SpanGetter object and counting a prediction as correct if it overlaps by at least the given Dice\u2010coefficient threshold with a gold span of the same label.</p> <p>This metric is useful for evaluating NER systems where the exact boundaries do not matter too much, but the presence of the entity at the same spot is important. For instance, you may not want to penalize a system that forgets determiners if the rest of the entity is correctly identified.</p> <p>Let's view these elements as sets of (span \u2192 label) and count how many of the predicted spans match the gold spans by at least the given Dice coefficient (and vice versa):</p> <p>pred</p> <p>ref</p> <p>La patiente fi\u00e8vre aigu\u00eb</p> <p>patiente une fi\u00e8vre </p> <p>Precision, Recall and F1 (micro-average and per\u2010label) are computed as follows:</p> <ul> <li>Precision: <code>p = |matched items of pred| / |pred|</code></li> <li>Recall: <code>r = |matched items of ref| / |ref|</code></li> <li>F1: <code>f = 2 / (1/p + 1/f)</code></li> </ul> <p>Overlap threshold</p> <p>The threshold is the minimum Dice coefficient to consider two spans as overlapping. Setting it to 1.0 will yield the same results as the <code>eds.ner_exact</code> metric, while setting it to a near-zero value (e.g., like 1e-14) will match any two spans that share at least one token.</p>"},{"location":"reference/edsnlp/metrics/ner/#edsnlp.metrics.ner.NerOverlapMetric--examples","title":"Examples","text":"<pre><code>from edsnlp.metrics.ner import NerOverlapMetric\n\nmetric = NerOverlapMetric(\n    span_getter=conv.span_setter, micro_key=\"micro\", threshold=0.5\n)\nmetric([ref], [pred])\n# Out: {\n#   'micro': {'f': 0.8, 'p': 0.67, 'r': 1.0, 'tp': 2, 'support': 2, 'positives': 3},\n#   'PER': {'f': 0.67, 'p': 0.5, 'r': 1.0, 'tp': 1, 'support': 1, 'positives': 2},\n#   'DIS': {'f': 1.0, 'p': 1.0, 'r': 1.0, 'tp': 1, 'support': 1, 'positives': 1}\n# }\n</code></pre>"},{"location":"reference/edsnlp/metrics/ner/#edsnlp.metrics.ner.NerOverlapMetric--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to use to extract the spans from the document</p> <p> TYPE: <code>SpanGetterArg</code> </p> <code>micro_key</code> <p>The key to use to store the micro-averaged results for spans of all types</p> <p> TYPE: <code>str</code> DEFAULT: <code>'micro'</code> </p> <code>filter_expr</code> <p>The filter expression to use to filter the documents</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>threshold</code> <p>The threshold on the Dice coefficient to consider two spans as overlapping</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p>"},{"location":"reference/edsnlp/metrics/ner/#edsnlp.metrics.ner.dice","title":"<code>dice</code>","text":"<p>Compute the Dice coefficient between two spans</p>"},{"location":"reference/edsnlp/metrics/span_attribute/","title":"<code>edsnlp.metrics.span_attribute</code>","text":"<p>Metrics for Span Attribute Classification</p>"},{"location":"reference/edsnlp/metrics/span_attribute/#edsnlp.metrics.span_attribute--edsnlp.metrics.span_attribute.SpanAttributeMetric","title":"Span Attribute Classification Metrics","text":"<p>Several NLP tasks consist in classifying existing spans of text into multiple classes, such as the detection of negation, hypothesis or span linking.</p> <p>We provide a metric to evaluate the performance of such tasks,</p> <p>Let's look at an example:</p> <p>pred</p> <p>ref</p> <p>Le patient n'est pas fi\u00e8vreux, son p\u00e8re a du diab\u00e8te. Pas d'\u00e9volution du cancer.</p> <p>Le patient n'est pas fi\u00e8vreux, son p\u00e8re a du diab\u00e8te. Pas d'\u00e9volution du cancer.</p> <p>We can quickly create matching documents in EDS-NLP using the following code snippet:</p> <pre><code>from edsnlp.data.converters import MarkupToDocConverter\n\nconv = MarkupToDocConverter(preset=\"md\", span_setter=\"entities\")\n# Create a document with predicted attributes and a reference document\npred = conv(\n    \"Le patient n'est pas [fi\u00e8vreux](SYMP neg=true), \"\n    \"son p\u00e8re a [du diab\u00e8te](DIS neg=false carrier=PATIENT). \"\n    \"Pas d'\u00e9volution du [cancer](DIS neg=true carrier=PATIENT).\"\n)\nref = conv(\n    \"Le patient n'est pas [fi\u00e8vreux](SYMP neg=true), \"\n    \"son p\u00e8re a [du diab\u00e8te](DIS neg=false carrier=FATHER). \"\n    \"Pas d'\u00e9volution du [cancer](DIS neg=false carrier=PATIENT).\"\n)\n</code></pre>"},{"location":"reference/edsnlp/metrics/span_attribute/#edsnlp.metrics.span_attribute.SpanAttributeMetric","title":"<code>SpanAttributeMetric</code>","text":"<p>The <code>eds.span_attribute</code> metric evaluates span\u2010level attribute classification by comparing predicted and gold attribute values on the same set of spans. For each attribute you specify, it computes Precision, Recall, F1, number of true positives (tp), number of gold instances (support), number of predicted instances (positives), and the Average Precision (ap). A micro\u2010average over all attributes is also provided under <code>micro_key</code>.</p> <pre><code>from edsnlp.metrics.span_attribute import SpanAttributeMetric\n\nmetric = SpanAttributeMetric(\n    span_getter=conv.span_setter,\n    # Evaluated attributes\n    attributes={\n        \"neg\": True,  # 'neg' on every entity\n        \"carrier\": [\"DIS\"],  # 'carrier' only on 'DIS' entities\n    },\n    # Ignore these default values when counting matches\n    default_values={\n        \"neg\": False,\n    },\n    micro_key=\"micro\",\n)\n</code></pre> <p>Let's enumerate (span -&gt; attr = value) items in our documents. Only the items with matching span boundaries, attribute name, and value are counted as a true positives. For instance, with the predicted and reference spans of the example above:</p> <p>pred</p> <p>ref</p> <p>fi\u00e8vreux \u2192 neg = True du diab\u00e8te \u2192 neg = False du diab\u00e8te \u2192 carrier = PATIENT cancer \u2192 neg = True cancer \u2192 carrier = PATIENT</p> <p>fi\u00e8vreux \u2192 neg = True du diab\u00e8te \u2192 neg = False du diab\u00e8te \u2192 carrier = FATHER cancer \u2192 neg = False cancer \u2192 carrier = PATIENT</p> <p>Default values</p> <p>Note that there we don't count \"neg=False\" items, shown in grey in the table. In EDS-NLP, this is done by setting <code>defaults_values={\"neg\": False}</code> when creating the metric. This is quite common in classification tasks, where one of the values is both the most common and the \"default\" (hence the name of the parameter). Counting these values would likely skew the micro-average metrics towards the default value.</p> <p>Precision, Recall and F1 (micro-average and per\u2010label) are computed as follows:</p> <ul> <li>Precision: <code>p = |matched items of pred| / |pred|</code></li> <li>Recall: <code>r = |matched items of ref| / |ref|</code></li> <li>F1: <code>f = 2 / (1/p + 1/f)</code></li> </ul> <p>This yields the following metrics:</p> <pre><code>metric([ref], [pred])\n# Out: {\n#   'micro': {'f': 0.57, 'p': 0.5, 'r': 0.67, 'tp': 2, 'support': 3, 'positives': 4, 'ap': 0.17},\n#   'neg': {'f': 0.67, 'p': 0.5, 'r': 1, 'tp': 1, 'support': 1, 'positives': 2, 'ap': 0.0},\n#   'carrier': {'f': 0.5, 'p': 0.5, 'r': 0.5, 'tp': 1, 'support': 2, 'positives': 2, 'ap': 0.25},\n# }\n</code></pre>"},{"location":"reference/edsnlp/metrics/span_attribute/#edsnlp.metrics.span_attribute.SpanAttributeMetric--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>The span getter to extract spans from each <code>Doc</code>.</p> <p> TYPE: <code>SpanGetterArg</code> </p> <code>attributes</code> <p>Map each attribute name to <code>True</code> (evaluate on all spans) or a sequence of labels restricting which spans to test.</p> <p> TYPE: <code>Mapping[str, Union[bool, Sequence[str]]]</code> DEFAULT: <code>None</code> </p> <code>default_values</code> <p>Attribute values to omit from micro\u2010average counts (e.g., common negative or default labels).</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>include_falsy</code> <p>If <code>False</code>, ignore falsy values (e.g., <code>False</code>, <code>None</code>, <code>''</code>) in predictions or gold when computing metrics; if <code>True</code>, count them.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>micro_key</code> <p>Key under which to store the micro\u2010averaged results across all attributes.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'micro'</code> </p> <code>filter_expr</code> <p>A Python expression (using <code>doc</code>) to filter which examples are scored.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dict[str, Dict[str, float]]</code> <p>A dictionary mapping each attribute name (and the <code>micro_key</code>) to its metrics:</p> <ul> <li> <p><code>label</code> or micro_key :</p> <ul> <li><code>p</code> : precision</li> <li><code>r</code> : recall</li> <li><code>f</code> : F1 score</li> <li><code>tp</code> : true positive count</li> <li><code>support</code> : number of gold instances</li> <li><code>positives</code> : number of predicted instances</li> <li><code>ap</code> : average precision</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/metrics/span_attribute/#edsnlp.metrics.span_attribute.SpanAttributeMetric.__call__","title":"<code>__call__</code>","text":"<p>Compute the span attribute metrics for the given examples.</p>"},{"location":"reference/edsnlp/metrics/span_attribute/#edsnlp.metrics.span_attribute.SpanAttributeMetric.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>examples</code> <p>The examples to score, either a tuple of (golds, preds) or a list of spacy.training.Example objects</p> <p> TYPE: <code>Examples</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Dict[str, Dict[str, float]]</code> <p>The scores for the attributes</p>"},{"location":"reference/edsnlp/package/","title":"<code>edsnlp.package</code>","text":""},{"location":"reference/edsnlp/patch_spacy/","title":"<code>edsnlp.patch_spacy</code>","text":""},{"location":"reference/edsnlp/patch_spacy/#edsnlp.patch_spacy.factory","title":"<code>factory</code>  <code>classmethod</code>","text":"<p>Patched from spaCy to allow back dots in factory names (https://github.com/aphp/edsnlp/pull/152)</p> <p>Register a new pipeline component factory. Can be used as a decorator on a function or classmethod, or called as a function with the factory provided as the func keyword argument. To create a component and add it to the pipeline, you can use nlp.add_pipe(name).</p> <p>name (str): The name of the component factory. default_config (Dict[str, Any]): Default configuration, describing the     default values of the factory arguments. assigns (Iterable[str]): Doc/Token attributes assigned by this component,     e.g. \"token.ent_id\". Used for pipeline analysis. requires (Iterable[str]): Doc/Token attributes required by this component,     e.g. \"token.ent_id\". Used for pipeline analysis. retokenizes (bool): Whether the component changes the tokenization.     Used for pipeline analysis. default_score_weights (Dict[str, Optional[float]]): The scores to report during     training, and their default weight towards the final score used to     select the best model. Weights should sum to 1.0 per component and     will be combined and normalized for the whole pipeline. If None,     the score won't be shown in the logs or be weighted. func (Optional[Callable]): Factory function if not used as a decorator.</p> <p>DOCS: https://spacy.io/api/language#factory</p>"},{"location":"reference/edsnlp/patch_spacy/#edsnlp.patch_spacy.__init__","title":"<code>__init__</code>","text":"<p>EDS-NLP: Patched from spaCy do enable lazy-loading components</p> <p>Initialise a Language object.</p> <p>vocab (Vocab): A <code>Vocab</code> object. If <code>True</code>, a vocab is created. meta (dict): Custom meta data for the Language class. Is written to by     models to add model meta data. max_length (int): Maximum number of characters in a single text. The     current models may run out memory on extremely long texts, due to     large internal allocations. You should segment these texts into     meaningful units, e.g. paragraphs, subsections etc, before passing     them to spaCy. Default maximum length is 1,000,000 charas (1mb). As     a rule of thumb, if all pipeline components are enabled, spaCy's     default models currently requires roughly 1GB of temporary memory per     100,000 characters in one text. create_tokenizer (Callable): Function that takes the nlp object and     returns a tokenizer. batch_size (int): Default batch size for pipe and evaluate.</p> <p>DOCS: https://spacy.io/api/language#init</p>"},{"location":"reference/edsnlp/pipes/","title":"<code>edsnlp.pipes</code>","text":""},{"location":"reference/edsnlp/pipes/base/","title":"<code>edsnlp.pipes.base</code>","text":""},{"location":"reference/edsnlp/pipes/base/#edsnlp.pipes.base.BaseComponent","title":"<code>BaseComponent</code>","text":"<p>           Bases: <code>ABC</code></p> <p>The <code>BaseComponent</code> adds a <code>set_extensions</code> method, called at the creation of the object.</p> <p>It helps decouple the initialisation of the pipeline from the creation of extensions, and is particularly usefull when distributing EDSNLP on a cluster, since the serialisation mechanism imposes that the extensions be reset.</p>"},{"location":"reference/edsnlp/pipes/base/#edsnlp.pipes.base.BaseComponent.set_extensions","title":"<code>set_extensions</code>","text":"<p>Set <code>Doc</code>, <code>Span</code> and <code>Token</code> extensions.</p>"},{"location":"reference/edsnlp/pipes/core/","title":"<code>edsnlp.pipes.core</code>","text":""},{"location":"reference/edsnlp/pipes/core/contextual_matcher/","title":"<code>edsnlp.pipes.core.contextual_matcher</code>","text":""},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/","title":"<code>edsnlp.pipes.core.contextual_matcher.contextual_matcher</code>","text":""},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher","title":"<code>ContextualMatcher</code>","text":"<p>           Bases: <code>BaseNERComponent</code></p> <p>Allows additional matching in the surrounding context of the main match group, for qualification/filtering.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'contextual_matcher'</code> </p> <code>patterns</code> The patterns to match PARAMETER DESCRIPTION <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities in the doc.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> </p> <code>terms</code> <p>A single term or a list of terms (for exact matches)</p> <p> TYPE: <code>Union[RegexFlag, int]</code> </p> <code>exclude</code> One or more exclusion patterns PARAMETER DESCRIPTION <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> TYPE: <code>RegexFlag</code> </p> <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>window</code> <p>Context window to search for patterns around the anchor. Defaults to \"sent\" ( i.e. the sentence of the anchor span).</p> <p> TYPE: <code>Optional[ContextWindow]</code> </p> <p> TYPE: <code>AsList[SingleExcludeModel]</code> </p> <code>include</code> One or more inclusion patterns PARAMETER DESCRIPTION <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> TYPE: <code>RegexFlag</code> </p> <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>window</code> <p>Context window to search for patterns around the anchor. Defaults to \"sent\" ( i.e. the sentence of the anchor span).</p> <p> TYPE: <code>Optional[ContextWindow]</code> </p> <p> TYPE: <code>AsList[SingleIncludeModel]</code> </p> <code>assign</code> One or more assignment patterns PARAMETER DESCRIPTION <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities in the doc.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> TYPE: <code>RegexFlag</code> </p> <code>window</code> <p>Context window to search for patterns around the anchor. Defaults to \"sent\" ( i.e. the sentence of the anchor span).</p> <p> TYPE: <code>Optional[ContextWindow]</code> </p> <code>replace_entity</code> <p>If set to <code>True</code>, the match from the corresponding assign key will be used as entity, instead of the main match. See this paragraph</p> <p> TYPE: <code>Optional[bool]</code> </p> <code>reduce_mode</code> <p>Set how multiple assign matches are handled. See the documentation of the <code>reduce_mode</code> parameter</p> <p> TYPE: <code>Optional[Flags]</code> </p> <code>required</code> <p>If set to <code>True</code>, the assign key must match for the extraction to be kept. If it does not match, the extraction is discarded.</p> <p> TYPE: <code>Optional[str]</code> </p> <p> TYPE: <code>AsList[SingleAssignModel]</code> </p> <code>source</code> <p>A label describing the pattern</p> <p> TYPE: <code>str</code> </p> <p> TYPE: <code>FullConfig</code> </p> <code>assign_as_span</code> <p>Whether to store eventual extractions defined via the <code>assign</code> key as Spans or as string</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>alignment_mode</code> <p>Overwrite alignment mode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>expand</code> </p> <code>regex_flags</code> <p>RegExp flags to use when matching, filtering and assigning (See here)</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>include_assigned</code> <p>Whether to include (eventual) assign matches to the final entity</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>label_name</code> <p>Deprecated, use <code>label</code> instead. The label to assign to the matched entities</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>The label to assign to the matched entities</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.set_extensions","title":"<code>set_extensions</code>","text":"<p>Define the extensions used by the component</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.filter_one","title":"<code>filter_one</code>","text":"<p>Filter extracted entity based on the exclusion and inclusion filters of the configuration.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.filter_one--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span</code> <p>Span to filter</p> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>Optional[Span]</code> <p>None if the span was filtered, the span else</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.assign_one","title":"<code>assign_one</code>","text":"<p>Get additional information in the context of each entity. This function will populate two custom attributes:</p> <ul> <li><code>ent._.source</code></li> <li><code>ent._.assigned</code>, a dictionary with all retrieved information</li> </ul>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.assign_one--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span</code> <p>Span to enrich</p> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>Spans with additional information</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.process_one","title":"<code>process_one</code>","text":"<p>Processes one span, applying both the filters and the assignments</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.process_one--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span</code> <p>Span object</p> <p> TYPE: <code>Span</code> </p> <code>pattern</code> <p> TYPE: <code>SingleConfig</code> </p> YIELDS DESCRIPTION <code>span</code> <p>Filtered spans, with optional assignments</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.process","title":"<code>process</code>","text":"<p>Process the document, looking for named entities.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>List of detected spans.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.__call__","title":"<code>__call__</code>","text":"<p>Adds spans to document.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/contextual_matcher/#edsnlp.pipes.core.contextual_matcher.contextual_matcher.ContextualMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for extracted terms.</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/factory/","title":"<code>edsnlp.pipes.core.contextual_matcher.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/contextual_matcher/factory/#edsnlp.pipes.core.contextual_matcher.factory.create_component","title":"<code>create_component = registry.factory.register('eds.contextual_matcher', deprecated=['eds.contextual-matcher', 'contextual-matcher'])(ContextualMatcher)</code>  <code>module-attribute</code>","text":"<p>Allows additional matching in the surrounding context of the main match group, for qualification/filtering.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/factory/#edsnlp.pipes.core.contextual_matcher.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'contextual_matcher'</code> </p> <code>patterns</code> The patterns to match PARAMETER DESCRIPTION <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities in the doc.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> </p> <code>terms</code> <p>A single term or a list of terms (for exact matches)</p> <p> TYPE: <code>Union[RegexFlag, int]</code> </p> <code>exclude</code> One or more exclusion patterns PARAMETER DESCRIPTION <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> TYPE: <code>RegexFlag</code> </p> <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>window</code> <p>Context window to search for patterns around the anchor. Defaults to \"sent\" ( i.e. the sentence of the anchor span).</p> <p> TYPE: <code>Optional[ContextWindow]</code> </p> <p> TYPE: <code>AsList[SingleExcludeModel]</code> </p> <code>include</code> One or more inclusion patterns PARAMETER DESCRIPTION <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> TYPE: <code>RegexFlag</code> </p> <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>window</code> <p>Context window to search for patterns around the anchor. Defaults to \"sent\" ( i.e. the sentence of the anchor span).</p> <p> TYPE: <code>Optional[ContextWindow]</code> </p> <p> TYPE: <code>AsList[SingleIncludeModel]</code> </p> <code>assign</code> One or more assignment patterns PARAMETER DESCRIPTION <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities in the doc.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> TYPE: <code>RegexFlag</code> </p> <code>window</code> <p>Context window to search for patterns around the anchor. Defaults to \"sent\" ( i.e. the sentence of the anchor span).</p> <p> TYPE: <code>Optional[ContextWindow]</code> </p> <code>replace_entity</code> <p>If set to <code>True</code>, the match from the corresponding assign key will be used as entity, instead of the main match. See this paragraph</p> <p> TYPE: <code>Optional[bool]</code> </p> <code>reduce_mode</code> <p>Set how multiple assign matches are handled. See the documentation of the <code>reduce_mode</code> parameter</p> <p> TYPE: <code>Optional[Flags]</code> </p> <code>required</code> <p>If set to <code>True</code>, the assign key must match for the extraction to be kept. If it does not match, the extraction is discarded.</p> <p> TYPE: <code>Optional[str]</code> </p> <p> TYPE: <code>AsList[SingleAssignModel]</code> </p> <code>source</code> <p>A label describing the pattern</p> <p> TYPE: <code>str</code> </p> <p> TYPE: <code>FullConfig</code> </p> <code>assign_as_span</code> <p>Whether to store eventual extractions defined via the <code>assign</code> key as Spans or as string</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>alignment_mode</code> <p>Overwrite alignment mode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>expand</code> </p> <code>regex_flags</code> <p>RegExp flags to use when matching, filtering and assigning (See here)</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>include_assigned</code> <p>Whether to include (eventual) assign matches to the final entity</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>label_name</code> <p>Deprecated, use <code>label</code> instead. The label to assign to the matched entities</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>The label to assign to the matched entities</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/models/","title":"<code>edsnlp.pipes.core.contextual_matcher.models</code>","text":""},{"location":"reference/edsnlp/pipes/core/contextual_matcher/models/#edsnlp.pipes.core.contextual_matcher.models.SingleExcludeModel","title":"<code>SingleExcludeModel</code>","text":"<p>           Bases: <code>BaseModel</code></p> <p>A dictionary to define exclusion rules. Exclusion rules are given as Regexes, and if a match is found in the surrounding context of an extraction, the extraction is removed. Note that only take a match into account if it is not inside the anchor span.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/models/#edsnlp.pipes.core.contextual_matcher.models.SingleExcludeModel--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> TYPE: <code>RegexFlag</code> </p> <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>window</code> <p>Context window to search for patterns around the anchor. Defaults to \"sent\" ( i.e. the sentence of the anchor span).</p> <p> TYPE: <code>Optional[ContextWindow]</code> </p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/models/#edsnlp.pipes.core.contextual_matcher.models.SingleIncludeModel","title":"<code>SingleIncludeModel</code>","text":"<p>           Bases: <code>BaseModel</code></p> <p>A dictionary to define inclusion rules. Inclusion rules are given as Regexes, and if a match isn't found in the surrounding context of an extraction, the extraction is removed. Note that only take a match into account if it is not inside the anchor span.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/models/#edsnlp.pipes.core.contextual_matcher.models.SingleIncludeModel--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> TYPE: <code>RegexFlag</code> </p> <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>window</code> <p>Context window to search for patterns around the anchor. Defaults to \"sent\" ( i.e. the sentence of the anchor span).</p> <p> TYPE: <code>Optional[ContextWindow]</code> </p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/models/#edsnlp.pipes.core.contextual_matcher.models.ExcludeModel","title":"<code>ExcludeModel</code>","text":"<p>           Bases: <code>AsList[SingleExcludeModel]</code></p> <p>A list of <code>SingleExcludeModel</code> objects. If a single config is passed, it will be automatically converted to a list of a single element.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/models/#edsnlp.pipes.core.contextual_matcher.models.IncludeModel","title":"<code>IncludeModel</code>","text":"<p>           Bases: <code>AsList[SingleIncludeModel]</code></p> <p>A list of <code>SingleIncludeModel</code> objects. If a single config is passed, it will be automatically converted to a list of a single element.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/models/#edsnlp.pipes.core.contextual_matcher.models.SingleAssignModel","title":"<code>SingleAssignModel</code>","text":"<p>           Bases: <code>BaseModel</code></p> <p>A dictionary to refine the extraction. Similarly to the <code>exclude</code> key, you can provide a dictionary to use on the context before and after the extraction.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/models/#edsnlp.pipes.core.contextual_matcher.models.SingleAssignModel--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities in the doc.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> TYPE: <code>RegexFlag</code> </p> <code>window</code> <p>Context window to search for patterns around the anchor. Defaults to \"sent\" ( i.e. the sentence of the anchor span).</p> <p> TYPE: <code>Optional[ContextWindow]</code> </p> <code>replace_entity</code> <p>If set to <code>True</code>, the match from the corresponding assign key will be used as entity, instead of the main match. See this paragraph</p> <p> TYPE: <code>Optional[bool]</code> </p> <code>reduce_mode</code> <p>Set how multiple assign matches are handled. See the documentation of the <code>reduce_mode</code> parameter</p> <p> TYPE: <code>Optional[Flags]</code> </p> <code>required</code> <p>If set to <code>True</code>, the assign key must match for the extraction to be kept. If it does not match, the extraction is discarded.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>name</code> <p>A name (string)</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/models/#edsnlp.pipes.core.contextual_matcher.models.AssignModel","title":"<code>AssignModel</code>","text":"<p>           Bases: <code>AsList[SingleAssignModel]</code></p> <p>A list of <code>SingleAssignModel</code> objects that should have at most one element with <code>replace_entity=True</code>. If a single config is passed, it will be automatically converted to a list of a single element.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/models/#edsnlp.pipes.core.contextual_matcher.models.SingleConfig","title":"<code>SingleConfig</code>","text":"<p>           Bases: <code>BaseModel</code></p> <p>A single configuration for the contextual matcher.</p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/models/#edsnlp.pipes.core.contextual_matcher.models.SingleConfig--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities in the doc.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> </p> <code>terms</code> <p>A single term or a list of terms (for exact matches)</p> <p> TYPE: <code>Union[RegexFlag, int]</code> </p> <code>exclude</code> One or more exclusion patterns PARAMETER DESCRIPTION <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> TYPE: <code>RegexFlag</code> </p> <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>window</code> <p>Context window to search for patterns around the anchor. Defaults to \"sent\" ( i.e. the sentence of the anchor span).</p> <p> TYPE: <code>Optional[ContextWindow]</code> </p> <p> TYPE: <code>AsList[SingleExcludeModel]</code> </p> <code>include</code> One or more inclusion patterns PARAMETER DESCRIPTION <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> TYPE: <code>RegexFlag</code> </p> <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>window</code> <p>Context window to search for patterns around the anchor. Defaults to \"sent\" ( i.e. the sentence of the anchor span).</p> <p> TYPE: <code>Optional[ContextWindow]</code> </p> <p> TYPE: <code>AsList[SingleIncludeModel]</code> </p> <code>assign</code> One or more assignment patterns PARAMETER DESCRIPTION <code>span_getter</code> <p>A span getter to pick the assigned spans from already extracted entities in the doc.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> </p> <code>regex</code> <p>A single Regex or a list of Regexes</p> <p> TYPE: <code>ListOrStr</code> </p> <code>regex_attr</code> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>regex_flags</code> <p>Regex flags</p> <p> TYPE: <code>RegexFlag</code> </p> <code>window</code> <p>Context window to search for patterns around the anchor. Defaults to \"sent\" ( i.e. the sentence of the anchor span).</p> <p> TYPE: <code>Optional[ContextWindow]</code> </p> <code>replace_entity</code> <p>If set to <code>True</code>, the match from the corresponding assign key will be used as entity, instead of the main match. See this paragraph</p> <p> TYPE: <code>Optional[bool]</code> </p> <code>reduce_mode</code> <p>Set how multiple assign matches are handled. See the documentation of the <code>reduce_mode</code> parameter</p> <p> TYPE: <code>Optional[Flags]</code> </p> <code>required</code> <p>If set to <code>True</code>, the assign key must match for the extraction to be kept. If it does not match, the extraction is discarded.</p> <p> TYPE: <code>Optional[str]</code> </p> <p> TYPE: <code>AsList[SingleAssignModel]</code> </p> <code>source</code> <p>A label describing the pattern</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/edsnlp/pipes/core/contextual_matcher/models/#edsnlp.pipes.core.contextual_matcher.models.FullConfig","title":"<code>FullConfig</code>","text":"<p>           Bases: <code>AsList[SingleConfig]</code></p> <p>A list of <code>SingleConfig</code> objects that should have distinct <code>source</code> fields. If a single config is passed, it will be automatically converted to a list of a single element.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/","title":"<code>edsnlp.pipes.core.endlines</code>","text":""},{"location":"reference/edsnlp/pipes/core/endlines/endlines/","title":"<code>edsnlp.pipes.core.endlines.endlines</code>","text":"<ol><li><p><p>Zweigenbaum P., Grouin C. and Lavergne T., 2016. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (End-of-line classification with no supervision). https://aclanthology.org/2016.jeptalnrecital-poster.7</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/core/endlines/endlines/#edsnlp.pipes.core.endlines.endlines.EndLinesMatcher","title":"<code>EndLinesMatcher</code>","text":"<p>           Bases: <code>GenericMatcher</code></p> <p>The <code>eds.endlines</code> component classifies newline characters as actual end of lines or mere spaces. In the latter case, the token is removed from the normalised document.</p> <p>Behind the scenes, it uses a <code>endlinesmodel</code> instance, which is an unsupervised algorithm based on the work of Zweigenbaum et al., 2016.</p> <p>Installation</p> <p>To use this component, you need to install the <code>scikit-learn</code> library.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/endlines/#edsnlp.pipes.core.endlines.endlines.EndLinesMatcher--training","title":"Training","text":"<pre><code>import edsnlp\nfrom edsnlp.pipes.core.endlines.model import EndLinesModel\n\nnlp = edsnlp.blank(\"eds\")\n\ntexts = [\n\"\"\"\nLe patient est arriv\u00e9 hier soir.\nIl est accompagn\u00e9 par son fils\n\nANTECEDENTS\nIl a fait une TS en 2010\nFumeur, il est arret\u00e9 il a 5 mois\nChirurgie de coeur en 2011\nCONCLUSION\nIl doit prendre\nle medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin\ndans 1 mois.\nDIAGNOSTIC :\nIl aime le fromage...\n\nAntecedents Familiaux:\n- 1. P\u00e8re avec diabete\n\"\"\",\n\"\"\"\nJ'aime le\nfromage...\n\"\"\",\n]\n\ndocs = list(nlp.pipe(texts))\n\n# Train and predict an EndLinesModel\nendlines = EndLinesModel(nlp=nlp)\n\ndf = endlines.fit_and_predict(docs)\ndf.head()\n\nPATH = \"/tmp/path_to_save\"\nendlines.save(PATH)\n</code></pre>"},{"location":"reference/edsnlp/pipes/core/endlines/endlines/#edsnlp.pipes.core.endlines.endlines.EndLinesMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\nfrom spacy.tokens import Span\nfrom spacy import displacy\n\nnlp = edsnlp.blank(\"eds\")\n\nPATH = \"/tmp/path_to_save\"\nnlp.add_pipe(eds.endlines(model_path=PATH))\n\ndocs = list(nlp.pipe(texts))\n\ndoc_exemple = docs[1]\n\ndoc_exemple.ents = tuple(\n    Span(doc_exemple, token.i, token.i + 1, \"excluded\")\n    for token in doc_exemple\n    if token.tag_ == \"EXCLUDED\"\n)\n\ndisplacy.render(doc_exemple, style=\"ent\", options={\"colors\": {\"space\": \"red\"}})\n</code></pre>"},{"location":"reference/edsnlp/pipes/core/endlines/endlines/#edsnlp.pipes.core.endlines.endlines.EndLinesMatcher--extensions","title":"Extensions","text":"<p>The <code>eds.endlines</code> pipe declares one extension, on both <code>Span</code> and <code>Token</code> objects. The <code>end_line</code> attribute is a boolean, set to <code>True</code> if the pipe predicts that the new line is an end line character. Otherwise, it is set to <code>False</code> if the new line is classified as a space.</p> <p>The pipe also sets the <code>excluded</code> custom attribute on newlines that are classified as spaces. It lets downstream matchers skip excluded tokens (see normalisation) for more detail.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/endlines/#edsnlp.pipes.core.endlines.endlines.EndLinesMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'endlines'</code> </p> <code>model_path</code> <p>Path to trained model. If None, it will use a default model</p> <p> TYPE: <code>Optional[Union[str, EndLinesModel]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/core/endlines/endlines/#edsnlp.pipes.core.endlines.endlines.EndLinesMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.endlines</code> pipe was developed by AP-HP's Data Science team based on the work of Zweigenbaum et al., 2016.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/endlines/#edsnlp.pipes.core.endlines.endlines.EndLinesMatcher.__call__","title":"<code>__call__</code>","text":"<p>Predict for each new line if it's an end of line or a space.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/endlines/#edsnlp.pipes.core.endlines.endlines.EndLinesMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p> TYPE: <code>spaCy Doc object, with each new line annotated</code> </p>"},{"location":"reference/edsnlp/pipes/core/endlines/factory/","title":"<code>edsnlp.pipes.core.endlines.factory</code>","text":"<ol><li><p><p>Zweigenbaum P., Grouin C. and Lavergne T., 2016. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (End-of-line classification with no supervision). https://aclanthology.org/2016.jeptalnrecital-poster.7</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/core/endlines/factory/#edsnlp.pipes.core.endlines.factory.create_component","title":"<code>create_component = registry.factory.register('eds.endlines', assigns=['doc.ents', 'doc.spans'], deprecated=['spaces'])(EndLinesMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.endlines</code> component classifies newline characters as actual end of lines or mere spaces. In the latter case, the token is removed from the normalised document.</p> <p>Behind the scenes, it uses a <code>endlinesmodel</code> instance, which is an unsupervised algorithm based on the work of Zweigenbaum et al., 2016.</p> <p>Installation</p> <p>To use this component, you need to install the <code>scikit-learn</code> library.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/factory/#edsnlp.pipes.core.endlines.factory.create_component--training","title":"Training","text":"<pre><code>import edsnlp\nfrom edsnlp.pipes.core.endlines.model import EndLinesModel\n\nnlp = edsnlp.blank(\"eds\")\n\ntexts = [\n\"\"\"\nLe patient est arriv\u00e9 hier soir.\nIl est accompagn\u00e9 par son fils\n\nANTECEDENTS\nIl a fait une TS en 2010\nFumeur, il est arret\u00e9 il a 5 mois\nChirurgie de coeur en 2011\nCONCLUSION\nIl doit prendre\nle medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin\ndans 1 mois.\nDIAGNOSTIC :\nIl aime le fromage...\n\nAntecedents Familiaux:\n- 1. P\u00e8re avec diabete\n\"\"\",\n\"\"\"\nJ'aime le\nfromage...\n\"\"\",\n]\n\ndocs = list(nlp.pipe(texts))\n\n# Train and predict an EndLinesModel\nendlines = EndLinesModel(nlp=nlp)\n\ndf = endlines.fit_and_predict(docs)\ndf.head()\n\nPATH = \"/tmp/path_to_save\"\nendlines.save(PATH)\n</code></pre>"},{"location":"reference/edsnlp/pipes/core/endlines/factory/#edsnlp.pipes.core.endlines.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\nfrom spacy.tokens import Span\nfrom spacy import displacy\n\nnlp = edsnlp.blank(\"eds\")\n\nPATH = \"/tmp/path_to_save\"\nnlp.add_pipe(eds.endlines(model_path=PATH))\n\ndocs = list(nlp.pipe(texts))\n\ndoc_exemple = docs[1]\n\ndoc_exemple.ents = tuple(\n    Span(doc_exemple, token.i, token.i + 1, \"excluded\")\n    for token in doc_exemple\n    if token.tag_ == \"EXCLUDED\"\n)\n\ndisplacy.render(doc_exemple, style=\"ent\", options={\"colors\": {\"space\": \"red\"}})\n</code></pre>"},{"location":"reference/edsnlp/pipes/core/endlines/factory/#edsnlp.pipes.core.endlines.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.endlines</code> pipe declares one extension, on both <code>Span</code> and <code>Token</code> objects. The <code>end_line</code> attribute is a boolean, set to <code>True</code> if the pipe predicts that the new line is an end line character. Otherwise, it is set to <code>False</code> if the new line is classified as a space.</p> <p>The pipe also sets the <code>excluded</code> custom attribute on newlines that are classified as spaces. It lets downstream matchers skip excluded tokens (see normalisation) for more detail.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/factory/#edsnlp.pipes.core.endlines.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'endlines'</code> </p> <code>model_path</code> <p>Path to trained model. If None, it will use a default model</p> <p> TYPE: <code>Optional[Union[str, EndLinesModel]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/core/endlines/factory/#edsnlp.pipes.core.endlines.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.endlines</code> pipe was developed by AP-HP's Data Science team based on the work of Zweigenbaum et al., 2016.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/functional/","title":"<code>edsnlp.pipes.core.endlines.functional</code>","text":""},{"location":"reference/edsnlp/pipes/core/endlines/functional/#edsnlp.pipes.core.endlines.functional.build_path","title":"<code>build_path</code>","text":"<p>Function to build an absolut path.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/functional/#edsnlp.pipes.core.endlines.functional.build_path--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>file</code> <code>relative_path</code> <p>relative path from the main file to the desired output</p> <p> </p> RETURNS DESCRIPTION <code>path</code> <p> TYPE: <code>absolute path</code> </p>"},{"location":"reference/edsnlp/pipes/core/endlines/model/","title":"<code>edsnlp.pipes.core.endlines.model</code>","text":""},{"location":"reference/edsnlp/pipes/core/endlines/model/#edsnlp.pipes.core.endlines.model.EndLinesModel","title":"<code>EndLinesModel</code>","text":"<p>Model to classify if an end line is a real one or it should be a space.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/model/#edsnlp.pipes.core.endlines.model.EndLinesModel--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>PipelineProtocol</code> </p>"},{"location":"reference/edsnlp/pipes/core/endlines/model/#edsnlp.pipes.core.endlines.model.EndLinesModel.fit_and_predict","title":"<code>fit_and_predict</code>","text":"<p>Fit the model and predict for the training data</p>"},{"location":"reference/edsnlp/pipes/core/endlines/model/#edsnlp.pipes.core.endlines.model.EndLinesModel.fit_and_predict--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>corpus</code> <p>An iterable of Documents</p> <p> TYPE: <code>Iterable[Doc]</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>one line by end_line prediction</p>"},{"location":"reference/edsnlp/pipes/core/endlines/model/#edsnlp.pipes.core.endlines.model.EndLinesModel.predict","title":"<code>predict</code>","text":"<p>Use the model for inference</p> <p>The df should have the following columns: <code>[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]</code></p>"},{"location":"reference/edsnlp/pipes/core/endlines/model/#edsnlp.pipes.core.endlines.model.EndLinesModel.predict--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>df</code> <p>The df should have the following columns: <code>[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]</code></p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>The result is added to the column <code>PREDICTED_END_LINE</code></p>"},{"location":"reference/edsnlp/pipes/core/endlines/model/#edsnlp.pipes.core.endlines.model.EndLinesModel.save","title":"<code>save</code>","text":"<p>Save a pickle of the model. It could be read by the pipeline later.</p>"},{"location":"reference/edsnlp/pipes/core/endlines/model/#edsnlp.pipes.core.endlines.model.EndLinesModel.save--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>path</code> <p>path to file .pkl, by default <code>base_model.pkl</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>'base_model.pkl'</code> </p>"},{"location":"reference/edsnlp/pipes/core/matcher/","title":"<code>edsnlp.pipes.core.matcher</code>","text":""},{"location":"reference/edsnlp/pipes/core/matcher/factory/","title":"<code>edsnlp.pipes.core.matcher.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/matcher/factory/#edsnlp.pipes.core.matcher.factory.create_component","title":"<code>create_component = registry.factory.register('eds.matcher', assigns=['doc.ents', 'doc.spans'], deprecated=['matcher'])(GenericMatcher)</code>  <code>module-attribute</code>","text":"<p>EDS-NLP simplifies the matching process by exposing a <code>eds.matcher</code> component that can match on terms or regular expressions.</p>"},{"location":"reference/edsnlp/pipes/core/matcher/factory/#edsnlp.pipes.core.matcher.factory.create_component--examples","title":"Examples","text":"<p>Let us redefine the pipeline :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],  # (1)\n    patient=\"patient\",  # (2)\n)\n\nregex = dict(\n    covid=r\"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\",  # (3)\n)\n\nnlp.add_pipe(\n    eds.matcher(\n        terms=terms,\n        regex=regex,\n        attr=\"LOWER\",\n        term_matcher=\"exact\",\n        term_matcher_config={},\n    ),\n)\n</code></pre> <ol> <li>Every key in the <code>terms</code> dictionary is mapped to a concept.</li> <li>The <code>eds.matcher</code> pipeline expects a list of expressions, or a single expression.</li> <li>We can also define regular expression patterns.</li> </ol> <p>This snippet is complete, and should run as is.</p> <p>Patterns, be they <code>terms</code> or <code>regex</code>, are defined as dictionaries where keys become  the label of the extracted entities. Dictionary values are either a single  expression or a list of expressions that match the concept.</p>"},{"location":"reference/edsnlp/pipes/core/matcher/factory/#edsnlp.pipes.core.matcher.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'matcher'</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p>You won't be able to match on newlines if this is enabled and the \"spaces\"/\"newline\" option of <code>eds.normalizer</code> is enabled (by default).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>exact</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>span_setter</code> <p>How to set the spans in the doc.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p>"},{"location":"reference/edsnlp/pipes/core/matcher/factory/#edsnlp.pipes.core.matcher.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.matcher</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/core/matcher/matcher/","title":"<code>edsnlp.pipes.core.matcher.matcher</code>","text":""},{"location":"reference/edsnlp/pipes/core/matcher/matcher/#edsnlp.pipes.core.matcher.matcher.GenericMatcher","title":"<code>GenericMatcher</code>","text":"<p>           Bases: <code>BaseNERComponent</code></p> <p>EDS-NLP simplifies the matching process by exposing a <code>eds.matcher</code> component that can match on terms or regular expressions.</p>"},{"location":"reference/edsnlp/pipes/core/matcher/matcher/#edsnlp.pipes.core.matcher.matcher.GenericMatcher--examples","title":"Examples","text":"<p>Let us redefine the pipeline :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],  # (1)\n    patient=\"patient\",  # (2)\n)\n\nregex = dict(\n    covid=r\"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\",  # (3)\n)\n\nnlp.add_pipe(\n    eds.matcher(\n        terms=terms,\n        regex=regex,\n        attr=\"LOWER\",\n        term_matcher=\"exact\",\n        term_matcher_config={},\n    ),\n)\n</code></pre> <ol> <li>Every key in the <code>terms</code> dictionary is mapped to a concept.</li> <li>The <code>eds.matcher</code> pipeline expects a list of expressions, or a single expression.</li> <li>We can also define regular expression patterns.</li> </ol> <p>This snippet is complete, and should run as is.</p> <p>Patterns, be they <code>terms</code> or <code>regex</code>, are defined as dictionaries where keys become  the label of the extracted entities. Dictionary values are either a single  expression or a list of expressions that match the concept.</p>"},{"location":"reference/edsnlp/pipes/core/matcher/matcher/#edsnlp.pipes.core.matcher.matcher.GenericMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'matcher'</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p>You won't be able to match on newlines if this is enabled and the \"spaces\"/\"newline\" option of <code>eds.normalizer</code> is enabled (by default).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>exact</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>span_setter</code> <p>How to set the spans in the doc.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p>"},{"location":"reference/edsnlp/pipes/core/matcher/matcher/#edsnlp.pipes.core.matcher.matcher.GenericMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.matcher</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/core/matcher/matcher/#edsnlp.pipes.core.matcher.matcher.GenericMatcher.process","title":"<code>process</code>","text":"<p>Find matching spans in doc.</p>"},{"location":"reference/edsnlp/pipes/core/matcher/matcher/#edsnlp.pipes.core.matcher.matcher.GenericMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>spans</code> <p>List of Spans returned by the matchers.</p> <p> TYPE: <code>List[Span]</code> </p>"},{"location":"reference/edsnlp/pipes/core/matcher/matcher/#edsnlp.pipes.core.matcher.matcher.GenericMatcher.__call__","title":"<code>__call__</code>","text":"<p>Adds spans to document.</p>"},{"location":"reference/edsnlp/pipes/core/matcher/matcher/#edsnlp.pipes.core.matcher.matcher.GenericMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for extracted terms.</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/","title":"<code>edsnlp.pipes.core.normalizer</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/accents/","title":"<code>edsnlp.pipes.core.normalizer.accents</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/accents/accents/","title":"<code>edsnlp.pipes.core.normalizer.accents.accents</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/accents/accents/#edsnlp.pipes.core.normalizer.accents.accents.AccentsConverter","title":"<code>AccentsConverter</code>","text":"<p>           Bases: <code>BaseComponent</code></p> <p>Normalises accents, using a same-length strategy.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/accents/accents/#edsnlp.pipes.core.normalizer.accents.accents.AccentsConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'spaces'</code> </p> <code>accents</code> <p>List of accentuated characters and their transcription.</p> <p> TYPE: <code>List[Tuple[str, str]]</code> DEFAULT: <code>[('\u00e7', 'c'), ('\u00e0\u00e1\u00e2\u00e4', 'a'), ('\u00e8\u00e9\u00ea\u00eb', 'e'), ('\u00ec\u00ed...</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/accents/accents/#edsnlp.pipes.core.normalizer.accents.accents.AccentsConverter.__call__","title":"<code>__call__</code>","text":"<p>Remove accents from spacy <code>NORM</code> attribute.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/accents/accents/#edsnlp.pipes.core.normalizer.accents.accents.AccentsConverter.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>The spaCy <code>Doc</code> object.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code> <p>The document, with accents removed in <code>Token.norm_</code>.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/accents/factory/","title":"<code>edsnlp.pipes.core.normalizer.accents.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/accents/factory/#edsnlp.pipes.core.normalizer.accents.factory.create_component","title":"<code>create_component = registry.factory.register('eds.accents', assigns=['token.norm'], deprecated=['accents'])(AccentsConverter)</code>  <code>module-attribute</code>","text":"<p>Normalises accents, using a same-length strategy.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/accents/factory/#edsnlp.pipes.core.normalizer.accents.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'spaces'</code> </p> <code>accents</code> <p>List of accentuated characters and their transcription.</p> <p> TYPE: <code>List[Tuple[str, str]]</code> DEFAULT: <code>[('\u00e7', 'c'), ('\u00e0\u00e1\u00e2\u00e4', 'a'), ('\u00e8\u00e9\u00ea\u00eb', 'e'), ('\u00ec\u00ed...</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/accents/patterns/","title":"<code>edsnlp.pipes.core.normalizer.accents.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/factory/","title":"<code>edsnlp.pipes.core.normalizer.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/factory/#edsnlp.pipes.core.normalizer.factory.create_component","title":"<code>create_component</code>","text":"<p>Normalisation pipeline. Modifies the <code>NORM</code> attribute, acting on five dimensions :</p> <ul> <li><code>lowercase</code>: using the default <code>NORM</code></li> <li><code>accents</code>: deterministic and fixed-length normalisation of accents.</li> <li><code>quotes</code>: deterministic and fixed-length normalisation of quotation marks.</li> <li><code>spaces</code>: \"removal\" of spaces tokens (via the tag_ attribute).</li> <li><code>pollution</code>: \"removal\" of pollutions (via the tag_ attribute).</li> </ul>"},{"location":"reference/edsnlp/pipes/core/normalizer/factory/#edsnlp.pipes.core.normalizer.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'normalizer'</code> </p> <code>lowercase</code> <p>Whether to remove case.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>accents</code> <p><code>Accents</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>quotes</code> <p><code>Quotes</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>spaces</code> <p><code>Spaces</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>pollution</code> <p>Optional <code>Pollution</code> configuration object.</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/normalizer/","title":"<code>edsnlp.pipes.core.normalizer.normalizer</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/normalizer/#edsnlp.pipes.core.normalizer.normalizer.Normalizer","title":"<code>Normalizer</code>","text":"<p>           Bases: <code>BaseComponent</code></p> <p>Normalisation pipeline. Modifies the <code>NORM</code> attribute, acting on five dimensions :</p> <ul> <li><code>lowercase</code>: using the default <code>NORM</code></li> <li><code>accents</code>: deterministic and fixed-length normalisation of accents.</li> <li><code>quotes</code>: deterministic and fixed-length normalisation of quotation marks.</li> <li><code>spaces</code>: \"removal\" of spaces tokens (via the tag_ attribute).</li> <li><code>pollution</code>: \"removal\" of pollutions (via the tag_ attribute).</li> </ul>"},{"location":"reference/edsnlp/pipes/core/normalizer/normalizer/#edsnlp.pipes.core.normalizer.normalizer.Normalizer--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'normalizer'</code> </p> <code>lowercase</code> <p>Whether to remove case.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>accents</code> <p>Optional <code>Accents</code> object.</p> <p> TYPE: <code>Optional[Accents]</code> DEFAULT: <code>None</code> </p> <code>quotes</code> <p>Optional <code>Quotes</code> object.</p> <p> TYPE: <code>Optional[Quotes]</code> DEFAULT: <code>None</code> </p> <code>spaces</code> <p>Optional <code>Spaces</code> object.</p> <p> TYPE: <code>Optional[Spaces]</code> DEFAULT: <code>None</code> </p> <code>pollution</code> <p>Optional <code>Pollution</code> object.</p> <p> TYPE: <code>Optional[Pollution]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/normalizer/#edsnlp.pipes.core.normalizer.normalizer.Normalizer.__call__","title":"<code>__call__</code>","text":"<p>Apply the normalisation pipeline, one component at a time.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/normalizer/#edsnlp.pipes.core.normalizer.normalizer.Normalizer.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy <code>Doc</code> object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code> <p>Doc object with <code>NORM</code> attribute modified</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/","title":"<code>edsnlp.pipes.core.normalizer.pollution</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/factory/","title":"<code>edsnlp.pipes.core.normalizer.pollution.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/factory/#edsnlp.pipes.core.normalizer.pollution.factory.create_component","title":"<code>create_component = registry.factory.register('eds.pollution', assigns=['doc.spans'], deprecated=['pollution'])(PollutionTagger)</code>  <code>module-attribute</code>","text":"<p>Tags pollution tokens.</p> <p>Populates a number of spaCy extensions :</p> <ul> <li><code>Token._.pollution</code> : indicates whether the token is a pollution</li> <li><code>Doc._.clean</code> : lists non-pollution tokens</li> <li><code>Doc._.clean_</code> : original text with pollutions removed.</li> <li><code>Doc._.char_clean_span</code> : method to create a Span using character   indices extracted using the cleaned text.</li> </ul>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/factory/#edsnlp.pipes.core.normalizer.pollution.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>pollution</code> <p>Dictionary containing regular expressions of pollution.</p> <p> TYPE: <code>Dict[str, Union[str, List[str]]]</code> DEFAULT: <code>{'information': True, 'bars': True, 'biology': ...</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/patterns/","title":"<code>edsnlp.pipes.core.normalizer.pollution.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/pollution/","title":"<code>edsnlp.pipes.core.normalizer.pollution.pollution</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/pollution/#edsnlp.pipes.core.normalizer.pollution.pollution.PollutionTagger","title":"<code>PollutionTagger</code>","text":"<p>           Bases: <code>BaseComponent</code></p> <p>Tags pollution tokens.</p> <p>Populates a number of spaCy extensions :</p> <ul> <li><code>Token._.pollution</code> : indicates whether the token is a pollution</li> <li><code>Doc._.clean</code> : lists non-pollution tokens</li> <li><code>Doc._.clean_</code> : original text with pollutions removed.</li> <li><code>Doc._.char_clean_span</code> : method to create a Span using character   indices extracted using the cleaned text.</li> </ul>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/pollution/#edsnlp.pipes.core.normalizer.pollution.pollution.PollutionTagger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>pollution</code> <p>Dictionary containing regular expressions of pollution.</p> <p> TYPE: <code>Dict[str, Union[str, List[str]]]</code> DEFAULT: <code>{'information': True, 'bars': True, 'biology': ...</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/pollution/#edsnlp.pipes.core.normalizer.pollution.pollution.PollutionTagger.build_patterns","title":"<code>build_patterns</code>","text":"<p>Builds the patterns for phrase matching.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/pollution/#edsnlp.pipes.core.normalizer.pollution.pollution.PollutionTagger.process","title":"<code>process</code>","text":"<p>Find pollutions in doc and clean candidate negations to remove pseudo negations</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/pollution/#edsnlp.pipes.core.normalizer.pollution.pollution.PollutionTagger.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>pollution</code> <p>list of pollution spans</p> <p> TYPE: <code>List[Span]</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/pollution/#edsnlp.pipes.core.normalizer.pollution.pollution.PollutionTagger.__call__","title":"<code>__call__</code>","text":"<p>Tags pollutions.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/pollution/pollution/#edsnlp.pipes.core.normalizer.pollution.pollution.PollutionTagger.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for pollutions.</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/","title":"<code>edsnlp.pipes.core.normalizer.quotes</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/factory/","title":"<code>edsnlp.pipes.core.normalizer.quotes.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/factory/#edsnlp.pipes.core.normalizer.quotes.factory.create_component","title":"<code>create_component = registry.factory.register('eds.quotes', assigns=['token.norm'], deprecated=['quotes'])(QuotesConverter)</code>  <code>module-attribute</code>","text":"<p>We normalise quotes, following this <code>source &lt;https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html&gt;</code>_.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/factory/#edsnlp.pipes.core.normalizer.quotes.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>quotes</code> <p>List of quotation characters and their transcription.</p> <p> TYPE: <code>List[Tuple[str, str]]</code> DEFAULT: <code>[('\uff02\u3003\u05f2\u1cd3\u2033\u05f4\u2036\u02f6\u02ba\u201c\u201d\u02dd\u201f', '\"'), ('\uff40\u0384\uff07\u02c8\u02ca\u144a\u02cb\ua78c\u16cc\ud81b\udf52\ud81b\udf51\u2018\u2019\u05d9\u055a\u201b\u055d`\u1fef\u2032...</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/patterns/","title":"<code>edsnlp.pipes.core.normalizer.quotes.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/quotes/","title":"<code>edsnlp.pipes.core.normalizer.quotes.quotes</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/quotes/#edsnlp.pipes.core.normalizer.quotes.quotes.QuotesConverter","title":"<code>QuotesConverter</code>","text":"<p>           Bases: <code>BaseComponent</code></p> <p>We normalise quotes, following this <code>source &lt;https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html&gt;</code>_.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/quotes/#edsnlp.pipes.core.normalizer.quotes.quotes.QuotesConverter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>quotes</code> <p>List of quotation characters and their transcription.</p> <p> TYPE: <code>List[Tuple[str, str]]</code> DEFAULT: <code>[('\uff02\u3003\u05f2\u1cd3\u2033\u05f4\u2036\u02f6\u02ba\u201c\u201d\u02dd\u201f', '\"'), ('\uff40\u0384\uff07\u02c8\u02ca\u144a\u02cb\ua78c\u16cc\ud81b\udf52\ud81b\udf51\u2018\u2019\u05d9\u055a\u201b\u055d`\u1fef\u2032...</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/quotes/#edsnlp.pipes.core.normalizer.quotes.quotes.QuotesConverter.__call__","title":"<code>__call__</code>","text":"<p>Normalises quotes.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/quotes/quotes/#edsnlp.pipes.core.normalizer.quotes.quotes.QuotesConverter.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>Document to process.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code> <p>Same document, with quotes normalised.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/remove_lowercase/","title":"<code>edsnlp.pipes.core.normalizer.remove_lowercase</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/remove_lowercase/factory/","title":"<code>edsnlp.pipes.core.normalizer.remove_lowercase.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/remove_lowercase/factory/#edsnlp.pipes.core.normalizer.remove_lowercase.factory.remove_lowercase","title":"<code>remove_lowercase</code>","text":"<p>Add case on the <code>NORM</code> custom attribute. Should always be applied first.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/remove_lowercase/factory/#edsnlp.pipes.core.normalizer.remove_lowercase.factory.remove_lowercase--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>The spaCy <code>Doc</code> object.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code> <p>The document, with case put back in <code>NORM</code>.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/remove_lowercase/factory/#edsnlp.pipes.core.normalizer.remove_lowercase.factory.create_component","title":"<code>create_component</code>","text":"<p>Add case on the <code>NORM</code> custom attribute. Should always be applied first.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/remove_lowercase/factory/#edsnlp.pipes.core.normalizer.remove_lowercase.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/","title":"<code>edsnlp.pipes.core.normalizer.spaces</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/factory/","title":"<code>edsnlp.pipes.core.normalizer.spaces.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/factory/#edsnlp.pipes.core.normalizer.spaces.factory.create_component","title":"<code>create_component = registry.factory.register('eds.spaces', assigns=['token.tag'], deprecated=['spaces'])(SpacesTagger)</code>  <code>module-attribute</code>","text":"<p>We assign \"SPACE\" to <code>token.tag</code> to be used by optimized components such as the EDSPhraseMatcher</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/factory/#edsnlp.pipes.core.normalizer.spaces.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'spaces'</code> </p> <code>newline</code> <p>Whether to update the newline tokens too</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/spaces/","title":"<code>edsnlp.pipes.core.normalizer.spaces.spaces</code>","text":""},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/spaces/#edsnlp.pipes.core.normalizer.spaces.spaces.SpacesTagger","title":"<code>SpacesTagger</code>","text":"<p>           Bases: <code>BaseComponent</code></p> <p>We assign \"SPACE\" to <code>token.tag</code> to be used by optimized components such as the EDSPhraseMatcher</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/spaces/#edsnlp.pipes.core.normalizer.spaces.spaces.SpacesTagger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'spaces'</code> </p> <code>newline</code> <p>Whether to update the newline tokens too</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/spaces/#edsnlp.pipes.core.normalizer.spaces.spaces.SpacesTagger.__call__","title":"<code>__call__</code>","text":"<p>Apply the component to the doc.</p>"},{"location":"reference/edsnlp/pipes/core/normalizer/spaces/spaces/#edsnlp.pipes.core.normalizer.spaces.spaces.SpacesTagger.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/core/sentences/","title":"<code>edsnlp.pipes.core.sentences</code>","text":""},{"location":"reference/edsnlp/pipes/core/sentences/factory/","title":"<code>edsnlp.pipes.core.sentences.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/sentences/factory/#edsnlp.pipes.core.sentences.factory.create_component","title":"<code>create_component = registry.factory.register('eds.sentences', assigns=['token.is_sent_start'], deprecated=['sentences'])(SentenceSegmenter)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.sentences</code> matcher provides an alternative to spaCy's default <code>sentencizer</code>, aiming to overcome some of its limitations.</p> <p>Indeed, the <code>sentencizer</code> merely looks at period characters to detect the end of a sentence, a strategy that often fails in a clinical note settings. Our <code>eds.sentences</code> component also classifies end-of-lines as sentence boundaries if the subsequent token begins with an uppercase character, leading to slightly better performances.</p> <p>Moreover, the <code>eds.sentences</code> component use the output of the <code>eds.normalizer</code> and <code>eds.endlines</code> output by default when these components are added to the pipeline.</p>"},{"location":"reference/edsnlp/pipes/core/sentences/factory/#edsnlp.pipes.core.sentences.factory.create_component--examples","title":"Examples","text":"EDS-NLPspaCy sentencizer <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())  # same as nlp.add_pipe(\"eds.sentences\")\n\ntext = \"\"\"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\nIl lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\"\n\"\"\"\n\ndoc = nlp(text)\n\nfor sentence in doc.sents:\n    print(\"&lt;s&gt;\", sentence, \"&lt;/s&gt;\")\n# Out: &lt;s&gt; Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\n# Out:  &lt;\\s&gt;\n# Out: &lt;s&gt; Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. &lt;\\s&gt;\n</code></pre> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\"sentencizer\")\n\ntext = \"\"\"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\"\nIl lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\n\"\"\"\n\ndoc = nlp(text)\n\nfor sentence in doc.sents:\n    print(\"&lt;s&gt;\", sentence, \"&lt;/s&gt;\")\n# Out: &lt;s&gt; Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\n# Out: Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. &lt;\\s&gt;\n</code></pre> <p>Notice how EDS-NLP's implementation is more robust to ill-defined sentence endings.</p>"},{"location":"reference/edsnlp/pipes/core/sentences/factory/#edsnlp.pipes.core.sentences.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The EDS-NLP pipeline</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'sentences'</code> </p> <code>punct_chars</code> <p>Punctuation characters.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>use_endlines</code> <p>Whether to use endlines prediction.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>check_capitalized</code> <p>Whether to check for capitalized words after newlines or full stops.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>min_newline_count</code> <p>The minimum number of newlines to consider a newline-triggered sentence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p>"},{"location":"reference/edsnlp/pipes/core/sentences/factory/#edsnlp.pipes.core.sentences.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.sentences</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/core/sentences/sentences/","title":"<code>edsnlp.pipes.core.sentences.sentences</code>","text":""},{"location":"reference/edsnlp/pipes/core/sentences/sentences/#edsnlp.pipes.core.sentences.sentences.SentenceSegmenter","title":"<code>SentenceSegmenter</code>","text":"<p>           Bases: <code>BaseComponent</code></p> <p>The <code>eds.sentences</code> matcher provides an alternative to spaCy's default <code>sentencizer</code>, aiming to overcome some of its limitations.</p> <p>Indeed, the <code>sentencizer</code> merely looks at period characters to detect the end of a sentence, a strategy that often fails in a clinical note settings. Our <code>eds.sentences</code> component also classifies end-of-lines as sentence boundaries if the subsequent token begins with an uppercase character, leading to slightly better performances.</p> <p>Moreover, the <code>eds.sentences</code> component use the output of the <code>eds.normalizer</code> and <code>eds.endlines</code> output by default when these components are added to the pipeline.</p>"},{"location":"reference/edsnlp/pipes/core/sentences/sentences/#edsnlp.pipes.core.sentences.sentences.SentenceSegmenter--examples","title":"Examples","text":"EDS-NLPspaCy sentencizer <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())  # same as nlp.add_pipe(\"eds.sentences\")\n\ntext = \"\"\"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\nIl lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\"\n\"\"\"\n\ndoc = nlp(text)\n\nfor sentence in doc.sents:\n    print(\"&lt;s&gt;\", sentence, \"&lt;/s&gt;\")\n# Out: &lt;s&gt; Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\n# Out:  &lt;\\s&gt;\n# Out: &lt;s&gt; Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. &lt;\\s&gt;\n</code></pre> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\"sentencizer\")\n\ntext = \"\"\"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\"\nIl lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\n\"\"\"\n\ndoc = nlp(text)\n\nfor sentence in doc.sents:\n    print(\"&lt;s&gt;\", sentence, \"&lt;/s&gt;\")\n# Out: &lt;s&gt; Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\n# Out: Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. &lt;\\s&gt;\n</code></pre> <p>Notice how EDS-NLP's implementation is more robust to ill-defined sentence endings.</p>"},{"location":"reference/edsnlp/pipes/core/sentences/sentences/#edsnlp.pipes.core.sentences.sentences.SentenceSegmenter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The EDS-NLP pipeline</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'sentences'</code> </p> <code>punct_chars</code> <p>Punctuation characters.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>use_endlines</code> <p>Whether to use endlines prediction.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>check_capitalized</code> <p>Whether to check for capitalized words after newlines or full stops.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>min_newline_count</code> <p>The minimum number of newlines to consider a newline-triggered sentence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p>"},{"location":"reference/edsnlp/pipes/core/sentences/sentences/#edsnlp.pipes.core.sentences.sentences.SentenceSegmenter--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.sentences</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/core/sentences/terms/","title":"<code>edsnlp.pipes.core.sentences.terms</code>","text":""},{"location":"reference/edsnlp/pipes/core/terminology/","title":"<code>edsnlp.pipes.core.terminology</code>","text":""},{"location":"reference/edsnlp/pipes/core/terminology/factory/","title":"<code>edsnlp.pipes.core.terminology.factory</code>","text":""},{"location":"reference/edsnlp/pipes/core/terminology/factory/#edsnlp.pipes.core.terminology.factory.create_component","title":"<code>create_component = registry.factory.register('eds.terminology', assigns=['doc.ents', 'doc.spans'], deprecated=['terminology'])(TerminologyMatcher)</code>  <code>module-attribute</code>","text":"<p>EDS-NLP simplifies the terminology matching process by exposing a <code>eds.terminology</code> pipeline that can match on terms or regular expressions.</p> <p>The terminology matcher is very similar to the generic matcher, although the use case differs slightly. The generic matcher is designed to extract any entity, while the terminology matcher is specifically tailored towards high volume terminologies.</p> <p>There are some key differences:</p> <ol> <li>It labels every matched entity to the same value, provided to the pipeline</li> <li>The keys provided in the <code>regex</code> and <code>terms</code> dictionaries are used as the    <code>kb_id_</code> of the entity, which handles fine-grained labelling</li> </ol> <p>For instance, a terminology matcher could detect every drug mention under the top-level label <code>drug</code>, and link each individual mention to a given drug through its <code>kb_id_</code> attribute.</p>"},{"location":"reference/edsnlp/pipes/core/terminology/factory/#edsnlp.pipes.core.terminology.factory.create_component--examples","title":"Examples","text":"<p>Let us redefine the pipeline :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],  # (1)\n    flu=[\"grippe saisonni\u00e8re\"],  # (2)\n)\n\nregex = dict(\n    covid=r\"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\",  # (3)\n)\n\nnlp.add_pipe(\n    eds.terminology(\n        label=\"disease\",\n        terms=terms,\n        regex=regex,\n        attr=\"LOWER\",\n    ),\n)\n</code></pre> <ol> <li>Every key in the <code>terms</code> dictionary is mapped to a concept.</li> <li>The <code>eds.matcher</code> pipeline expects a list of expressions, or a single expression.</li> <li>We can also define regular expression patterns.</li> </ol> <p>This snippet is complete, and should run as is.</p>"},{"location":"reference/edsnlp/pipes/core/terminology/factory/#edsnlp.pipes.core.terminology.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>exact</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <p>Patterns, be they <code>terms</code> or <code>regex</code>, are defined as dictionaries where keys become the <code>kb_id_</code> of the extracted entities. Dictionary values are either a single expression or a list of expressions that match the concept (see example).</p>"},{"location":"reference/edsnlp/pipes/core/terminology/factory/#edsnlp.pipes.core.terminology.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.terminology</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/core/terminology/terminology/","title":"<code>edsnlp.pipes.core.terminology.terminology</code>","text":""},{"location":"reference/edsnlp/pipes/core/terminology/terminology/#edsnlp.pipes.core.terminology.terminology.TerminologyMatcher","title":"<code>TerminologyMatcher</code>","text":"<p>           Bases: <code>BaseNERComponent</code></p> <p>EDS-NLP simplifies the terminology matching process by exposing a <code>eds.terminology</code> pipeline that can match on terms or regular expressions.</p> <p>The terminology matcher is very similar to the generic matcher, although the use case differs slightly. The generic matcher is designed to extract any entity, while the terminology matcher is specifically tailored towards high volume terminologies.</p> <p>There are some key differences:</p> <ol> <li>It labels every matched entity to the same value, provided to the pipeline</li> <li>The keys provided in the <code>regex</code> and <code>terms</code> dictionaries are used as the    <code>kb_id_</code> of the entity, which handles fine-grained labelling</li> </ol> <p>For instance, a terminology matcher could detect every drug mention under the top-level label <code>drug</code>, and link each individual mention to a given drug through its <code>kb_id_</code> attribute.</p>"},{"location":"reference/edsnlp/pipes/core/terminology/terminology/#edsnlp.pipes.core.terminology.terminology.TerminologyMatcher--examples","title":"Examples","text":"<p>Let us redefine the pipeline :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],  # (1)\n    flu=[\"grippe saisonni\u00e8re\"],  # (2)\n)\n\nregex = dict(\n    covid=r\"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\",  # (3)\n)\n\nnlp.add_pipe(\n    eds.terminology(\n        label=\"disease\",\n        terms=terms,\n        regex=regex,\n        attr=\"LOWER\",\n    ),\n)\n</code></pre> <ol> <li>Every key in the <code>terms</code> dictionary is mapped to a concept.</li> <li>The <code>eds.matcher</code> pipeline expects a list of expressions, or a single expression.</li> <li>We can also define regular expression patterns.</li> </ol> <p>This snippet is complete, and should run as is.</p>"},{"location":"reference/edsnlp/pipes/core/terminology/terminology/#edsnlp.pipes.core.terminology.terminology.TerminologyMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>exact</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <p>Patterns, be they <code>terms</code> or <code>regex</code>, are defined as dictionaries where keys become the <code>kb_id_</code> of the extracted entities. Dictionary values are either a single expression or a list of expressions that match the concept (see example).</p>"},{"location":"reference/edsnlp/pipes/core/terminology/terminology/#edsnlp.pipes.core.terminology.terminology.TerminologyMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.terminology</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/core/terminology/terminology/#edsnlp.pipes.core.terminology.terminology.TerminologyMatcher.process","title":"<code>process</code>","text":"<p>Find matching spans in doc.</p> <p>Post-process matches to account for terminology.</p>"},{"location":"reference/edsnlp/pipes/core/terminology/terminology/#edsnlp.pipes.core.terminology.terminology.TerminologyMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>spans</code> <p>List of Spans returned by the matchers.</p> <p> TYPE: <code>List[Span]</code> </p>"},{"location":"reference/edsnlp/pipes/core/terminology/terminology/#edsnlp.pipes.core.terminology.terminology.TerminologyMatcher.__call__","title":"<code>__call__</code>","text":"<p>Adds spans to document.</p>"},{"location":"reference/edsnlp/pipes/core/terminology/terminology/#edsnlp.pipes.core.terminology.terminology.TerminologyMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for extracted terms.</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/misc/","title":"<code>edsnlp.pipes.misc</code>","text":""},{"location":"reference/edsnlp/pipes/misc/consultation_dates/","title":"<code>edsnlp.pipes.misc.consultation_dates</code>","text":""},{"location":"reference/edsnlp/pipes/misc/consultation_dates/consultation_dates/","title":"<code>edsnlp.pipes.misc.consultation_dates.consultation_dates</code>","text":""},{"location":"reference/edsnlp/pipes/misc/consultation_dates/consultation_dates/#edsnlp.pipes.misc.consultation_dates.consultation_dates.ConsultationDatesMatcher","title":"<code>ConsultationDatesMatcher</code>","text":"<p>           Bases: <code>GenericMatcher</code></p> <p>The <code>eds.consultation-dates</code> matcher consists of two main parts:</p> <ul> <li>A matcher which finds mentions of consultation events (more details below)</li> <li>A date parser (see the corresponding pipe) that links a date to those events</li> </ul>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/consultation_dates/#edsnlp.pipes.misc.consultation_dates.consultation_dates.ConsultationDatesMatcher--examples","title":"Examples","text":"<p>Note</p> <p>The matcher has been built to run on consultation notes (<code>CR-CONS</code> at APHP), so please filter accordingly before proceeding.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        lowercase=True,\n        accents=True,\n        quotes=True,\n        pollution=False,\n    ),\n)\nnlp.add_pipe(eds.consultation_dates())\n\ntext = \"\"\"\nXXX\nObjet : Compte-Rendu de Consultation du 03/10/2018.\nXXX\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.spans[\"consultation_dates\"]\n# Out: [Consultation du 03/10/2018]\n\ndoc.spans[\"consultation_dates\"][0]._.consultation_date.to_datetime()\n# Out: 2018-10-03 00:00:00\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/consultation_dates/#edsnlp.pipes.misc.consultation_dates.consultation_dates.ConsultationDatesMatcher--extensions","title":"Extensions","text":"<p>The <code>eds.consultation_dates</code> pipeline declares one extension on the <code>Span</code> object: the <code>consultation_date</code> attribute, which is a Python <code>datetime</code> object.</p>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/consultation_dates/#edsnlp.pipes.misc.consultation_dates.consultation_dates.ConsultationDatesMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Language pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>consultation_mention</code> <p>List of RegEx for consultation mentions.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains terms directly referring to consultations, such as \"Consultation du...\" or \"Compte rendu du...\". This list is the only one enabled by default since it is fairly precise and not error-prone.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>True</code> </p> <code>town_mention</code> <p>List of RegEx for all AP-HP hospitals' towns mentions.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains the towns of each AP-HP's hospital. Its goal is to fetch dates mentioned as \"Paris, le 13 d\u00e9cembre 2015\". It has a high recall but poor precision, since those dates can often be dates of letter redaction instead of consultation dates.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>False</code> </p> <code>document_date_mention</code> <p>List of RegEx for document date.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains expressions mentioning the date of creation/edition of a document, such as \"Date du rapport: 13/12/2015\" or \"Sign\u00e9 le 13/12/2015\". Like <code>town_mention</code> patterns, it has a high recall but is prone to errors since document date and consultation date aren't necessary similar.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/consultation_dates/#edsnlp.pipes.misc.consultation_dates.consultation_dates.ConsultationDatesMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.consultation_dates</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/consultation_dates/#edsnlp.pipes.misc.consultation_dates.consultation_dates.ConsultationDatesMatcher.process","title":"<code>process</code>","text":"<p>Finds entities</p>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/consultation_dates/#edsnlp.pipes.misc.consultation_dates.consultation_dates.ConsultationDatesMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object with additional <code>doc.spans['consultation_dates]</code> <code>SpanGroup</code></p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/factory/","title":"<code>edsnlp.pipes.misc.consultation_dates.factory</code>","text":""},{"location":"reference/edsnlp/pipes/misc/consultation_dates/factory/#edsnlp.pipes.misc.consultation_dates.factory.create_component","title":"<code>create_component = registry.factory.register('eds.consultation_dates', assigns=['doc.spans', 'doc.ents'], deprecated=['consultation_dates'])(ConsultationDatesMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.consultation-dates</code> matcher consists of two main parts:</p> <ul> <li>A matcher which finds mentions of consultation events (more details below)</li> <li>A date parser (see the corresponding pipe) that links a date to those events</li> </ul>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/factory/#edsnlp.pipes.misc.consultation_dates.factory.create_component--examples","title":"Examples","text":"<p>Note</p> <p>The matcher has been built to run on consultation notes (<code>CR-CONS</code> at APHP), so please filter accordingly before proceeding.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        lowercase=True,\n        accents=True,\n        quotes=True,\n        pollution=False,\n    ),\n)\nnlp.add_pipe(eds.consultation_dates())\n\ntext = \"\"\"\nXXX\nObjet : Compte-Rendu de Consultation du 03/10/2018.\nXXX\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.spans[\"consultation_dates\"]\n# Out: [Consultation du 03/10/2018]\n\ndoc.spans[\"consultation_dates\"][0]._.consultation_date.to_datetime()\n# Out: 2018-10-03 00:00:00\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/factory/#edsnlp.pipes.misc.consultation_dates.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.consultation_dates</code> pipeline declares one extension on the <code>Span</code> object: the <code>consultation_date</code> attribute, which is a Python <code>datetime</code> object.</p>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/factory/#edsnlp.pipes.misc.consultation_dates.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Language pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>consultation_mention</code> <p>List of RegEx for consultation mentions.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains terms directly referring to consultations, such as \"Consultation du...\" or \"Compte rendu du...\". This list is the only one enabled by default since it is fairly precise and not error-prone.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>True</code> </p> <code>town_mention</code> <p>List of RegEx for all AP-HP hospitals' towns mentions.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains the towns of each AP-HP's hospital. Its goal is to fetch dates mentioned as \"Paris, le 13 d\u00e9cembre 2015\". It has a high recall but poor precision, since those dates can often be dates of letter redaction instead of consultation dates.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>False</code> </p> <code>document_date_mention</code> <p>List of RegEx for document date.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p>This list contains expressions mentioning the date of creation/edition of a document, such as \"Date du rapport: 13/12/2015\" or \"Sign\u00e9 le 13/12/2015\". Like <code>town_mention</code> patterns, it has a high recall but is prone to errors since document date and consultation date aren't necessary similar.</p> <p> TYPE: <code>Union[List[str], bool]</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/factory/#edsnlp.pipes.misc.consultation_dates.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.consultation_dates</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/consultation_dates/patterns/","title":"<code>edsnlp.pipes.misc.consultation_dates.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/","title":"<code>edsnlp.pipes.misc.dates</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/dates/","title":"<code>edsnlp.pipes.misc.dates.dates</code>","text":"<p><code>eds.dates</code> pipeline.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher","title":"<code>DatesMatcher</code>","text":"<p>           Bases: <code>BaseNERComponent</code></p> <p>The <code>eds.dates</code> matcher detects and normalize dates within a medical document. We use simple regular expressions to extract date mentions.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher--scope","title":"Scope","text":"<p>The <code>eds.dates</code> pipeline finds absolute (eg <code>23/08/2021</code>) and relative (eg <code>hier</code>, <code>la semaine derni\u00e8re</code>) dates alike. It also handles mentions of duration.</p> Type Example <code>absolute</code> <code>3 mai</code>, <code>03/05/2020</code> <code>relative</code> <code>hier</code>, <code>la semaine derni\u00e8re</code> <code>duration</code> <code>pendant quatre jours</code> <p>See the tutorial for a presentation of a full pipeline featuring the <code>eds.dates</code> component.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\nimport datetime\nimport pytz\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.dates())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac. \"\n    \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a un an pendant une semaine. \"\n    \"Il a \u00e9t\u00e9 diagnostiqu\u00e9 en mai 1995.\"\n)\n\ndoc = nlp(text)\n\ndates = doc.spans[\"dates\"]\ndates\n# Out: [23 ao\u00fbt 2021, il y a un an, mai 1995]\n\ndates[0]._.date.to_datetime()\n# Out: 2021-08-23 00:00:00\n\ndates[1]._.date.to_datetime()\n# Out: None\n\nnote_datetime = datetime.datetime(2021, 8, 27, tzinfo=pytz.timezone(\"Europe/Paris\"))\ndoc._.note_datetime = note_datetime\n\ndates[1]._.date.to_datetime()\n# Out: 2020-08-27 00:00:00+00:09\n\ndate_2_output = dates[2]._.date.to_datetime(\n    note_datetime=note_datetime,\n    infer_from_context=True,\n    tz=\"Europe/Paris\",\n    default_day=15,\n)\ndate_2_output\n# Out: 1995-05-15 00:00:00+02:00\n\ndoc.spans[\"durations\"]\n# Out: [pendant une semaine]\n</code></pre> <p>Example on a collection of documents stored in the OMOP schema :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\n# with cols \"note_id\", \"note_text\" and optionally \"note_datetime\"\nmy_omop_df = ...\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.dates(as_ents=True))\ndocs = edsnlp.data.from_pandas(my_omop_df)\ndocs = docs.map_pipeline(nlp)\ndocs = docs.to_pandas(\n    converter=\"ents\",\n    span_attributes=[\"date.datetime\"],\n)\nprint(docs)\n# note_id  start  end label lexical_variant span_type datetime\n# ...\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher--extensions","title":"Extensions","text":"<p>The <code>eds.dates</code> pipeline declares two extensions on the <code>Span</code> object:</p> <ul> <li>the <code>span._.date</code> attribute of a date contains a parsed version of the date.</li> <li>the <code>span._.duration</code> attribute of a duration contains a parsed version of the   duration.</li> </ul> <p>As with other components, you can use the <code>span._.value</code> attribute to get either the parsed date or the duration depending on the span.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the pipeline component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'dates'</code> </p> <code>absolute</code> <p>List of regular expressions for absolute dates.</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>relative</code> <p>List of regular expressions for relative dates (eg <code>hier</code>, <code>la semaine prochaine</code>).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>duration</code> <p>List of regular expressions for durations (eg <code>pendant trois mois</code>).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>false_positive</code> <p>List of regular expressions for false positive (eg phone numbers, etc).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Where to look for dates in the doc. By default, look in the whole doc. You can combine this with the <code>merge_mode</code> argument for interesting results.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>merge_mode</code> <p>How to merge matched dates with the spans from <code>span_getter</code>, if given:</p> <ul> <li><code>intersect</code>: return only the matches that fall in the <code>span_getter</code> spans</li> <li><code>align</code>: if a date overlaps a span from <code>span_getter</code> (e.g. a date extracted   by a machine learning model), return the <code>span_getter</code> span instead, and   assign all the parsed information (<code>._.date</code> / <code>._.duration</code>) to it. Otherwise   don't return the date.</li> </ul> <p> TYPE: <code>Literal['intersect', 'align']</code> DEFAULT: <code>intersect</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> and <code>merge_mode</code> instead. Whether to look on dates in the whole document or in specific sentences:</p> <ul> <li>If <code>True</code>: Only look in the sentences of each entity in doc.ents</li> <li>If False: Look in the whole document</li> <li>If given a string <code>key</code> or list of string: Only look in the sentences of   each entity in <code>doc.spans[key]</code></li> </ul> <p> TYPE: <code>Union[bool, str, Iterable[str]]</code> DEFAULT: <code>None</code> </p> <code>detect_periods</code> <p>Whether to detect periods (experimental)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>detect_time</code> <p>Whether to detect time inside dates</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>period_proximity_threshold</code> <p>Max number of words between two dates to extract a period.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>as_ents</code> <p>Deprecated, use span_setter instead. Whether to treat dates as entities</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>spaCy attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>LOWER</code> </p> <code>date_label</code> <p>Label to use for dates</p> <p> TYPE: <code>str</code> DEFAULT: <code>date</code> </p> <code>duration_label</code> <p>Label to use for durations</p> <p> TYPE: <code>str</code> DEFAULT: <code>duration</code> </p> <code>period_label</code> <p>Label to use for periods</p> <p> TYPE: <code>str</code> DEFAULT: <code>period</code> </p> <code>span_setter</code> <p>How to set matches in the doc.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'dates': ['date'], 'durations': ['duration'], ...</code> </p> <code>explain</code> <p>Whether to keep track of regex cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.dates</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.set_extensions","title":"<code>set_extensions</code>","text":"<p>Set extensions for the dates pipeline.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.process","title":"<code>process</code>","text":"<p>Find dates in doc.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>dates</code> <p>list of date spans</p> <p> TYPE: <code>List[Tuple[Span, Dict[str, str]]]</code> </p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.parse","title":"<code>parse</code>","text":"<p>Parse dates/durations using the groupdict returned by the matcher.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.parse--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>matches</code> <p>List of tuples containing the spans and groupdict returned by the matcher.</p> <p> TYPE: <code>List[Tuple[Span, Dict[str, str]]]</code> </p> RETURNS DESCRIPTION <code>Tuple[List[Span], List[Span]]</code> <p>List of processed spans, with the date parsed.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.process_periods","title":"<code>process_periods</code>","text":"<p>Experimental period detection.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.process_periods--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>dates</code> <p>List of detected dates.</p> <p> TYPE: <code>List[Span]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>List of detected periods.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.__call__","title":"<code>__call__</code>","text":"<p>Tags dates.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/dates/#edsnlp.pipes.misc.dates.dates.DatesMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for dates</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/misc/dates/factory/","title":"<code>edsnlp.pipes.misc.dates.factory</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/factory/#edsnlp.pipes.misc.dates.factory.create_component","title":"<code>create_component = registry.factory.register('eds.dates', assigns=['doc.spans', 'doc.ents'], deprecated=['dates'])(DatesMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.dates</code> matcher detects and normalize dates within a medical document. We use simple regular expressions to extract date mentions.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/factory/#edsnlp.pipes.misc.dates.factory.create_component--scope","title":"Scope","text":"<p>The <code>eds.dates</code> pipeline finds absolute (eg <code>23/08/2021</code>) and relative (eg <code>hier</code>, <code>la semaine derni\u00e8re</code>) dates alike. It also handles mentions of duration.</p> Type Example <code>absolute</code> <code>3 mai</code>, <code>03/05/2020</code> <code>relative</code> <code>hier</code>, <code>la semaine derni\u00e8re</code> <code>duration</code> <code>pendant quatre jours</code> <p>See the tutorial for a presentation of a full pipeline featuring the <code>eds.dates</code> component.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/factory/#edsnlp.pipes.misc.dates.factory.create_component--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\nimport datetime\nimport pytz\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.dates())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac. \"\n    \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a un an pendant une semaine. \"\n    \"Il a \u00e9t\u00e9 diagnostiqu\u00e9 en mai 1995.\"\n)\n\ndoc = nlp(text)\n\ndates = doc.spans[\"dates\"]\ndates\n# Out: [23 ao\u00fbt 2021, il y a un an, mai 1995]\n\ndates[0]._.date.to_datetime()\n# Out: 2021-08-23 00:00:00\n\ndates[1]._.date.to_datetime()\n# Out: None\n\nnote_datetime = datetime.datetime(2021, 8, 27, tzinfo=pytz.timezone(\"Europe/Paris\"))\ndoc._.note_datetime = note_datetime\n\ndates[1]._.date.to_datetime()\n# Out: 2020-08-27 00:00:00+00:09\n\ndate_2_output = dates[2]._.date.to_datetime(\n    note_datetime=note_datetime,\n    infer_from_context=True,\n    tz=\"Europe/Paris\",\n    default_day=15,\n)\ndate_2_output\n# Out: 1995-05-15 00:00:00+02:00\n\ndoc.spans[\"durations\"]\n# Out: [pendant une semaine]\n</code></pre> <p>Example on a collection of documents stored in the OMOP schema :</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\n# with cols \"note_id\", \"note_text\" and optionally \"note_datetime\"\nmy_omop_df = ...\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.dates(as_ents=True))\ndocs = edsnlp.data.from_pandas(my_omop_df)\ndocs = docs.map_pipeline(nlp)\ndocs = docs.to_pandas(\n    converter=\"ents\",\n    span_attributes=[\"date.datetime\"],\n)\nprint(docs)\n# note_id  start  end label lexical_variant span_type datetime\n# ...\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/dates/factory/#edsnlp.pipes.misc.dates.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.dates</code> pipeline declares two extensions on the <code>Span</code> object:</p> <ul> <li>the <code>span._.date</code> attribute of a date contains a parsed version of the date.</li> <li>the <code>span._.duration</code> attribute of a duration contains a parsed version of the   duration.</li> </ul> <p>As with other components, you can use the <code>span._.value</code> attribute to get either the parsed date or the duration depending on the span.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/factory/#edsnlp.pipes.misc.dates.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the pipeline component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'dates'</code> </p> <code>absolute</code> <p>List of regular expressions for absolute dates.</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>relative</code> <p>List of regular expressions for relative dates (eg <code>hier</code>, <code>la semaine prochaine</code>).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>duration</code> <p>List of regular expressions for durations (eg <code>pendant trois mois</code>).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>false_positive</code> <p>List of regular expressions for false positive (eg phone numbers, etc).</p> <p> TYPE: <code>Union[List[str], str]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Where to look for dates in the doc. By default, look in the whole doc. You can combine this with the <code>merge_mode</code> argument for interesting results.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>merge_mode</code> <p>How to merge matched dates with the spans from <code>span_getter</code>, if given:</p> <ul> <li><code>intersect</code>: return only the matches that fall in the <code>span_getter</code> spans</li> <li><code>align</code>: if a date overlaps a span from <code>span_getter</code> (e.g. a date extracted   by a machine learning model), return the <code>span_getter</code> span instead, and   assign all the parsed information (<code>._.date</code> / <code>._.duration</code>) to it. Otherwise   don't return the date.</li> </ul> <p> TYPE: <code>Literal['intersect', 'align']</code> DEFAULT: <code>intersect</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> and <code>merge_mode</code> instead. Whether to look on dates in the whole document or in specific sentences:</p> <ul> <li>If <code>True</code>: Only look in the sentences of each entity in doc.ents</li> <li>If False: Look in the whole document</li> <li>If given a string <code>key</code> or list of string: Only look in the sentences of   each entity in <code>doc.spans[key]</code></li> </ul> <p> TYPE: <code>Union[bool, str, Iterable[str]]</code> DEFAULT: <code>None</code> </p> <code>detect_periods</code> <p>Whether to detect periods (experimental)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>detect_time</code> <p>Whether to detect time inside dates</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>period_proximity_threshold</code> <p>Max number of words between two dates to extract a period.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>as_ents</code> <p>Deprecated, use span_setter instead. Whether to treat dates as entities</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>spaCy attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>LOWER</code> </p> <code>date_label</code> <p>Label to use for dates</p> <p> TYPE: <code>str</code> DEFAULT: <code>date</code> </p> <code>duration_label</code> <p>Label to use for durations</p> <p> TYPE: <code>str</code> DEFAULT: <code>duration</code> </p> <code>period_label</code> <p>Label to use for periods</p> <p> TYPE: <code>str</code> DEFAULT: <code>period</code> </p> <code>span_setter</code> <p>How to set matches in the doc.</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'dates': ['date'], 'durations': ['duration'], ...</code> </p> <code>explain</code> <p>Whether to keep track of regex cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/misc/dates/factory/#edsnlp.pipes.misc.dates.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.dates</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/","title":"<code>edsnlp.pipes.misc.dates.models</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.BaseDate","title":"<code>BaseDate</code>","text":"<p>           Bases: <code>BaseModel</code></p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.BaseDate.remove_space","title":"<code>remove_space</code>","text":"<p>Remove spaces. Useful for coping with ill-formatted PDF extractions.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.AbsoluteDate","title":"<code>AbsoluteDate</code>","text":"<p>           Bases: <code>BaseDate</code></p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.AbsoluteDate.to_datetime","title":"<code>to_datetime</code>","text":"<p>Convert the date to a datetime.datetime object.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.AbsoluteDate.to_datetime--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>tz</code> <p>The timezone to use. Defaults to None.</p> <p> TYPE: <code>Optional[Union[str, timezone]]</code> DEFAULT: <code>None</code> </p> <code>note_datetime</code> <p>The datetime of the note. Used to infer missing parts of the date.</p> <p> TYPE: <code>Optional[Union[datetime, datetime]]</code> DEFAULT: <code>None</code> </p> <code>infer_from_context</code> <p>Whether to infer missing parts of the date from the note datetime. In a (year, month, day) triplet:</p> <pre><code>- if only year is missing, it will be inferred from the note datetime\n- if only month is missing, it will be inferred from the note datetime\n- if only day is missing, it will be set to `default_day`\n- if only the year is given, the day and month will be set to\n  `default_day` and `default_month`\n- if only the month is given, the day will be set to `default_day`\n  and the year will be inferred from the note datetime\n- if only the day is given, the month and year will be inferred from\n  the note datetime\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>default_day</code> <p>Default day to use when inferring missing parts of the date.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>default_month</code> <p>Default month to use when inferring missing parts of the date.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Union[datetime, None]</code>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.Relative","title":"<code>Relative</code>","text":"<p>           Bases: <code>BaseDate</code></p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.Relative.parse_unit","title":"<code>parse_unit</code>","text":"<p>Units need to be handled separately.</p> <p>This validator modifies the key corresponding to the unit with the detected value</p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.Relative.parse_unit--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>d</code> <p>Original data</p> <p> TYPE: <code>Dict[str, str]</code> </p> RETURNS DESCRIPTION <code>Dict[str, str]</code> <p>Transformed data</p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.RelativeDate","title":"<code>RelativeDate</code>","text":"<p>           Bases: <code>Relative</code></p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.RelativeDate.handle_specifics","title":"<code>handle_specifics</code>","text":"<p>Specific patterns such as <code>aujourd'hui</code>, <code>hier</code>, etc, need to be handled separately.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/models/#edsnlp.pipes.misc.dates.models.RelativeDate.handle_specifics--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>d</code> <p>Original data.</p> <p> TYPE: <code>Dict[str, str]</code> </p> RETURNS DESCRIPTION <code>Dict[str, str]</code> <p>Modified data.</p>"},{"location":"reference/edsnlp/pipes/misc/dates/patterns/","title":"<code>edsnlp.pipes.misc.dates.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/absolute/","title":"<code>edsnlp.pipes.misc.dates.patterns.absolute</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/days/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.days</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/delimiters/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.delimiters</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/directions/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.directions</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/modes/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.modes</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/months/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.months</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/numbers/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.numbers</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/time/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.time</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/units/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.units</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/atomic/years/","title":"<code>edsnlp.pipes.misc.dates.patterns.atomic.years</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/current/","title":"<code>edsnlp.pipes.misc.dates.patterns.current</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/duration/","title":"<code>edsnlp.pipes.misc.dates.patterns.duration</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/false_positive/","title":"<code>edsnlp.pipes.misc.dates.patterns.false_positive</code>","text":""},{"location":"reference/edsnlp/pipes/misc/dates/patterns/relative/","title":"<code>edsnlp.pipes.misc.dates.patterns.relative</code>","text":""},{"location":"reference/edsnlp/pipes/misc/explode/","title":"<code>edsnlp.pipes.misc.explode</code>","text":""},{"location":"reference/edsnlp/pipes/misc/explode/explode/","title":"<code>edsnlp.pipes.misc.explode.explode</code>","text":""},{"location":"reference/edsnlp/pipes/misc/explode/explode/#edsnlp.pipes.misc.explode.explode.Explode","title":"<code>Explode</code>","text":"<p>Explode a Doc into multiple distinct Doc objects, one per span retrieved through the <code>span_getter</code> : each span becomes alone in its own Doc. Note that entities that are not selected by the <code>span_getter</code> will be lost in the new docs.</p> <p>Not for pipelines</p> <p>This component is not meant to be used in a pipeline, but rather as a preprocessing step when dealing with a stream of documents as in the example below.</p> <p>Difference with <code>eds.split</code></p> <p>While <code>eds.split</code> breaks a document into smaller chunks based on length or regex rules, <code>eds.explode</code> creates a separate document for each selected span. This means <code>eds.split</code> is typically used for segmenting text for context size or processing constraints, whereas <code>eds.explode</code> is designed for span-level tasks that require span-level mixing, like training span classifiers, ensuring that each span is isolated in its own document while preserving the original context.</p>"},{"location":"reference/edsnlp/pipes/misc/explode/explode/#edsnlp.pipes.misc.explode.explode.Explode--examples","title":"Examples","text":"<pre><code>import edsnlp.pipes as eds\nfrom edsnlp.data.converters import MarkupToDocConverter\n\nconverter = MarkupToDocConverter(\n    preset=\"xml\",\n    # Put xml annotated spans in distinct doc.spans[label] groups\n    span_setter={\"*\": True},\n)\ndoc = converter(\n    \"Le &lt;person&gt;patient&lt;/person&gt; a mal au &lt;body_part&gt;bras&lt;/body_part&gt;, \u00e0 la \"\n    \"&lt;body_part&gt;jambe&lt;/body_part&gt; et au &lt;body_part&gt;torse&lt;/body_part&gt;\"\n)\n\nexploder = eds.explode(span_getter=[\"body_part\"])\nprint(doc.text, \"-&gt;\", doc.spans)\n# Out: Le patient a mal au bras, \u00e0 la jambe et au torse -&gt; {'person': [patient], 'body_part': [bras, jambe, torse]}\n\nfor new_doc in exploder(doc):\n    print(new_doc.text, \"-&gt;\", new_doc.spans)\n# Out: Le patient a mal au bras, \u00e0 la jambe et au torse -&gt; {'person': [], 'body_part': [bras]}\n# Out: Le patient a mal au bras, \u00e0 la jambe et au torse -&gt; {'person': [], 'body_part': [jambe]}\n# Out: Le patient a mal au bras, \u00e0 la jambe et au torse -&gt; {'person': [], 'body_part': [torse]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/explode/explode/#edsnlp.pipes.misc.explode.explode.Explode--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[Pipeline]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'explode'</code> </p> <code>span_getter</code> <p>The span getter to use to retrieve spans from the Doc. Default is <code>{\"ents\": True}</code> which retrieves all entities in <code>doc.ents</code>.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>filter_expr</code> <p>An optional filter expression to filter the produced documents. The callable expects a single argument, the new Doc, and should return a boolean.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/misc/explode/explode/#edsnlp.pipes.misc.explode.explode.Explode.explode_doc","title":"<code>explode_doc</code>","text":"<p>Yield a sequence of docs, one per span returned by the getter.</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/","title":"<code>edsnlp.pipes.misc.quantities</code>","text":""},{"location":"reference/edsnlp/pipes/misc/quantities/factory/","title":"<code>edsnlp.pipes.misc.quantities.factory</code>","text":""},{"location":"reference/edsnlp/pipes/misc/quantities/factory/#edsnlp.pipes.misc.quantities.factory.create_component","title":"<code>create_component = registry.factory.register('eds.quantities', assigns=['doc.spans', 'doc.ents'], deprecated=['eds.measures', 'eds.measurements'])(QuantitiesMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.quantities</code> matcher detects and normalizes numerical quantities within a medical document.</p> <p>Warning</p> <p>The <code>quantities</code> pipeline is still in active development and has not been rigorously validated. If you come across a quantity expression that goes undetected, please file an issue !</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/factory/#edsnlp.pipes.misc.quantities.factory.create_component--pipe-definition","title":"Pipe definition","text":"<pre><code>text = \"\"\"Poids : 65. Taille : 1.75\n          On mesure ... \u00e0 3mmol/l ; pression : 100mPa-110mPa.\n          Acte r\u00e9alis\u00e9 par ... \u00e0 12h13\"\"\"\n</code></pre> All quantitiesCustom quantitiesPredefined quantities <pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.tables\")\nnlp.add_pipe(\n    \"eds.quantities\",\n    config=dict(\n        quantities=\"all\", extract_ranges=True, use_tables=True  # (3)  # (1)\n    ),  # (2)\n)\nnlp(text).spans[\"quantities\"]\n# Out: [65, 1.75, 3mmol/l, 100mPa-110mPa, 12h13]\n</code></pre> <ol> <li>100-110mg, 2 \u00e0 4 jours ...</li> <li>If True <code>eds.tables</code> must be called</li> <li>All units from Availability will be detected</li> </ol> <pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.tables\")\nnlp.add_pipe(\n    \"eds.quantities\",\n    config=dict(\n        quantities={\n            \"concentration\": {\"unit\": \"mol_per_l\"},\n            \"pressure\": {\"unit\": \"Pa\"},\n        },  # (3)\n        extract_ranges=True,  # (1)\n        use_tables=True,\n    ),  # (2)\n)\nnlp(text).spans[\"quantities\"]\n# Out: [3mmol/l, 100mPa-110mPa]\n</code></pre> <ol> <li>100-110mg, 2 \u00e0 4 jours ...</li> <li>If True <code>eds.tables</code> must be called</li> <li>Which units are available ? See Availability.    More on customization ? See Customization</li> </ol> <pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.tables\")\nnlp.add_pipe(\n    \"eds.quantities\",\n    config=dict(\n        quantities=[\"weight\", \"size\"],  # (3)\n        extract_ranges=True,  # (1)\n        use_tables=True,\n    ),  # (2)\n)\nnlp(text).spans[\"quantities\"]\n# Out: [65, 1.75]\n</code></pre> <ol> <li>100-110mg, 2 \u00e0 4 jours ...</li> <li>If True <code>eds.tables</code> must be called</li> <li>Which quantities are available ? See Availability</li> </ol>"},{"location":"reference/edsnlp/pipes/misc/quantities/factory/#edsnlp.pipes.misc.quantities.factory.create_component--scope","title":"Scope","text":"<p>The <code>eds.quantities</code> matcher can extract simple (e.g. <code>3cm</code>) quantities. It can also detect elliptic enumerations (eg <code>32, 33 et 34kg</code>) of quantities of the same type and split the quantities accordingly.</p> <p>The normalized value can then be accessed via the <code>span._.{measure_name}</code> attribute, for instance <code>span._.size</code> or <code>span._.weight</code> and be converted on the fly to a desired unit. Like for other components, the <code>span._.value</code> extension can also be used to access the normalized value for any quantity span.</p> <p>See Availability section for details on which units are handled</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/factory/#edsnlp.pipes.misc.quantities.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.quantities(\n        quantities=[\"size\", \"weight\", \"bmi\"],\n        extract_ranges=True,\n    ),\n)\n\ntext = \"\"\"\nLe patient est admis hier, fait 1m78 pour 76kg.\nLes deux nodules b\u00e9nins sont larges de 1,2 et 2.4mm.\nBMI: 24.\n\nLe nodule fait entre 1 et 1.5 cm\n\"\"\"\n\ndoc = nlp(text)\n\nquantities = doc.spans[\"quantities\"]\n\nquantities\n# Out: [1m78, 76kg, 1,2, 2.4mm, 24, entre 1 et 1.5 cm]\n\nquantities[0]\n# Out: 1m78\n\nstr(quantities[0]._.size), str(quantities[0]._.value)\n# Out: ('1.78 m', '1.78 m')\n\nquantities[0]._.value.cm\n# Out: 178.0\n\nquantities[2]\n# Out: 1,2\n\nstr(quantities[2]._.value)\n# Out: '1.2 mm'\n\nstr(quantities[2]._.value.mm)\n# Out: 1.2\n\nquantities[4]\n# Out: 24\n\nstr(quantities[4]._.value)\n# Out: '24 kg_per_m2'\n\nstr(quantities[4]._.value.kg_per_m2)\n# Out: 24\n\nstr(quantities[5]._.value)\n# Out: 1-1.5 cm\n</code></pre> <p>To extract all sizes in centimeters, and average range quantities, you can use the following snippet:</p> <pre><code>sizes = [\n    sum(item.cm for item in m._.value) / len(m._.value)\n    for m in doc.spans[\"quantities\"]\n    if m.label_ == \"size\"\n]\nsizes\n# Out: [178.0, 0.12, 0.24, 1.25]\n</code></pre> <p>To extract the quantities from many texts, you can use the following snippet:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.quantities(quantities=\"weight\", extract_ranges=True, as_ents=True),\n)\ntexts = [\"Le patient mesure 40000,0 g (aussi not\u00e9 40 kg)\"]\ndocs = edsnlp.data.from_iterable(texts)\ndocs = docs.map_pipeline(nlp)\ndocs.to_pandas(\n    converter=\"ents\",\n    span_attributes=[\"value.unit\", \"value.kg\"],\n)\n#   note_id  start  end   label lexical_variant span_type original_unit    kg\n# 0    None     18   27  weight       40000,0 g      ents             g  40.0\n# 1    None     40   45  weight           40 kg      ents            kg  40.0\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/quantities/factory/#edsnlp.pipes.misc.quantities.factory.create_component--available-units-and-quantities","title":"Available units and quantities","text":"<p>Feel free to propose any missing raw unit or predefined quantity.</p> <p>Raw units and their derivations (g, mg, mgr ...) and their compositions (g/ml, cac/j ...) can be detected.</p> <p>Available raw units :</p> <p><code>g, m, m2, m3, mol, ui, Pa, %, log, mmHg, s/min/h/d/w/m/y, arc-second, \u00b0, \u00b0C, cac, goutte, l, x10*4, x10*5</code></p> <p>Available predefined quantities :</p> quantity_name Example <code>size</code> <code>1m50</code>, <code>1.50m</code>... <code>weight</code> <code>1kg</code>, <code>Poids : 65</code>... <code>bmi</code> <code>BMI: 24</code>, <code>24 kg.m-2</code> <code>volume</code> <code>2 cac</code>, <code>8ml</code>... <p>See the patterns for exhaustive definition.</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/factory/#edsnlp.pipes.misc.quantities.factory.create_component--customization","title":"Customization","text":"<p>You can declare custom quantities by altering the patterns:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.quantities(\n        quantities={\n            \"my_custom_surface_quantity\": {\n                # This quantity unit is homogenous to square meters\n                \"unit\": \"m2\",\n                # Handle cases like \"surface: 1.8\" (implied m2),\n                # vs \"surface: 50\" (implied cm2)\n                \"unitless_patterns\": [\n                    {\n                        \"terms\": [\"surface\", \"aire\"],\n                        \"ranges\": [\n                            {\"unit\": \"m2\", \"min\": 0, \"max\": 9},\n                            {\"unit\": \"cm2\", \"min\": 10, \"max\": 100},\n                        ],\n                    }\n                ],\n            },\n        }\n    ),\n)\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/quantities/factory/#edsnlp.pipes.misc.quantities.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.quantities</code> pipeline declares its extensions dynamically, depending on the <code>quantities</code> parameter: each quantity gets its own extension, and is assigned to a different span group.</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/factory/#edsnlp.pipes.misc.quantities.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'quantities'</code> </p> <code>quantities</code> <p>A mapping from measure names to MsrConfig Each measure's configuration has the following shape: <pre><code>{\n  # the unit (e.g. \"kg\"),\n  \"unit\": str,\n  \"unitless_patterns\": {\n    # preceding trigger terms\n    \"terms\": List[str],\n    # unitless ranges -&gt; unit patterns\n    \"ranges\": List[\n      {\"min\": int, \"max\": int, \"unit\": str},\n      {\"min\": int, \"unit\": str},\n      ...,\n    ],\n    ...\n  }\n}\n</code></pre> Set <code>quantities=\"all\"</code> to extract all raw quantities from units_config file.</p> <p> TYPE: <code>Union[str, List[Union[str, MsrConfig]], Dict[str, MsrConfig]]</code> DEFAULT: <code>['weight', 'size', 'bmi', 'volume']</code> </p> <code>number_terms</code> <p>A mapping of numbers to their lexical variants</p> <p> TYPE: <code>Dict[str, List[str]]</code> DEFAULT: <code>{'0.125': ['\u215b'], '0.16666666': ['\u2159'], '0.2': ['...</code> </p> <code>stopwords</code> <p>A list of stopwords that do not matter when placed between a unitless trigger and a number</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>['par', 'sur', 'de', 'a', ',', 'et', '-', '\u00e0']</code> </p> <code>unit_divisors</code> <p>A list of terms used to divide two units (like: m / s)</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>['/', 'par']</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>ignore_excluded</code> <p>Whether to exclude pollution patterns when matching in the text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>compose_units</code> <p>Whether to compose units (like \"m/s\" or \"m.s-1\")</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>extract_ranges</code> <p>Whether to extract ranges (like \"entre 1 et 2 cm\")</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>range_patterns</code> <p>A list of \"{FROM} xx {TO} yy\" patterns to match range quantities</p> <p> TYPE: <code>List[Tuple[Optional[str], Optional[str]]]</code> DEFAULT: <code>[('De', '\u00e0'), ('De', 'a'), ('de', '\u00e0'), ('de', ...</code> </p> <code>after_snippet_limit</code> <p>Maximum word distance after to link a part of a quantity after its number</p> <p> TYPE: <code>int</code> DEFAULT: <code>6</code> </p> <code>before_snippet_limit</code> <p>Maximum word distance after to link a part of a quantity before its number</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>span_setter</code> <p>How to set the spans in the document. By default, each quantity will be assigned to its own span group (using either the \"name\" field of the config, or the key if you passed a dict), and to the \"quantities\" group.</p> <p> TYPE: <code>Optional[SpanSetterArg]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Where to look for quantities in the doc. By default, look in the whole doc. You can combine this with the <code>merge_mode</code> argument for interesting results.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>merge_mode</code> <p>How to merge matches with the spans from <code>span_getter</code>, if given:</p> <ul> <li><code>intersect</code>: return only the matches that fall in the <code>span_getter</code> spans</li> <li><code>align</code>: if a match overlaps a span from <code>span_getter</code> (e.g. a match   extracted by a machine learning model), return the <code>span_getter</code> span   instead, and assign all the parsed information (<code>._.date</code> / <code>._.duration</code>)   to it. Otherwise, don't return the date.</li> </ul> <p> TYPE: <code>Literal['intersect', 'align']</code> DEFAULT: <code>intersect</code> </p>"},{"location":"reference/edsnlp/pipes/misc/quantities/factory/#edsnlp.pipes.misc.quantities.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.quantities</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/patterns/","title":"<code>edsnlp.pipes.misc.quantities.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/","title":"<code>edsnlp.pipes.misc.quantities.quantities</code>","text":""},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.Quantity","title":"<code>Quantity</code>","text":"<p>           Bases: <code>ABC</code></p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.Quantity.__len__","title":"<code>__len__</code>  <code>abstractmethod</code>","text":"<p>Number of items in the measure (only one for SimpleQuantity)</p> RETURNS DESCRIPTION <code>int</code>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.Quantity.__iter__","title":"<code>__iter__</code>  <code>abstractmethod</code>","text":"<p>Iter over items of the measure (only one for SimpleQuantity)</p> RETURNS DESCRIPTION <code>Iterable[SimpleQuantity]</code>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.Quantity.__getitem__","title":"<code>__getitem__</code>  <code>abstractmethod</code>","text":"<p>Access items of the measure (only one for SimpleQuantity)</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.Quantity.__getitem__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>item</code> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>SimpleQuantity</code>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.SimpleQuantity","title":"<code>SimpleQuantity</code>","text":"<p>           Bases: <code>Quantity</code></p> <p>The SimpleQuantity class contains the value and unit for a single non-composite measure</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.SimpleQuantity--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>value</code> <p> TYPE: <code>float</code> </p> <code>unit</code> <p> TYPE: <code>str</code> </p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.RangeQuantity","title":"<code>RangeQuantity</code>","text":"<p>           Bases: <code>Quantity</code></p> <p>A class emulating a range of a unit from one value to another.</p> <p>Initialize the class with two values, a unit and a registry.</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.RangeQuantity--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>from_value</code> <p> TYPE: <code>float</code> </p> <code>to_value</code> <p> TYPE: <code>float</code> </p> <code>unit</code> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>RangeQuantity</code>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.RangeQuantity.from_quantities","title":"<code>from_quantities</code>  <code>classmethod</code>","text":"<p>Build the RangeQuantity object from two SimpleQuantity instances.</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.RangeQuantity.from_quantities--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>a</code> <p> TYPE: <code>SimpleQuantity</code> </p> <code>b</code> <p> TYPE: <code>SimpleQuantity</code> </p> RETURNS DESCRIPTION <code>RangeQuantity</code>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher","title":"<code>QuantitiesMatcher</code>","text":"<p>           Bases: <code>BaseNERComponent</code></p> <p>The <code>eds.quantities</code> matcher detects and normalizes numerical quantities within a medical document.</p> <p>Warning</p> <p>The <code>quantities</code> pipeline is still in active development and has not been rigorously validated. If you come across a quantity expression that goes undetected, please file an issue !</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher--pipe-definition","title":"Pipe definition","text":"<pre><code>text = \"\"\"Poids : 65. Taille : 1.75\n          On mesure ... \u00e0 3mmol/l ; pression : 100mPa-110mPa.\n          Acte r\u00e9alis\u00e9 par ... \u00e0 12h13\"\"\"\n</code></pre> All quantitiesCustom quantitiesPredefined quantities <pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.tables\")\nnlp.add_pipe(\n    \"eds.quantities\",\n    config=dict(\n        quantities=\"all\", extract_ranges=True, use_tables=True  # (3)  # (1)\n    ),  # (2)\n)\nnlp(text).spans[\"quantities\"]\n# Out: [65, 1.75, 3mmol/l, 100mPa-110mPa, 12h13]\n</code></pre> <ol> <li>100-110mg, 2 \u00e0 4 jours ...</li> <li>If True <code>eds.tables</code> must be called</li> <li>All units from Availability will be detected</li> </ol> <pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.tables\")\nnlp.add_pipe(\n    \"eds.quantities\",\n    config=dict(\n        quantities={\n            \"concentration\": {\"unit\": \"mol_per_l\"},\n            \"pressure\": {\"unit\": \"Pa\"},\n        },  # (3)\n        extract_ranges=True,  # (1)\n        use_tables=True,\n    ),  # (2)\n)\nnlp(text).spans[\"quantities\"]\n# Out: [3mmol/l, 100mPa-110mPa]\n</code></pre> <ol> <li>100-110mg, 2 \u00e0 4 jours ...</li> <li>If True <code>eds.tables</code> must be called</li> <li>Which units are available ? See Availability.    More on customization ? See Customization</li> </ol> <pre><code>import edsnlp\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.tables\")\nnlp.add_pipe(\n    \"eds.quantities\",\n    config=dict(\n        quantities=[\"weight\", \"size\"],  # (3)\n        extract_ranges=True,  # (1)\n        use_tables=True,\n    ),  # (2)\n)\nnlp(text).spans[\"quantities\"]\n# Out: [65, 1.75]\n</code></pre> <ol> <li>100-110mg, 2 \u00e0 4 jours ...</li> <li>If True <code>eds.tables</code> must be called</li> <li>Which quantities are available ? See Availability</li> </ol>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher--scope","title":"Scope","text":"<p>The <code>eds.quantities</code> matcher can extract simple (e.g. <code>3cm</code>) quantities. It can also detect elliptic enumerations (eg <code>32, 33 et 34kg</code>) of quantities of the same type and split the quantities accordingly.</p> <p>The normalized value can then be accessed via the <code>span._.{measure_name}</code> attribute, for instance <code>span._.size</code> or <code>span._.weight</code> and be converted on the fly to a desired unit. Like for other components, the <code>span._.value</code> extension can also be used to access the normalized value for any quantity span.</p> <p>See Availability section for details on which units are handled</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.quantities(\n        quantities=[\"size\", \"weight\", \"bmi\"],\n        extract_ranges=True,\n    ),\n)\n\ntext = \"\"\"\nLe patient est admis hier, fait 1m78 pour 76kg.\nLes deux nodules b\u00e9nins sont larges de 1,2 et 2.4mm.\nBMI: 24.\n\nLe nodule fait entre 1 et 1.5 cm\n\"\"\"\n\ndoc = nlp(text)\n\nquantities = doc.spans[\"quantities\"]\n\nquantities\n# Out: [1m78, 76kg, 1,2, 2.4mm, 24, entre 1 et 1.5 cm]\n\nquantities[0]\n# Out: 1m78\n\nstr(quantities[0]._.size), str(quantities[0]._.value)\n# Out: ('1.78 m', '1.78 m')\n\nquantities[0]._.value.cm\n# Out: 178.0\n\nquantities[2]\n# Out: 1,2\n\nstr(quantities[2]._.value)\n# Out: '1.2 mm'\n\nstr(quantities[2]._.value.mm)\n# Out: 1.2\n\nquantities[4]\n# Out: 24\n\nstr(quantities[4]._.value)\n# Out: '24 kg_per_m2'\n\nstr(quantities[4]._.value.kg_per_m2)\n# Out: 24\n\nstr(quantities[5]._.value)\n# Out: 1-1.5 cm\n</code></pre> <p>To extract all sizes in centimeters, and average range quantities, you can use the following snippet:</p> <pre><code>sizes = [\n    sum(item.cm for item in m._.value) / len(m._.value)\n    for m in doc.spans[\"quantities\"]\n    if m.label_ == \"size\"\n]\nsizes\n# Out: [178.0, 0.12, 0.24, 1.25]\n</code></pre> <p>To extract the quantities from many texts, you can use the following snippet:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.quantities(quantities=\"weight\", extract_ranges=True, as_ents=True),\n)\ntexts = [\"Le patient mesure 40000,0 g (aussi not\u00e9 40 kg)\"]\ndocs = edsnlp.data.from_iterable(texts)\ndocs = docs.map_pipeline(nlp)\ndocs.to_pandas(\n    converter=\"ents\",\n    span_attributes=[\"value.unit\", \"value.kg\"],\n)\n#   note_id  start  end   label lexical_variant span_type original_unit    kg\n# 0    None     18   27  weight       40000,0 g      ents             g  40.0\n# 1    None     40   45  weight           40 kg      ents            kg  40.0\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher--available-units-and-quantities","title":"Available units and quantities","text":"<p>Feel free to propose any missing raw unit or predefined quantity.</p> <p>Raw units and their derivations (g, mg, mgr ...) and their compositions (g/ml, cac/j ...) can be detected.</p> <p>Available raw units :</p> <p><code>g, m, m2, m3, mol, ui, Pa, %, log, mmHg, s/min/h/d/w/m/y, arc-second, \u00b0, \u00b0C, cac, goutte, l, x10*4, x10*5</code></p> <p>Available predefined quantities :</p> quantity_name Example <code>size</code> <code>1m50</code>, <code>1.50m</code>... <code>weight</code> <code>1kg</code>, <code>Poids : 65</code>... <code>bmi</code> <code>BMI: 24</code>, <code>24 kg.m-2</code> <code>volume</code> <code>2 cac</code>, <code>8ml</code>... <p>See the patterns for exhaustive definition.</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher--customization","title":"Customization","text":"<p>You can declare custom quantities by altering the patterns:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.quantities(\n        quantities={\n            \"my_custom_surface_quantity\": {\n                # This quantity unit is homogenous to square meters\n                \"unit\": \"m2\",\n                # Handle cases like \"surface: 1.8\" (implied m2),\n                # vs \"surface: 50\" (implied cm2)\n                \"unitless_patterns\": [\n                    {\n                        \"terms\": [\"surface\", \"aire\"],\n                        \"ranges\": [\n                            {\"unit\": \"m2\", \"min\": 0, \"max\": 9},\n                            {\"unit\": \"cm2\", \"min\": 10, \"max\": 100},\n                        ],\n                    }\n                ],\n            },\n        }\n    ),\n)\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher--extensions","title":"Extensions","text":"<p>The <code>eds.quantities</code> pipeline declares its extensions dynamically, depending on the <code>quantities</code> parameter: each quantity gets its own extension, and is assigned to a different span group.</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'quantities'</code> </p> <code>quantities</code> <p>A mapping from measure names to MsrConfig Each measure's configuration has the following shape: <pre><code>{\n  # the unit (e.g. \"kg\"),\n  \"unit\": str,\n  \"unitless_patterns\": {\n    # preceding trigger terms\n    \"terms\": List[str],\n    # unitless ranges -&gt; unit patterns\n    \"ranges\": List[\n      {\"min\": int, \"max\": int, \"unit\": str},\n      {\"min\": int, \"unit\": str},\n      ...,\n    ],\n    ...\n  }\n}\n</code></pre> Set <code>quantities=\"all\"</code> to extract all raw quantities from units_config file.</p> <p> TYPE: <code>Union[str, List[Union[str, MsrConfig]], Dict[str, MsrConfig]]</code> DEFAULT: <code>['weight', 'size', 'bmi', 'volume']</code> </p> <code>number_terms</code> <p>A mapping of numbers to their lexical variants</p> <p> TYPE: <code>Dict[str, List[str]]</code> DEFAULT: <code>{'0.125': ['\u215b'], '0.16666666': ['\u2159'], '0.2': ['...</code> </p> <code>stopwords</code> <p>A list of stopwords that do not matter when placed between a unitless trigger and a number</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>['par', 'sur', 'de', 'a', ',', 'et', '-', '\u00e0']</code> </p> <code>unit_divisors</code> <p>A list of terms used to divide two units (like: m / s)</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>['/', 'par']</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>ignore_excluded</code> <p>Whether to exclude pollution patterns when matching in the text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>compose_units</code> <p>Whether to compose units (like \"m/s\" or \"m.s-1\")</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>extract_ranges</code> <p>Whether to extract ranges (like \"entre 1 et 2 cm\")</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>range_patterns</code> <p>A list of \"{FROM} xx {TO} yy\" patterns to match range quantities</p> <p> TYPE: <code>List[Tuple[Optional[str], Optional[str]]]</code> DEFAULT: <code>[('De', '\u00e0'), ('De', 'a'), ('de', '\u00e0'), ('de', ...</code> </p> <code>after_snippet_limit</code> <p>Maximum word distance after to link a part of a quantity after its number</p> <p> TYPE: <code>int</code> DEFAULT: <code>6</code> </p> <code>before_snippet_limit</code> <p>Maximum word distance after to link a part of a quantity before its number</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>span_setter</code> <p>How to set the spans in the document. By default, each quantity will be assigned to its own span group (using either the \"name\" field of the config, or the key if you passed a dict), and to the \"quantities\" group.</p> <p> TYPE: <code>Optional[SpanSetterArg]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Where to look for quantities in the doc. By default, look in the whole doc. You can combine this with the <code>merge_mode</code> argument for interesting results.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>merge_mode</code> <p>How to merge matches with the spans from <code>span_getter</code>, if given:</p> <ul> <li><code>intersect</code>: return only the matches that fall in the <code>span_getter</code> spans</li> <li><code>align</code>: if a match overlaps a span from <code>span_getter</code> (e.g. a match   extracted by a machine learning model), return the <code>span_getter</code> span   instead, and assign all the parsed information (<code>._.date</code> / <code>._.duration</code>)   to it. Otherwise, don't return the date.</li> </ul> <p> TYPE: <code>Literal['intersect', 'align']</code> DEFAULT: <code>intersect</code> </p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.quantities</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.set_extensions","title":"<code>set_extensions</code>","text":"<p>Set extensions for the quantities pipeline.</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.extract_units","title":"<code>extract_units</code>","text":"<p>Extracts unit spans from the document by extracting unit atoms (declared in the units_config parameter) and aggregating them automatically Ex: \"il faut 2 g par jour\" =&gt; we extract [g]=unit(g), [par]=divisor(per), [jour]=unit(day) =&gt; we aggregate these adjacent matches together to compose a new unit g_per_day</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.extract_units--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>term_matches</code> <p> TYPE: <code>Iterable[Span]</code> </p> RETURNS DESCRIPTION <code>Iterable[Span]</code>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.make_pseudo_sentence","title":"<code>make_pseudo_sentence</code>  <code>classmethod</code>","text":"<p>Creates a pseudo sentence (one letter per entity) to extract higher order patterns Ex: the sentence \"Il font {1}{,} {2} {et} {3} {cm} de long{.}\" is transformed into \"wn,n,nuw.\"</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.make_pseudo_sentence--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p>The document or span to transform</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>matches</code> <p>List of tuple of span and whether the span represents a sentence end</p> <p> TYPE: <code>List[Tuple[Span, bool]]</code> </p> <code>pseudo_mapping</code> <p>A mapping from label to char in the pseudo sentence</p> <p> TYPE: <code>Dict[int, str]</code> </p> RETURNS DESCRIPTION <code>(str, List[int])</code> <ul> <li>the pseudo sentence</li> <li>a list of offsets to convert match indices into pseudo sent char indices</li> </ul>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.get_matches","title":"<code>get_matches</code>","text":"<p>Extract and filter regex and phrase matches in the document to prepare the quantity extraction. Returns the matches and a list of hashes to quickly find unit matches</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.get_matches--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> RETURNS DESCRIPTION <code>Tuple[List[Span, bool], Set[int]]</code> <ul> <li>List of tuples of spans and whether the spans represents a sentence end</li> <li>List of hash label to distinguish unit from other matches</li> </ul>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.extract_quantities","title":"<code>extract_quantities</code>","text":"<p>Extracts measure entities from the document</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.extract_quantities--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>List[Span]</code>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.merge_adjacent_quantities","title":"<code>merge_adjacent_quantities</code>  <code>classmethod</code>","text":"<p>Aggregates extracted quantities together when they are adjacent to handle cases like - 1 meter 50 cm - 30\u00b0 4' 54\"</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.merge_adjacent_quantities--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>quantities</code> <p> TYPE: <code>List[Span]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.merge_quantities_in_ranges","title":"<code>merge_quantities_in_ranges</code>","text":"<p>Aggregates extracted quantities together when they are adjacent to handle cases like - 1 meter 50 cm - 30\u00b0 4' 54\"</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.merge_quantities_in_ranges--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>quantities</code> <p> TYPE: <code>List[Span]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.merge_with_existing","title":"<code>merge_with_existing</code>","text":"<p>Merges the extracted quantities with the existing quantities in the document.</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.merge_with_existing--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>extracted</code> <p>The extracted quantities</p> <p> TYPE: <code>List[Span]</code> </p> <code>existing</code> <p>The existing quantities in the document</p> <p> TYPE: <code>List[Span]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.__call__","title":"<code>__call__</code>","text":"<p>Adds quantities to document's \"quantities\" SpanGroup.</p>"},{"location":"reference/edsnlp/pipes/misc/quantities/quantities/#edsnlp.pipes.misc.quantities.quantities.QuantitiesMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for extracted quantities.</p>"},{"location":"reference/edsnlp/pipes/misc/reason/","title":"<code>edsnlp.pipes.misc.reason</code>","text":""},{"location":"reference/edsnlp/pipes/misc/reason/factory/","title":"<code>edsnlp.pipes.misc.reason.factory</code>","text":""},{"location":"reference/edsnlp/pipes/misc/reason/factory/#edsnlp.pipes.misc.reason.factory.create_component","title":"<code>create_component = registry.factory.register('eds.reason', assigns=['doc.spans', 'doc.ents'], deprecated=['reason'])(ReasonMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.reason</code> matcher uses a rule-based algorithm to detect spans that relate to the reason of the hospitalisation. It was designed at AP-HP's EDS.</p>"},{"location":"reference/edsnlp/pipes/misc/reason/factory/#edsnlp.pipes.misc.reason.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and looks for spans of hospitalisation reasons. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018\nMOTIF D'HOSPITALISATION\nMonsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978,\na \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nANT\u00c9C\u00c9DENTS\nAnt\u00e9c\u00e9dents m\u00e9dicaux :\nPremier \u00e9pisode d'asthme en mai 2018.\"\"\"\n\nnlp = edsnlp.blank(\"eds\")\n\n# Extraction of entities\nnlp.add_pipe(\n    eds.matcher(\n        terms=dict(\n            respiratoire=[\n                \"asthmatique\",\n                \"asthme\",\n                \"toux\",\n            ]\n        )\n    ),\n)\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.reason(use_sections=True))\ndoc = nlp(text)\n\nreason = doc.spans[\"reasons\"][0]\nreason\n# Out: hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nreason._.is_reason\n# Out: True\n\nentities = reason._.ents_reason\nentities\n# Out: [asthme]\n\nentities[0].label_\n# Out: 'respiratoire'\n\nent = entities[0]\nent._.is_reason\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/reason/factory/#edsnlp.pipes.misc.reason.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.reason</code> pipeline adds the key <code>reasons</code> to <code>doc.spans</code> and declares one extension, on the <code>Span</code> objects called <code>ents_reason</code>.</p> <p>The <code>ents_reason</code> extension is a list of named entities that overlap the <code>Span</code>, typically entities found in upstream components like <code>matcher</code>.</p> <p>It also declares the boolean extension <code>is_reason</code>. This extension is set to True for the Reason Spans but also for the entities that overlap the reason span.</p>"},{"location":"reference/edsnlp/pipes/misc/reason/factory/#edsnlp.pipes.misc.reason.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> </p> <code>reasons</code> <p>Reason patterns</p> <p> TYPE: <code>Dict[str, Union[List[str], str]]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>Default token attribute to use to build the text to match on.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>use_sections</code> <p>Whether or not use the <code>sections</code> matcher to improve results.</p> <p> TYPE: <code>(bool)</code> DEFAULT: <code>False</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/misc/reason/factory/#edsnlp.pipes.misc.reason.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.reason</code> matcher was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/reason/patterns/","title":"<code>edsnlp.pipes.misc.reason.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/misc/reason/reason/","title":"<code>edsnlp.pipes.misc.reason.reason</code>","text":""},{"location":"reference/edsnlp/pipes/misc/reason/reason/#edsnlp.pipes.misc.reason.reason.ReasonMatcher","title":"<code>ReasonMatcher</code>","text":"<p>           Bases: <code>GenericMatcher</code></p> <p>The <code>eds.reason</code> matcher uses a rule-based algorithm to detect spans that relate to the reason of the hospitalisation. It was designed at AP-HP's EDS.</p>"},{"location":"reference/edsnlp/pipes/misc/reason/reason/#edsnlp.pipes.misc.reason.reason.ReasonMatcher--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and looks for spans of hospitalisation reasons. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\ntext = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018\nMOTIF D'HOSPITALISATION\nMonsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978,\na \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nANT\u00c9C\u00c9DENTS\nAnt\u00e9c\u00e9dents m\u00e9dicaux :\nPremier \u00e9pisode d'asthme en mai 2018.\"\"\"\n\nnlp = edsnlp.blank(\"eds\")\n\n# Extraction of entities\nnlp.add_pipe(\n    eds.matcher(\n        terms=dict(\n            respiratoire=[\n                \"asthmatique\",\n                \"asthme\",\n                \"toux\",\n            ]\n        )\n    ),\n)\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.reason(use_sections=True))\ndoc = nlp(text)\n\nreason = doc.spans[\"reasons\"][0]\nreason\n# Out: hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nreason._.is_reason\n# Out: True\n\nentities = reason._.ents_reason\nentities\n# Out: [asthme]\n\nentities[0].label_\n# Out: 'respiratoire'\n\nent = entities[0]\nent._.is_reason\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/reason/reason/#edsnlp.pipes.misc.reason.reason.ReasonMatcher--extensions","title":"Extensions","text":"<p>The <code>eds.reason</code> pipeline adds the key <code>reasons</code> to <code>doc.spans</code> and declares one extension, on the <code>Span</code> objects called <code>ents_reason</code>.</p> <p>The <code>ents_reason</code> extension is a list of named entities that overlap the <code>Span</code>, typically entities found in upstream components like <code>matcher</code>.</p> <p>It also declares the boolean extension <code>is_reason</code>. This extension is set to True for the Reason Spans but also for the entities that overlap the reason span.</p>"},{"location":"reference/edsnlp/pipes/misc/reason/reason/#edsnlp.pipes.misc.reason.reason.ReasonMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> </p> <code>reasons</code> <p>Reason patterns</p> <p> TYPE: <code>Dict[str, Union[List[str], str]]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>Default token attribute to use to build the text to match on.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>use_sections</code> <p>Whether or not use the <code>sections</code> matcher to improve results.</p> <p> TYPE: <code>(bool)</code> DEFAULT: <code>False</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/misc/reason/reason/#edsnlp.pipes.misc.reason.reason.ReasonMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.reason</code> matcher was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/reason/reason/#edsnlp.pipes.misc.reason.reason.ReasonMatcher.__call__","title":"<code>__call__</code>","text":"<p>Find spans related to the reasons of the hospitalisation</p>"},{"location":"reference/edsnlp/pipes/misc/reason/reason/#edsnlp.pipes.misc.reason.reason.ReasonMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code>"},{"location":"reference/edsnlp/pipes/misc/sections/","title":"<code>edsnlp.pipes.misc.sections</code>","text":""},{"location":"reference/edsnlp/pipes/misc/sections/factory/","title":"<code>edsnlp.pipes.misc.sections.factory</code>","text":""},{"location":"reference/edsnlp/pipes/misc/sections/factory/#edsnlp.pipes.misc.sections.factory.create_component","title":"<code>create_component = registry.factory.register('eds.sections', assigns=['doc.spans', 'doc.ents'], deprecated=['sections'])(SectionsMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.sections</code> component extracts section titles from clinical documents. A \"section\" is then defined as the span of text between two titles.</p> <p>Here is the list of sections that are currently targeted :</p> <ul> <li><code>allergies</code></li> <li><code>ant\u00e9c\u00e9dents</code></li> <li><code>ant\u00e9c\u00e9dents familiaux</code></li> <li><code>traitements entr\u00e9e</code></li> <li><code>conclusion</code></li> <li><code>conclusion entr\u00e9e</code></li> <li><code>habitus</code></li> <li><code>correspondants</code></li> <li><code>diagnostic</code></li> <li><code>donn\u00e9es biom\u00e9triques entr\u00e9e</code></li> <li><code>examens</code></li> <li><code>examens compl\u00e9mentaires</code></li> <li><code>facteurs de risques</code></li> <li><code>histoire de la maladie</code></li> <li><code>actes</code></li> <li><code>motif</code></li> <li><code>prescriptions</code></li> <li><code>traitements sortie</code></li> <li><code>evolution</code></li> <li><code>modalites sortie</code></li> <li><code>vaccinations</code></li> <li><code>introduction</code></li> </ul> <p>Remarks :</p> <ul> <li>section <code>introduction</code> corresponds to the span of text between the header   \"COMPTE RENDU D'HOSPITALISATION\" (usually denoting the beginning of the document)   and the title of the following detected section</li> <li>this matcher works well for hospitalization summaries (CRH), but not necessarily   for all types of documents (in particular for emergency or scan summaries   CR-IMAGERIE)</li> </ul> <p>Experimental</p> <p>Should you rely on <code>eds.sections</code> for critical downstream tasks, make sure to validate the results to make sure that the component works in your case.</p>"},{"location":"reference/edsnlp/pipes/misc/sections/factory/#edsnlp.pipes.misc.sections.factory.create_component--examples","title":"Examples","text":"<p>The following snippet detects section titles. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sections())\n\ntext = \"\"\"\nCRU du 10/09/2021\nMotif :\nPatient admis pour suspicion de COVID\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.spans[\"section_titles\"]\n# Out: [Motif]\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/sections/factory/#edsnlp.pipes.misc.sections.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.sections</code> matcher adds two fields to the <code>doc.spans</code> attribute :</p> <ol> <li>The <code>section_titles</code> key contains the list of all section titles extracted using    the list declared in the <code>terms.py</code> module.</li> <li>The <code>sections</code> key contains a list of sections, ie spans of text between two    section titles (or the last title and the end of the document).</li> </ol> <p>If the document has entities before calling this matcher an attribute <code>section</code> is added to each entity.</p>"},{"location":"reference/edsnlp/pipes/misc/sections/factory/#edsnlp.pipes.misc.sections.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>sections</code> <p>Dictionary of terms to look for.</p> <p> TYPE: <code>Dict[str, List[str]]</code> DEFAULT: <code>{'allergies': ['allergies'], 'ant\u00e9c\u00e9dents': ['a...</code> </p> <code>attr</code> <p>Default attribute to match on.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>add_patterns</code> <p>Whether add update patterns to match start / end of lines</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/misc/sections/factory/#edsnlp.pipes.misc.sections.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.sections</code> matcher was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/sections/patterns/","title":"<code>edsnlp.pipes.misc.sections.patterns</code>","text":"<p>These section titles were extracted from a work performed by Ivan Lerner at AP-HP. It supplied a number of documents annotated for section titles.</p> <p>The section titles were reviewed by Gilles Chatellier, who gave meaningful insights.</p> <p>See sections/section-dataset notebook for detail.</p>"},{"location":"reference/edsnlp/pipes/misc/sections/sections/","title":"<code>edsnlp.pipes.misc.sections.sections</code>","text":""},{"location":"reference/edsnlp/pipes/misc/sections/sections/#edsnlp.pipes.misc.sections.sections.SectionsMatcher","title":"<code>SectionsMatcher</code>","text":"<p>           Bases: <code>GenericMatcher</code></p> <p>The <code>eds.sections</code> component extracts section titles from clinical documents. A \"section\" is then defined as the span of text between two titles.</p> <p>Here is the list of sections that are currently targeted :</p> <ul> <li><code>allergies</code></li> <li><code>ant\u00e9c\u00e9dents</code></li> <li><code>ant\u00e9c\u00e9dents familiaux</code></li> <li><code>traitements entr\u00e9e</code></li> <li><code>conclusion</code></li> <li><code>conclusion entr\u00e9e</code></li> <li><code>habitus</code></li> <li><code>correspondants</code></li> <li><code>diagnostic</code></li> <li><code>donn\u00e9es biom\u00e9triques entr\u00e9e</code></li> <li><code>examens</code></li> <li><code>examens compl\u00e9mentaires</code></li> <li><code>facteurs de risques</code></li> <li><code>histoire de la maladie</code></li> <li><code>actes</code></li> <li><code>motif</code></li> <li><code>prescriptions</code></li> <li><code>traitements sortie</code></li> <li><code>evolution</code></li> <li><code>modalites sortie</code></li> <li><code>vaccinations</code></li> <li><code>introduction</code></li> </ul> <p>Remarks :</p> <ul> <li>section <code>introduction</code> corresponds to the span of text between the header   \"COMPTE RENDU D'HOSPITALISATION\" (usually denoting the beginning of the document)   and the title of the following detected section</li> <li>this matcher works well for hospitalization summaries (CRH), but not necessarily   for all types of documents (in particular for emergency or scan summaries   CR-IMAGERIE)</li> </ul> <p>Experimental</p> <p>Should you rely on <code>eds.sections</code> for critical downstream tasks, make sure to validate the results to make sure that the component works in your case.</p>"},{"location":"reference/edsnlp/pipes/misc/sections/sections/#edsnlp.pipes.misc.sections.sections.SectionsMatcher--examples","title":"Examples","text":"<p>The following snippet detects section titles. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sections())\n\ntext = \"\"\"\nCRU du 10/09/2021\nMotif :\nPatient admis pour suspicion de COVID\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.spans[\"section_titles\"]\n# Out: [Motif]\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/sections/sections/#edsnlp.pipes.misc.sections.sections.SectionsMatcher--extensions","title":"Extensions","text":"<p>The <code>eds.sections</code> matcher adds two fields to the <code>doc.spans</code> attribute :</p> <ol> <li>The <code>section_titles</code> key contains the list of all section titles extracted using    the list declared in the <code>terms.py</code> module.</li> <li>The <code>sections</code> key contains a list of sections, ie spans of text between two    section titles (or the last title and the end of the document).</li> </ol> <p>If the document has entities before calling this matcher an attribute <code>section</code> is added to each entity.</p>"},{"location":"reference/edsnlp/pipes/misc/sections/sections/#edsnlp.pipes.misc.sections.sections.SectionsMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>sections</code> <p>Dictionary of terms to look for.</p> <p> TYPE: <code>Dict[str, List[str]]</code> DEFAULT: <code>{'allergies': ['allergies'], 'ant\u00e9c\u00e9dents': ['a...</code> </p> <code>attr</code> <p>Default attribute to match on.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>add_patterns</code> <p>Whether add update patterns to match start / end of lines</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/misc/sections/sections/#edsnlp.pipes.misc.sections.sections.SectionsMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.sections</code> matcher was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/sections/sections/#edsnlp.pipes.misc.sections.sections.SectionsMatcher.__call__","title":"<code>__call__</code>","text":"<p>Divides the doc into sections</p>"},{"location":"reference/edsnlp/pipes/misc/sections/sections/#edsnlp.pipes.misc.sections.sections.SectionsMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for sections</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/misc/split/","title":"<code>edsnlp.pipes.misc.split</code>","text":""},{"location":"reference/edsnlp/pipes/misc/split/split/","title":"<code>edsnlp.pipes.misc.split.split</code>","text":""},{"location":"reference/edsnlp/pipes/misc/split/split/#edsnlp.pipes.misc.split.split.Split","title":"<code>Split</code>","text":"<p>The <code>eds.split</code> component splits a document into multiple documents based on a regex pattern or a maximum length.</p> <p>Not for pipelines</p> <p>This component is not meant to be used in a pipeline, but rather as a preprocessing step when dealing with a stream of documents as in the example below.</p>"},{"location":"reference/edsnlp/pipes/misc/split/split/#edsnlp.pipes.misc.split.split.Split--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\n# Create the stream\nstream = edsnlp.data.from_iterable(\n    [\"Sentence 1\\n\\nThis is another longer sentence more than 5 words\"]\n)\n\n# Convert texts into docs\nstream = stream.map_pipeline(edsnlp.blank(\"eds\"))\n\n# Apply the split component\nstream = stream.map(eds.split(max_length=5, regex=\"\\n{2,}\"))\n\nprint(\" | \".join(doc.text.strip() for doc in stream))\n# Out: Sentence 1 | This is another longer sentence | more than 5 words\n</code></pre>"},{"location":"reference/edsnlp/pipes/misc/split/split/#edsnlp.pipes.misc.split.split.Split--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[Pipeline]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The component name</p> <p> TYPE: <code>str</code> DEFAULT: <code>'split'</code> </p> <code>max_length</code> <p>The maximum length of the produced documents. If 0, the document will not be split based on length.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>regex</code> <p>The regex pattern to split the document on</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'\\n{2,}'</code> </p> <code>filter_expr</code> <p>An optional filter expression to filter the produced documents. The callable expects a single <code>doc</code> argument, the new Doc, and should return a boolean. For example, to filter out documents with less than 5 tokens, you can use <code>filter_expr=\"len(doc) &gt;= 5\"</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>randomize</code> <p>The randomization factor to split the documents, to avoid producing documents that are all <code>max_length</code> tokens long (0 means all documents will have the maximum possible length while 1 will produce documents with a length varying between 0 and <code>max_length</code> uniformly)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p>"},{"location":"reference/edsnlp/pipes/misc/split/split/#edsnlp.pipes.misc.split.split.Split.split_doc","title":"<code>split_doc</code>","text":"<p>Split a doc into multiple docs of max_length tokens.</p>"},{"location":"reference/edsnlp/pipes/misc/split/split/#edsnlp.pipes.misc.split.split.Split.split_doc--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>The doc to split</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Iterable[Doc]</code>"},{"location":"reference/edsnlp/pipes/misc/split/split/#edsnlp.pipes.misc.split.split.subset_doc","title":"<code>subset_doc</code>","text":"<p>Subset a doc given a start and end index.</p>"},{"location":"reference/edsnlp/pipes/misc/split/split/#edsnlp.pipes.misc.split.split.subset_doc--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>The doc to subset</p> <p> TYPE: <code>Doc</code> </p> <code>start</code> <p>The start index</p> <p> TYPE: <code>int</code> </p> <code>end</code> <p>The end index</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Doc</code>"},{"location":"reference/edsnlp/pipes/misc/tables/","title":"<code>edsnlp.pipes.misc.tables</code>","text":""},{"location":"reference/edsnlp/pipes/misc/tables/factory/","title":"<code>edsnlp.pipes.misc.tables.factory</code>","text":""},{"location":"reference/edsnlp/pipes/misc/tables/factory/#edsnlp.pipes.misc.tables.factory.create_component","title":"<code>create_component = registry.factory.register('eds.tables', assigns=['doc.spans', 'doc.ents'], deprecated=['tables'])(TablesMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.tables</code> matcher detects tables in a documents.</p>"},{"location":"reference/edsnlp/pipes/misc/tables/factory/#edsnlp.pipes.misc.tables.factory.create_component--examples","title":"Examples","text":"<p><pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.tables())\n\ntext = \"\"\"\nSERVICE\nMEDECINE INTENSIVE \u2013\nREANIMATION\nR\u00e9animation / Surveillance Continue\nM\u00e9dicale\n\nCOMPTE RENDU D'HOSPITALISATION du 05/06/2020 au 10/06/2020\nMadame DUPONT Marie, n\u00e9e le 16/05/1900, \u00e2g\u00e9e de 20 ans, a \u00e9t\u00e9 hospitalis\u00e9e en\nr\u00e9animation du 05/06/1920 au 10/06/1920 pour intoxication m\u00e9dicamenteuse volontaire.\n\nExamens compl\u00e9mentaires\nH\u00e9matologie\nNum\u00e9ration\nLeucocytes \u00a6x10*9/L \u00a64.97 \u00a64.09-11\nH\u00e9maties \u00a6x10*12/L\u00a64.68 \u00a64.53-5.79\nH\u00e9moglobine \u00a6g/dL \u00a614.8 \u00a613.4-16.7\nH\u00e9matocrite \u00a6% \u00a644.2 \u00a639.2-48.6\nVGM \u00a6fL \u00a694.4 + \u00a679.6-94\nTCMH \u00a6pg \u00a631.6 \u00a627.3-32.8\nCCMH \u00a6g/dL \u00a633.5 \u00a632.4-36.3\nPlaquettes \u00a6x10*9/L \u00a6191 \u00a6172-398\nVMP \u00a6fL \u00a611.5 + \u00a67.4-10.8\n\nSur le plan neurologique : Devant la persistance d'une confusion \u00e0 distance de\nl'intoxication au\n...\n\n2/2Pat : &lt;NOM&gt; &lt;Prenom&gt;|F |&lt;date&gt; | &lt;ipp&gt; |Intitul\u00e9 RCP\n\"\"\"\n\ndoc = nlp(text)\n\n# A table span\ntable = doc.spans[\"tables\"][0]\n\n# Leucocytes \u00a6x10*9/L \u00a64.97 \u00a64.09-11\n# H\u00e9maties \u00a6x10*12/L\u00a64.68 \u00a64.53-5.79\n# H\u00e9moglobine \u00a6g/dL \u00a614.8 \u00a613.4-16.7\n# H\u00e9matocrite \u00a6% \u00a644.2 \u00a639.2-48.6\n# VGM \u00a6fL \u00a694.4 + \u00a679.6-94\n# TCMH \u00a6pg \u00a631.6 \u00a627.3-32.8\n# CCMH \u00a6g/dL \u00a633.5 \u00a632.4-36.3\n# Plaquettes \u00a6x10*9/L \u00a6191 \u00a6172-398\n# VMP \u00a6fL \u00a611.5 + \u00a67.4-10.8\n\n# Convert span to Pandas table\ndf = table._.to_pd_table(\n    as_spans=False,  # set True to set the table cells as spans instead of strings\n    header=False,  # set True to use the first row as header\n    index=False,  # set True to use the first column as index\n)\ntype(df)\n# Out: &lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> The pandas DataFrame:</p> 0 1 2 3 0 Leucocytes x10*9/L 4.97 4.09-11 1 H\u00e9maties x10*12/L 4.68 4.53-5.79 2 H\u00e9moglobine g/dL 14.8 13.4-16.7 3 H\u00e9matocrite % 44.2 39.2-48.6 4 VGM fL 94.4 + 79.6-94 5 TCMH pg 31.6 27.3-32.8 6 CCMH g/dL 33.5 32.4-36.3 7 Plaquettes x10*9/L 191 172-398 8 VMP fL 11.5 + 7.4-10.8"},{"location":"reference/edsnlp/pipes/misc/tables/factory/#edsnlp.pipes.misc.tables.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.tables</code> pipeline declares the <code>span._.to_pd_table()</code> Span extension. This function returns a parsed pandas version of the table.</p>"},{"location":"reference/edsnlp/pipes/misc/tables/factory/#edsnlp.pipes.misc.tables.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the component.</p> <p> </p> <code>tables_pattern</code> <p>The regex pattern to identify tables. The key of dictionary should be <code>tables</code></p> <p> TYPE: <code>Optional[Dict[str, str]]</code> DEFAULT: <code>None</code> </p> <code>sep_pattern</code> <p>The regex pattern to identify the separator pattern. Used when calling <code>to_pd_table</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>min_rows</code> <p>Only tables with more then <code>min_rows</code> lines will be detected.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>2</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'. We can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/misc/tables/factory/#edsnlp.pipes.misc.tables.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.tables</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/tables/patterns/","title":"<code>edsnlp.pipes.misc.tables.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/misc/tables/tables/","title":"<code>edsnlp.pipes.misc.tables.tables</code>","text":""},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher","title":"<code>TablesMatcher</code>","text":"<p>           Bases: <code>BaseComponent</code></p> <p>The <code>eds.tables</code> matcher detects tables in a documents.</p>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher--examples","title":"Examples","text":"<p><pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.tables())\n\ntext = \"\"\"\nSERVICE\nMEDECINE INTENSIVE \u2013\nREANIMATION\nR\u00e9animation / Surveillance Continue\nM\u00e9dicale\n\nCOMPTE RENDU D'HOSPITALISATION du 05/06/2020 au 10/06/2020\nMadame DUPONT Marie, n\u00e9e le 16/05/1900, \u00e2g\u00e9e de 20 ans, a \u00e9t\u00e9 hospitalis\u00e9e en\nr\u00e9animation du 05/06/1920 au 10/06/1920 pour intoxication m\u00e9dicamenteuse volontaire.\n\nExamens compl\u00e9mentaires\nH\u00e9matologie\nNum\u00e9ration\nLeucocytes \u00a6x10*9/L \u00a64.97 \u00a64.09-11\nH\u00e9maties \u00a6x10*12/L\u00a64.68 \u00a64.53-5.79\nH\u00e9moglobine \u00a6g/dL \u00a614.8 \u00a613.4-16.7\nH\u00e9matocrite \u00a6% \u00a644.2 \u00a639.2-48.6\nVGM \u00a6fL \u00a694.4 + \u00a679.6-94\nTCMH \u00a6pg \u00a631.6 \u00a627.3-32.8\nCCMH \u00a6g/dL \u00a633.5 \u00a632.4-36.3\nPlaquettes \u00a6x10*9/L \u00a6191 \u00a6172-398\nVMP \u00a6fL \u00a611.5 + \u00a67.4-10.8\n\nSur le plan neurologique : Devant la persistance d'une confusion \u00e0 distance de\nl'intoxication au\n...\n\n2/2Pat : &lt;NOM&gt; &lt;Prenom&gt;|F |&lt;date&gt; | &lt;ipp&gt; |Intitul\u00e9 RCP\n\"\"\"\n\ndoc = nlp(text)\n\n# A table span\ntable = doc.spans[\"tables\"][0]\n\n# Leucocytes \u00a6x10*9/L \u00a64.97 \u00a64.09-11\n# H\u00e9maties \u00a6x10*12/L\u00a64.68 \u00a64.53-5.79\n# H\u00e9moglobine \u00a6g/dL \u00a614.8 \u00a613.4-16.7\n# H\u00e9matocrite \u00a6% \u00a644.2 \u00a639.2-48.6\n# VGM \u00a6fL \u00a694.4 + \u00a679.6-94\n# TCMH \u00a6pg \u00a631.6 \u00a627.3-32.8\n# CCMH \u00a6g/dL \u00a633.5 \u00a632.4-36.3\n# Plaquettes \u00a6x10*9/L \u00a6191 \u00a6172-398\n# VMP \u00a6fL \u00a611.5 + \u00a67.4-10.8\n\n# Convert span to Pandas table\ndf = table._.to_pd_table(\n    as_spans=False,  # set True to set the table cells as spans instead of strings\n    header=False,  # set True to use the first row as header\n    index=False,  # set True to use the first column as index\n)\ntype(df)\n# Out: &lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> The pandas DataFrame:</p> 0 1 2 3 0 Leucocytes x10*9/L 4.97 4.09-11 1 H\u00e9maties x10*12/L 4.68 4.53-5.79 2 H\u00e9moglobine g/dL 14.8 13.4-16.7 3 H\u00e9matocrite % 44.2 39.2-48.6 4 VGM fL 94.4 + 79.6-94 5 TCMH pg 31.6 27.3-32.8 6 CCMH g/dL 33.5 32.4-36.3 7 Plaquettes x10*9/L 191 172-398 8 VMP fL 11.5 + 7.4-10.8"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher--extensions","title":"Extensions","text":"<p>The <code>eds.tables</code> pipeline declares the <code>span._.to_pd_table()</code> Span extension. This function returns a parsed pandas version of the table.</p>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the component.</p> <p> </p> <code>tables_pattern</code> <p>The regex pattern to identify tables. The key of dictionary should be <code>tables</code></p> <p> TYPE: <code>Optional[Dict[str, str]]</code> DEFAULT: <code>None</code> </p> <code>sep_pattern</code> <p>The regex pattern to identify the separator pattern. Used when calling <code>to_pd_table</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>min_rows</code> <p>Only tables with more then <code>min_rows</code> lines will be detected.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>2</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'. We can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.tables</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher.set_extensions","title":"<code>set_extensions</code>  <code>classmethod</code>","text":"<p>Set extensions for the tables pipeline.</p>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher.get_table","title":"<code>get_table</code>","text":"<p>Convert spans of tables to dictionaries</p>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher.get_table--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>table</code> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>List[Span]</code>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher.__call__","title":"<code>__call__</code>","text":"<p>Find spans that contain tables</p>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher.to_pd_table","title":"<code>to_pd_table</code>","text":"<p>Return pandas DataFrame</p>"},{"location":"reference/edsnlp/pipes/misc/tables/tables/#edsnlp.pipes.misc.tables.tables.TablesMatcher.to_pd_table--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span</code> <p>The span containing the table</p> <p> TYPE: <code>Span</code> </p> <code>as_spans</code> <p>Whether to return the table cells as spans</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>header</code> <p>Whether the table has a header</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>index</code> <p>Whether the table has an index</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/ner/","title":"<code>edsnlp.pipes.ner</code>","text":""},{"location":"reference/edsnlp/pipes/ner/adicap/","title":"<code>edsnlp.pipes.ner.adicap</code>","text":""},{"location":"reference/edsnlp/pipes/ner/adicap/adicap/","title":"<code>edsnlp.pipes.ner.adicap.adicap</code>","text":"<p><code>eds.adicap</code> pipeline</p> <ol><li><p><p>sant\u00e9 A., 2019. Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions. http://esante.gouv.fr/terminologie-adicap</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/adicap/adicap/#edsnlp.pipes.ner.adicap.adicap.AdicapMatcher","title":"<code>AdicapMatcher</code>","text":"<p>           Bases: <code>ContextualMatcher</code></p> <p>The <code>eds.adicap</code> pipeline component matches the ADICAP codes. It was developped to run on anapathology reports.</p> <p>Document type</p> <p>It was developped to work on anapathology reports. We recommend also to use the <code>eds</code> language (<code>edsnlp.blank(\"eds\")</code>)</p> <p>The compulsory characters of the ADICAP code are identified and decoded. These characters represent the following attributes:</p> Field [en] Field [fr] Attribute Sampling mode Mode de prelevement sampling_mode Technic Type de technique technic Organ and regions Appareils, organes et r\u00e9gions organ Pathology Pathologie g\u00e9n\u00e9rale pathology Pathology type Type de la pathologie pathology_type Behaviour type Type de comportement behaviour_type <p>The pathology field takes 4 different values corresponding to the 4 possible interpretations of the ADICAP code, which are : \"PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE\", \"PATHOLOGIE TUMORALE\", \"PATHOLOGIE PARTICULIERE DES ORGANES\" and \"CYTOPATHOLOGIE\".</p> <p>Depending on the pathology value the behaviour type meaning changes, when the pathology is tumoral then it describes the malignancy of the tumor.</p> <p>For further details about the ADICAP code follow this link.</p>"},{"location":"reference/edsnlp/pipes/ner/adicap/adicap/#edsnlp.pipes.ner.adicap.adicap.AdicapMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.adicap())\n\ntext = \"\"\"\nCOMPTE RENDU D\u2019EXAMEN\n\nAnt\u00e9riorit\u00e9(s) :  NEANT\n\n\nRenseignements cliniques :\nContexte d'exploration d'un carcinome canalaire infiltrant du quadrant sup\u00e9ro-\nexterne du sein droit. La l\u00e9sion biopsi\u00e9e ce jour est situ\u00e9e \u00e0 5,5 cm de la l\u00e9sion\ndu quadrant sup\u00e9ro-externe, \u00e0 l'union des quadrants inf\u00e9rieurs.\n\n\nMacrobiopsie 10G sur une zone de prise de contraste focale \u00e0 l'union des quadrants\ninf\u00e9rieurs du sein droit, mesurant 4 mm, class\u00e9e ACR4\n\n14 fragments ont \u00e9t\u00e9 communiqu\u00e9s fix\u00e9s en formol (lame n\u00b0 1a et lame n\u00b0 1b) . Il\nn'y a pas eu d'\u00e9chantillon congel\u00e9. Ces fragments ont \u00e9t\u00e9 inclus en paraffine en\ntotalit\u00e9 et coup\u00e9s sur plusieurs niveaux.\nHistologiquement, il s'agit d'un parenchyme mammaire fibroadipeux parfois\nl\u00e9g\u00e8rement dystrophique avec quelques petits kystes. Il n'y a pas d'hyperplasie\n\u00e9pith\u00e9liale, pas d'atypie, pas de prolif\u00e9ration tumorale. On note quelques\nsuffusions h\u00e9morragiques focales.\n\nConclusion :\nL\u00e9gers remaniements dystrophiques \u00e0 l'union des quadrants inf\u00e9rieurs du sein droit.\nAbsence d'atypies ou de prolif\u00e9ration tumorale.\n\nCodification :   BHGS0040\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (BHGS0040,)\n\nent = doc.ents[0]\n\nent.label_\n# Out: adicap\n\nent._.adicap.dict()\n# Out: {'code': 'BHGS0040',\n# 'sampling_mode': 'BIOPSIE CHIRURGICALE',\n# 'technic': 'HISTOLOGIE ET CYTOLOGIE PAR INCLUSION',\n# 'organ': \"SEIN (\u00c9GALEMENT UTILIS\u00c9 CHEZ L'HOMME)\",\n# 'pathology': 'PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE',\n# 'pathology_type': 'ETAT SUBNORMAL - LESION MINEURE',\n# 'behaviour_type': 'CARACTERES GENERAUX'}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/adicap/adicap/#edsnlp.pipes.ner.adicap.adicap.AdicapMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'adicap'</code> </p> <code>pattern</code> <p>The regex pattern to use for matching ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>([A-Z]\\.?[A-Z]\\.?[A-Z]{2}\\.?(?:\\d{4}|\\d{4}|[A-Z...</code> </p> <code>prefix</code> <p>The regex pattern to use for matching the prefix before ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>(?i)(codification|adicap)</code> </p> <code>window</code> <p>Number of tokens to look for prefix. It will never go further the start of the sentence</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>adicap</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'adicap': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/adicap/adicap/#edsnlp.pipes.ner.adicap.adicap.AdicapMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.adicap</code> pipeline was developed by AP-HP's Data Science team. The codes were downloaded from the website of 'Agence du num\u00e9rique en sant\u00e9' (\"Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions\", sant\u00e9, 2019)</p>"},{"location":"reference/edsnlp/pipes/ner/adicap/adicap/#edsnlp.pipes.ner.adicap.adicap.AdicapMatcher.process","title":"<code>process</code>","text":"<p>Tags ADICAP mentions.</p>"},{"location":"reference/edsnlp/pipes/ner/adicap/adicap/#edsnlp.pipes.ner.adicap.adicap.AdicapMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for ADICAP</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/ner/adicap/factory/","title":"<code>edsnlp.pipes.ner.adicap.factory</code>","text":"<ol><li><p><p>sant\u00e9 A., 2019. Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions. http://esante.gouv.fr/terminologie-adicap</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/adicap/factory/#edsnlp.pipes.ner.adicap.factory.create_component","title":"<code>create_component = registry.factory.register('eds.adicap', assigns=['doc.ents', 'doc.spans'])(AdicapMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.adicap</code> pipeline component matches the ADICAP codes. It was developped to run on anapathology reports.</p> <p>Document type</p> <p>It was developped to work on anapathology reports. We recommend also to use the <code>eds</code> language (<code>edsnlp.blank(\"eds\")</code>)</p> <p>The compulsory characters of the ADICAP code are identified and decoded. These characters represent the following attributes:</p> Field [en] Field [fr] Attribute Sampling mode Mode de prelevement sampling_mode Technic Type de technique technic Organ and regions Appareils, organes et r\u00e9gions organ Pathology Pathologie g\u00e9n\u00e9rale pathology Pathology type Type de la pathologie pathology_type Behaviour type Type de comportement behaviour_type <p>The pathology field takes 4 different values corresponding to the 4 possible interpretations of the ADICAP code, which are : \"PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE\", \"PATHOLOGIE TUMORALE\", \"PATHOLOGIE PARTICULIERE DES ORGANES\" and \"CYTOPATHOLOGIE\".</p> <p>Depending on the pathology value the behaviour type meaning changes, when the pathology is tumoral then it describes the malignancy of the tumor.</p> <p>For further details about the ADICAP code follow this link.</p>"},{"location":"reference/edsnlp/pipes/ner/adicap/factory/#edsnlp.pipes.ner.adicap.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.adicap())\n\ntext = \"\"\"\nCOMPTE RENDU D\u2019EXAMEN\n\nAnt\u00e9riorit\u00e9(s) :  NEANT\n\n\nRenseignements cliniques :\nContexte d'exploration d'un carcinome canalaire infiltrant du quadrant sup\u00e9ro-\nexterne du sein droit. La l\u00e9sion biopsi\u00e9e ce jour est situ\u00e9e \u00e0 5,5 cm de la l\u00e9sion\ndu quadrant sup\u00e9ro-externe, \u00e0 l'union des quadrants inf\u00e9rieurs.\n\n\nMacrobiopsie 10G sur une zone de prise de contraste focale \u00e0 l'union des quadrants\ninf\u00e9rieurs du sein droit, mesurant 4 mm, class\u00e9e ACR4\n\n14 fragments ont \u00e9t\u00e9 communiqu\u00e9s fix\u00e9s en formol (lame n\u00b0 1a et lame n\u00b0 1b) . Il\nn'y a pas eu d'\u00e9chantillon congel\u00e9. Ces fragments ont \u00e9t\u00e9 inclus en paraffine en\ntotalit\u00e9 et coup\u00e9s sur plusieurs niveaux.\nHistologiquement, il s'agit d'un parenchyme mammaire fibroadipeux parfois\nl\u00e9g\u00e8rement dystrophique avec quelques petits kystes. Il n'y a pas d'hyperplasie\n\u00e9pith\u00e9liale, pas d'atypie, pas de prolif\u00e9ration tumorale. On note quelques\nsuffusions h\u00e9morragiques focales.\n\nConclusion :\nL\u00e9gers remaniements dystrophiques \u00e0 l'union des quadrants inf\u00e9rieurs du sein droit.\nAbsence d'atypies ou de prolif\u00e9ration tumorale.\n\nCodification :   BHGS0040\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (BHGS0040,)\n\nent = doc.ents[0]\n\nent.label_\n# Out: adicap\n\nent._.adicap.dict()\n# Out: {'code': 'BHGS0040',\n# 'sampling_mode': 'BIOPSIE CHIRURGICALE',\n# 'technic': 'HISTOLOGIE ET CYTOLOGIE PAR INCLUSION',\n# 'organ': \"SEIN (\u00c9GALEMENT UTILIS\u00c9 CHEZ L'HOMME)\",\n# 'pathology': 'PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE',\n# 'pathology_type': 'ETAT SUBNORMAL - LESION MINEURE',\n# 'behaviour_type': 'CARACTERES GENERAUX'}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/adicap/factory/#edsnlp.pipes.ner.adicap.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'adicap'</code> </p> <code>pattern</code> <p>The regex pattern to use for matching ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>([A-Z]\\.?[A-Z]\\.?[A-Z]{2}\\.?(?:\\d{4}|\\d{4}|[A-Z...</code> </p> <code>prefix</code> <p>The regex pattern to use for matching the prefix before ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>(?i)(codification|adicap)</code> </p> <code>window</code> <p>Number of tokens to look for prefix. It will never go further the start of the sentence</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>adicap</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'adicap': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/adicap/factory/#edsnlp.pipes.ner.adicap.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.adicap</code> pipeline was developed by AP-HP's Data Science team. The codes were downloaded from the website of 'Agence du num\u00e9rique en sant\u00e9' (\"Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions\", sant\u00e9, 2019)</p>"},{"location":"reference/edsnlp/pipes/ner/adicap/models/","title":"<code>edsnlp.pipes.ner.adicap.models</code>","text":""},{"location":"reference/edsnlp/pipes/ner/adicap/patterns/","title":"<code>edsnlp.pipes.ner.adicap.patterns</code>","text":"<p>Source : https://esante.gouv.fr/sites/default/files/media_entity/documents/cgts_sem_adicap_fiche-detaillee.pdf</p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/","title":"<code>edsnlp.pipes.ner.behaviors</code>","text":""},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/","title":"<code>edsnlp.pipes.ner.behaviors.alcohol</code>","text":""},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/alcohol/","title":"<code>edsnlp.pipes.ner.behaviors.alcohol.alcohol</code>","text":"<p><code>eds.alcohol</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.alcohol.AlcoholMatcher","title":"<code>AlcoholMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.alcohol</code> pipeline component extracts mentions of alcohol consumption. It won't match occasional consumption, nor acute intoxication.</p> Details of the used patterns <pre><code># fmt: off\ndefault_pattern = dict(\n    source=\"alcohol\",\n    regex=[\n        r\"\\balco[ol]\",\n        r\"\\bethyl\",\n        r\"(?&lt;!(25.{0,10}))\\boh\\b\",\n        r\"exogenose\",\n        r\"delirium.tremens\",\n    ],\n    exclude=[\n        dict(\n            regex=[\n                \"occasion\",\n                \"episod\",\n                \"festi\",\n                \"rare\",\n                \"libre\",  # OH-libres\n                \"aigu\",\n            ],\n            window=(-3, 5),\n        ),\n        dict(\n            regex=[\"pansement\", \"compress\"],\n            window=-3,\n        ),\n    ],\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"stopped\",\n            regex=r\"(\\bex\\b|sevr|arret|stop|ancien)\",\n            window=(-3, 15),\n            reduce_mode=\"keep_first\",\n        ),\n        dict(\n            name=\"zero_after\",\n            regex=r\"(?=^[a-z]*\\s*:?[\\s-]*(0|non|aucun|jamais))\",\n            window=3,\n            reduce_mode=\"keep_first\",\n        ),\n    ],\n)\ndefault_patterns = [default_pattern]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.alcohol.AlcoholMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: either None or <code>\"ABSTINENCE\"</code> if the patient stopped its consumption</li> <li><code>span._.negation</code>: set to True when a mention such as \"alcool: 0\" is found</li> </ul> <p>Use qualifiers !</p> <p>Although the alcohol pipe sometime sets value for the <code>negation</code> attribute, generic qualifier should still be used after the pipe.</p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.alcohol.AlcoholMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.alcohol())\n</code></pre> <p>Below are a few examples:</p> 12345678 <pre><code>text = \"Patient alcoolique.\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [alcoolique]\n</code></pre> <pre><code>text = \"OH chronique.\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [OH]\n</code></pre> <pre><code>text = \"Prise d'alcool occasionnelle\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Application d'un pansement alcoolis\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Alcoolisme sevr\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcoolisme sevr\u00e9]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': sevr\u00e9}\n</code></pre> <pre><code>text = \"Alcoolisme non sevr\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcoolisme non sevr\u00e9]\n\nspan = spans[0]\n\nspan._.detailed_status  # \"sevr\u00e9\" is negated, so no \"ABTINENCE\" status\n# Out: None\n</code></pre> <pre><code>text = \"Alcool: 0\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcool]\n\nspan = spans[0]\n\nspan._.negation\n# Out: True\n\nspan._.assigned\n# Out: {'zero_after': 0}\n</code></pre> <pre><code>text = \"Le patient est en cours de sevrage \u00e9thylotabagique\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [sevrage \u00e9thylotabagique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': sevrage}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.alcohol.AlcoholMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'alcohol', 'regex': ['\\\\balco[ol]',...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>alcohol</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'alcohol': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/alcohol/#edsnlp.pipes.ner.behaviors.alcohol.alcohol.AlcoholMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.alcohol</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/factory/","title":"<code>edsnlp.pipes.ner.behaviors.alcohol.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/factory/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component","title":"<code>create_component = registry.factory.register('eds.alcohol', assigns=['doc.ents', 'doc.spans'])(AlcoholMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.alcohol</code> pipeline component extracts mentions of alcohol consumption. It won't match occasional consumption, nor acute intoxication.</p> Details of the used patterns <pre><code># fmt: off\ndefault_pattern = dict(\n    source=\"alcohol\",\n    regex=[\n        r\"\\balco[ol]\",\n        r\"\\bethyl\",\n        r\"(?&lt;!(25.{0,10}))\\boh\\b\",\n        r\"exogenose\",\n        r\"delirium.tremens\",\n    ],\n    exclude=[\n        dict(\n            regex=[\n                \"occasion\",\n                \"episod\",\n                \"festi\",\n                \"rare\",\n                \"libre\",  # OH-libres\n                \"aigu\",\n            ],\n            window=(-3, 5),\n        ),\n        dict(\n            regex=[\"pansement\", \"compress\"],\n            window=-3,\n        ),\n    ],\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"stopped\",\n            regex=r\"(\\bex\\b|sevr|arret|stop|ancien)\",\n            window=(-3, 15),\n            reduce_mode=\"keep_first\",\n        ),\n        dict(\n            name=\"zero_after\",\n            regex=r\"(?=^[a-z]*\\s*:?[\\s-]*(0|non|aucun|jamais))\",\n            window=3,\n            reduce_mode=\"keep_first\",\n        ),\n    ],\n)\ndefault_patterns = [default_pattern]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/factory/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: either None or <code>\"ABSTINENCE\"</code> if the patient stopped its consumption</li> <li><code>span._.negation</code>: set to True when a mention such as \"alcool: 0\" is found</li> </ul> <p>Use qualifiers !</p> <p>Although the alcohol pipe sometime sets value for the <code>negation</code> attribute, generic qualifier should still be used after the pipe.</p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/factory/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.alcohol())\n</code></pre> <p>Below are a few examples:</p> 12345678 <pre><code>text = \"Patient alcoolique.\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [alcoolique]\n</code></pre> <pre><code>text = \"OH chronique.\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [OH]\n</code></pre> <pre><code>text = \"Prise d'alcool occasionnelle\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Application d'un pansement alcoolis\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Alcoolisme sevr\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcoolisme sevr\u00e9]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': sevr\u00e9}\n</code></pre> <pre><code>text = \"Alcoolisme non sevr\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcoolisme non sevr\u00e9]\n\nspan = spans[0]\n\nspan._.detailed_status  # \"sevr\u00e9\" is negated, so no \"ABTINENCE\" status\n# Out: None\n</code></pre> <pre><code>text = \"Alcool: 0\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [Alcool]\n\nspan = spans[0]\n\nspan._.negation\n# Out: True\n\nspan._.assigned\n# Out: {'zero_after': 0}\n</code></pre> <pre><code>text = \"Le patient est en cours de sevrage \u00e9thylotabagique\"\ndoc = nlp(text)\nspans = doc.spans[\"alcohol\"]\n\nspans\n# Out: [sevrage \u00e9thylotabagique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': sevrage}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/factory/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'alcohol', 'regex': ['\\\\balco[ol]',...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>alcohol</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'alcohol': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/factory/#edsnlp.pipes.ner.behaviors.alcohol.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.alcohol</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/alcohol/patterns/","title":"<code>edsnlp.pipes.ner.behaviors.alcohol.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/","title":"<code>edsnlp.pipes.ner.behaviors.tobacco</code>","text":""},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/factory/","title":"<code>edsnlp.pipes.ner.behaviors.tobacco.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/factory/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component","title":"<code>create_component = registry.factory.register('eds.tobacco', assigns=['doc.ents', 'doc.spans'])(TobaccoMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.tobacco</code> pipeline component extracts mentions of tobacco consumption.</p> Details of the used patterns <pre><code># fmt: off\nPA = r\"(?:\\bp/?a\\b|paquets?.?annee)\"\nQUANTITY = r\"(?P&lt;quantity&gt;[\\d]{1,3})\"\nPUNCT = r\"\\.,-;\\(\\)\"\n\ndefault_patterns = [\n    dict(\n        source=\"tobacco\",\n        regex=[\n            r\"tabagi\",\n            r\"tabac\",\n            r\"\\bfume\\b\",\n            r\"\\bfumeu\",\n            r\"\\bpipes?\\b\",\n        ],\n        exclude=dict(\n            regex=[\n                \"occasion\",\n                \"moder\",\n                \"quelqu\",\n                \"festi\",\n                \"rare\",\n                \"sujet\",  # Example : Chez le sujet fumeur ... generic sentences\n            ],\n            window=(-3, 5),\n        ),\n        regex_attr=\"NORM\",\n        assign=[\n            dict(\n                name=\"stopped\",\n                regex=r\"(\\bex\\b|sevr|arret|stop|ancien)\",\n                window=(-3, 15),\n                reduce_mode=\"keep_first\",\n            ),\n            dict(\n                name=\"zero_after\",\n                regex=r\"(?=^[a-z]*\\s*:?[\\s-]*(0|non|aucun|jamais))\",\n                window=3,\n                reduce_mode=\"keep_first\",\n            ),\n            dict(\n                name=\"PA\",\n                regex=rf\"{QUANTITY}[^{PUNCT}]{{0,10}}{PA}|{PA}[^{PUNCT}]{{0,10}}{QUANTITY}\",\n                window=(-10, 10),\n                reduce_mode=\"keep_first\",\n            ),\n            dict(\n                name=\"secondhand\",\n                regex=\"(passif)\",\n                window=5,\n                reduce_mode=\"keep_first\",\n            ),\n        ],\n    )\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/factory/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: either None or <code>\"ABSTINENCE\"</code> if the patient stopped its consumption</li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>PA</code>: the mentioned year-pack (= paquet-ann\u00e9e)</li> <li><code>secondhand</code>: if secondhand smoking</li> </ul> </li> <li><code>span._.negation</code>: set to True when either<ul> <li>A pack-year value of 0 is extracted</li> <li>A mention such as \"tabac: 0\" is found</li> <li>The patient experiences secondhand smoking</li> </ul> </li> </ul> <p>Use qualifiers !</p> <p>Although the tobacco pipe sometime sets value for the <code>negation</code> attribute, generic qualifier should still be used after the pipe.</p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/factory/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.tobacco())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Tabagisme \u00e9valu\u00e9 \u00e0 15 PA\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabagisme \u00e9valu\u00e9 \u00e0 15 PA]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'PA': 15}\n</code></pre> <pre><code>text = \"Patient tabagique\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [tabagique]\n</code></pre> <pre><code>text = \"Tabagisme festif\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"On a un tabagisme ancien\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [tabagisme ancien]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': ancien}\n</code></pre> <pre><code>text = \"Tabac: 0\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabac]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: None\n\nspan._.negation\n# Out: True\n\nspan._.assigned\n# Out: {'zero_after': 0}\n</code></pre> <pre><code>text = \"Tabagisme passif\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabagisme passif]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: None\n\nspan._.negation\n# Out: True\n\nspan._.assigned\n# Out: {'secondhand': passif}\n</code></pre> <pre><code>text = \"Tabac: sevr\u00e9 depuis 5 ans\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabac: sevr\u00e9]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': sevr\u00e9}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/factory/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'tobacco'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'tobacco', 'regex': ['tabagi', 'tab...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>tobacco</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'tobacco': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/factory/#edsnlp.pipes.ner.behaviors.tobacco.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.tobacco</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/patterns/","title":"<code>edsnlp.pipes.ner.behaviors.tobacco.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/tobacco/","title":"<code>edsnlp.pipes.ner.behaviors.tobacco.tobacco</code>","text":"<p><code>eds.tobacco</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.tobacco.TobaccoMatcher","title":"<code>TobaccoMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.tobacco</code> pipeline component extracts mentions of tobacco consumption.</p> Details of the used patterns <pre><code># fmt: off\nPA = r\"(?:\\bp/?a\\b|paquets?.?annee)\"\nQUANTITY = r\"(?P&lt;quantity&gt;[\\d]{1,3})\"\nPUNCT = r\"\\.,-;\\(\\)\"\n\ndefault_patterns = [\n    dict(\n        source=\"tobacco\",\n        regex=[\n            r\"tabagi\",\n            r\"tabac\",\n            r\"\\bfume\\b\",\n            r\"\\bfumeu\",\n            r\"\\bpipes?\\b\",\n        ],\n        exclude=dict(\n            regex=[\n                \"occasion\",\n                \"moder\",\n                \"quelqu\",\n                \"festi\",\n                \"rare\",\n                \"sujet\",  # Example : Chez le sujet fumeur ... generic sentences\n            ],\n            window=(-3, 5),\n        ),\n        regex_attr=\"NORM\",\n        assign=[\n            dict(\n                name=\"stopped\",\n                regex=r\"(\\bex\\b|sevr|arret|stop|ancien)\",\n                window=(-3, 15),\n                reduce_mode=\"keep_first\",\n            ),\n            dict(\n                name=\"zero_after\",\n                regex=r\"(?=^[a-z]*\\s*:?[\\s-]*(0|non|aucun|jamais))\",\n                window=3,\n                reduce_mode=\"keep_first\",\n            ),\n            dict(\n                name=\"PA\",\n                regex=rf\"{QUANTITY}[^{PUNCT}]{{0,10}}{PA}|{PA}[^{PUNCT}]{{0,10}}{QUANTITY}\",\n                window=(-10, 10),\n                reduce_mode=\"keep_first\",\n            ),\n            dict(\n                name=\"secondhand\",\n                regex=\"(passif)\",\n                window=5,\n                reduce_mode=\"keep_first\",\n            ),\n        ],\n    )\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.tobacco.TobaccoMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: either None or <code>\"ABSTINENCE\"</code> if the patient stopped its consumption</li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>PA</code>: the mentioned year-pack (= paquet-ann\u00e9e)</li> <li><code>secondhand</code>: if secondhand smoking</li> </ul> </li> <li><code>span._.negation</code>: set to True when either<ul> <li>A pack-year value of 0 is extracted</li> <li>A mention such as \"tabac: 0\" is found</li> <li>The patient experiences secondhand smoking</li> </ul> </li> </ul> <p>Use qualifiers !</p> <p>Although the tobacco pipe sometime sets value for the <code>negation</code> attribute, generic qualifier should still be used after the pipe.</p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.tobacco.TobaccoMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.tobacco())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Tabagisme \u00e9valu\u00e9 \u00e0 15 PA\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabagisme \u00e9valu\u00e9 \u00e0 15 PA]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'PA': 15}\n</code></pre> <pre><code>text = \"Patient tabagique\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [tabagique]\n</code></pre> <pre><code>text = \"Tabagisme festif\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"On a un tabagisme ancien\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [tabagisme ancien]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': ancien}\n</code></pre> <pre><code>text = \"Tabac: 0\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabac]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: None\n\nspan._.negation\n# Out: True\n\nspan._.assigned\n# Out: {'zero_after': 0}\n</code></pre> <pre><code>text = \"Tabagisme passif\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabagisme passif]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: None\n\nspan._.negation\n# Out: True\n\nspan._.assigned\n# Out: {'secondhand': passif}\n</code></pre> <pre><code>text = \"Tabac: sevr\u00e9 depuis 5 ans\"\ndoc = nlp(text)\nspans = doc.spans[\"tobacco\"]\n\nspans\n# Out: [Tabac: sevr\u00e9]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: ABSTINENCE\n\nspan._.assigned\n# Out: {'stopped': sevr\u00e9}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.tobacco.TobaccoMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'tobacco'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'tobacco', 'regex': ['tabagi', 'tab...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>tobacco</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'tobacco': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/behaviors/tobacco/tobacco/#edsnlp.pipes.ner.behaviors.tobacco.tobacco.TobaccoMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.tobacco</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/cim10/","title":"<code>edsnlp.pipes.ner.cim10</code>","text":""},{"location":"reference/edsnlp/pipes/ner/cim10/factory/","title":"<code>edsnlp.pipes.ner.cim10.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/cim10/factory/#edsnlp.pipes.ner.cim10.factory.create_component","title":"<code>create_component</code>","text":"<p>The <code>eds.cim10</code> pipeline component extract terms from documents using the CIM10 (French-language ICD) terminology as a reference.</p> <p>Very low recall</p> <p>When using the <code>exact</code> matching mode, this component has a very poor recall performance. We can use the <code>simstring</code> mode to retrieve approximate matches, albeit at the cost of a significantly higher computation time.</p>"},{"location":"reference/edsnlp/pipes/ner/cim10/factory/#edsnlp.pipes.ner.cim10.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.cim10(term_matcher=\"simstring\"))\n\ntext = \"Le patient est suivi pour fi\u00e8vres typho\u00efde et paratypho\u00efde.\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (fi\u00e8vres typho\u00efde et paratypho\u00efde,)\n\nent = doc.ents[0]\n\nent.label_\n# Out: cim10\n\nent.kb_id_\n# Out: A01\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/cim10/factory/#edsnlp.pipes.ner.cim10.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cim10'</code> </p> <code>attr</code> <p>The default attribute to use for matching.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>'exact'</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cim10'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'cim10': True}</code> </p> RETURNS DESCRIPTION <code>TerminologyMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/cim10/factory/#edsnlp.pipes.ner.cim10.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.cim10</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/ner/cim10/patterns/","title":"<code>edsnlp.pipes.ner.cim10.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/covid/","title":"<code>edsnlp.pipes.ner.covid</code>","text":""},{"location":"reference/edsnlp/pipes/ner/covid/factory/","title":"<code>edsnlp.pipes.ner.covid.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/covid/factory/#edsnlp.pipes.ner.covid.factory.create_component","title":"<code>create_component</code>","text":"<p>The <code>eds.covid</code> pipeline component detects mentions of COVID19.</p>"},{"location":"reference/edsnlp/pipes/ner/covid/factory/#edsnlp.pipes.ner.covid.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.covid())\n\ntext = \"Le patient est admis pour une infection au coronavirus.\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (infection au coronavirus,)\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/covid/factory/#edsnlp.pipes.ner.covid.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'covid'</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'LOWER'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>patterns</code> <p>The regex pattern to use</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>patterns</code> </p> <code>label</code> <p>Label to use for matches</p> <p> TYPE: <code>str</code> DEFAULT: <code>'covid'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'covid': True}</code> </p> RETURNS DESCRIPTION <code>GenericMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/covid/factory/#edsnlp.pipes.ner.covid.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.covid</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/ner/covid/patterns/","title":"<code>edsnlp.pipes.ner.covid.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/","title":"<code>edsnlp.pipes.ner.disorders</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/aids/","title":"<code>edsnlp.pipes.ner.disorders.aids</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/aids/aids/","title":"<code>edsnlp.pipes.ner.disorders.aids.aids</code>","text":"<p><code>eds.aids</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/aids/#edsnlp.pipes.ner.disorders.aids.aids.AIDSMatcher","title":"<code>AIDSMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.aids</code> pipeline component extracts mentions of AIDS. It will notably match:</p> <ul> <li>Mentions of VIH/HIV at the SIDA/AIDS stage</li> <li>Mentions of VIH/HIV with opportunistic(s) infection(s)</li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre> <p>On HIV infection</p> <p>pre-AIDS HIV infection are not extracted, only AIDS.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/aids/#edsnlp.pipes.ner.disorders.aids.aids.AIDSMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>opportunist</code>: list of opportunist infections extracted around the HIV mention</li> <li><code>stage</code>: stage of the HIV infection</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/aids/#edsnlp.pipes.ner.disorders.aids.aids.AIDSMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(f\"eds.aids\")\n</code></pre> <p>Below are a few examples:</p> SIDAVIHCoinfectionVIH stade SIDA <pre><code>text = \"Patient atteint du VIH au stade SIDA.\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH au stade SIDA]\n</code></pre> <pre><code>text = \"Patient atteint du VIH.\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Il y a un VIH avec coinfection pneumocystose\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'opportunist': [coinfection, pneumocystose]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un VIH stade C\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': [C]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/aids/#edsnlp.pipes.ner.disorders.aids.aids.AIDSMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'aids'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'aids', 'regex': ['(vih.{1,5}stade....</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>aids</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'aids': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/aids/#edsnlp.pipes.ner.disorders.aids.aids.AIDSMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.aids</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/factory/","title":"<code>edsnlp.pipes.ner.disorders.aids.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/factory/#edsnlp.pipes.ner.disorders.aids.factory.create_component","title":"<code>create_component = registry.factory.register('eds.aids', assigns=['doc.ents', 'doc.spans'], deprecated=['eds.AIDS'])(AIDSMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.aids</code> pipeline component extracts mentions of AIDS. It will notably match:</p> <ul> <li>Mentions of VIH/HIV at the SIDA/AIDS stage</li> <li>Mentions of VIH/HIV with opportunistic(s) infection(s)</li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre> <p>On HIV infection</p> <p>pre-AIDS HIV infection are not extracted, only AIDS.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/factory/#edsnlp.pipes.ner.disorders.aids.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>opportunist</code>: list of opportunist infections extracted around the HIV mention</li> <li><code>stage</code>: stage of the HIV infection</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/factory/#edsnlp.pipes.ner.disorders.aids.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(f\"eds.aids\")\n</code></pre> <p>Below are a few examples:</p> SIDAVIHCoinfectionVIH stade SIDA <pre><code>text = \"Patient atteint du VIH au stade SIDA.\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH au stade SIDA]\n</code></pre> <pre><code>text = \"Patient atteint du VIH.\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Il y a un VIH avec coinfection pneumocystose\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'opportunist': [coinfection, pneumocystose]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un VIH stade C\"\ndoc = nlp(text)\nspans = doc.spans[\"aids\"]\n\nspans\n# Out: [VIH]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': [C]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/factory/#edsnlp.pipes.ner.disorders.aids.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'aids'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'aids', 'regex': ['(vih.{1,5}stade....</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>aids</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'aids': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/factory/#edsnlp.pipes.ner.disorders.aids.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.aids</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/aids/patterns/","title":"<code>edsnlp.pipes.ner.disorders.aids.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/base/","title":"<code>edsnlp.pipes.ner.disorders.base</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/base/#edsnlp.pipes.ner.disorders.base.DisorderMatcher","title":"<code>DisorderMatcher</code>","text":"<p>           Bases: <code>ContextualMatcher</code></p> <p>Base class used to implement various disorders or behaviors extraction pipes</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/base/#edsnlp.pipes.ner.disorders.base.DisorderMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> </p> <code>patterns</code> <p>The configuration dictionary</p> <p> TYPE: <code>FullConfig</code> </p> <code>include_assigned</code> <p>Whether to include (eventual) assign matches to the final entity</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>detailed_status_mapping</code> <p>Mapping from integer status (0, 1 or 2) to human-readable string</p> <p> TYPE: <code>Dict[int, Union[str, None]]</code> DEFAULT: <code>{1: None}</code> </p> <p>alignment_mode : str     Overwrite alignment mode. regex_flags : Union[re.RegexFlag, int]     RegExp flags to use when matching, filtering and assigning (See     the re docs)</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/base/#edsnlp.pipes.ner.disorders.base.DisorderMatcher.__call__","title":"<code>__call__</code>","text":"<p>Tags entities.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/base/#edsnlp.pipes.ner.disorders.base.DisorderMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>annotated spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/","title":"<code>edsnlp.pipes.ner.disorders.cerebrovascular_accident</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/cerebrovascular_accident/","title":"<code>edsnlp.pipes.ner.disorders.cerebrovascular_accident.cerebrovascular_accident</code>","text":"<p><code>eds.cerebrovascular_accident</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/cerebrovascular_accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.cerebrovascular_accident.CerebrovascularAccidentMatcher","title":"<code>CerebrovascularAccidentMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.cerebrovascular_accident</code> pipeline component extracts mentions of cerebrovascular accident. It will notably match:</p> <ul> <li>Mentions of AVC/AIT</li> <li>Mentions of bleeding, hemorrhage, thrombus, ischemia, etc., localized in the brain</li> </ul> Details of the used patterns <pre><code># fmt: off\nimport re\n\nfrom edsnlp.utils.resources import get_AVC_care_site\n\nfrom ..terms import BRAIN, HEART, PERIPHERAL\n\nAVC_CARE_SITES_REGEX = [\n    r\"\\b\" + re.escape(cs.strip()) + r\"\\b\" for cs in get_AVC_care_site(prefix=True)\n] + [\n    r\"h[o\u00f4]p\",\n    r\"\\brcp\",\n    r\"service\",\n    r\"\\bsau\",\n    r\"ap.?hp\",\n    r\"\\burg\",\n    r\"finess\",\n    r\"\\bsiret\",\n    r\"[\u00e0a] avc\",\n    r\"consult\",\n]\n\navc = dict(\n    source=\"avc\",\n    regex=[\n        r\"\\bavc\\b\",\n    ],\n    exclude=[\n        dict(\n            regex=AVC_CARE_SITES_REGEX,\n            window=(-5, 5),\n            regex_flags=re.S | re.I,\n            limit_to_sentence=False,\n        ),\n        dict(\n            regex=r\"\\b[a-z]\\.\",\n            window=2,\n            limit_to_sentence=False,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"(hemorr?agie|hematome)\",\n        r\"angiopath\",\n        r\"angioplasti\",\n        r\"infarctus\",\n        r\"occlusion\",\n        r\"saignement\",\n        r\"embol\",\n        r\"vascularite\",\n        r\"\\bhsd\\b\",\n        r\"thrombos\",\n        r\"thrombol[^y]\",\n        r\"thrombophi\",\n        r\"thrombi[^n]\",\n        r\"thrombus\",\n        r\"thrombectomi\",\n        r\"phleb\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=[\n        dict(\n            regex=r\"pulmo|poumon\",\n            window=4,\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"brain_localized\",\n            regex=\"(\" + r\"|\".join(BRAIN) + \")\",\n            window=(-15, 15),\n            limit_to_sentence=False,\n            include_assigned=False,\n        ),\n    ],\n)\n\ngeneral = dict(\n    source=\"general\",\n    regex=[\n        r\"acc?ident.{1,5}\\s*vasculaire?.{1,6}\\s*cereb.{1,5}\",\n        r\"acc?ident.{1,5}\\s*vasculaire?.{1,6}\\s*ischemi\\w+\",\n        r\"acc?ident.{1,5}ischemi\\w+\",\n        r\"moya.?moya\",\n        r\"oc?clusion.{1,5}(arter|veine).{1,20}retine\",\n        r\"vasculo\\s*path\\w+.cerebr?a\\w+.isch\\w+\",\n        r\"maladies?.des.petites.arter\\w+\",\n        r\"maladies?.des.petits.vaisseaux\",\n        r\"thromboly?i?se\",\n        r\"\\bsusac\\b\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bAIC\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nAIT = dict(\n    source=\"AIT\",\n    regex=[\n        r\"\\bAIC\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n        r\"\\bAIT\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nischemia = dict(\n    source=\"ischemia\",\n    regex=[\n        r\"ischemi\",\n    ],\n    exclude=[\n        dict(\n            regex=PERIPHERAL + HEART,\n            window=(-7, 7),\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"brain\",\n            regex=\"(\" + r\"|\".join(BRAIN) + \")\",\n            window=(-10, 15),\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    avc,\n    with_localization,\n    general,\n    acronym,\n    AIT,\n    ischemia,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/cerebrovascular_accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.cerebrovascular_accident.CerebrovascularAccidentMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/cerebrovascular_accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.cerebrovascular_accident.CerebrovascularAccidentMatcher--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.cerebrovascular_accident())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Patient hospitalis\u00e9 \u00e0 AVC.\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Hospitalisation pour un AVC.\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [AVC]\n</code></pre> <pre><code>text = \"Saignement intracranien\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Saignement]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [intracranien]}\n</code></pre> <pre><code>text = \"Thrombose p\u00e9riph\u00e9rique\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Thrombose sylvienne\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Thrombose]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [sylvienne]}\n</code></pre> <pre><code>text = \"Infarctus c\u00e9r\u00e9bral\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Infarctus]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [c\u00e9r\u00e9bral]}\n</code></pre> <pre><code>text = \"Soign\u00e9 via un thrombolyse\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [thrombolyse]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/cerebrovascular_accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.cerebrovascular_accident.CerebrovascularAccidentMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'avc', 'regex': ['\\\\bavc\\\\b'], 'exc...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>cerebrovascular_accident</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'cerebrovascular_accident': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/cerebrovascular_accident/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.cerebrovascular_accident.CerebrovascularAccidentMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.cerebrovascular_accident</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/factory/","title":"<code>edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/factory/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component","title":"<code>create_component = registry.factory.register('eds.cerebrovascular_accident', assigns=['doc.ents', 'doc.spans'])(CerebrovascularAccidentMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.cerebrovascular_accident</code> pipeline component extracts mentions of cerebrovascular accident. It will notably match:</p> <ul> <li>Mentions of AVC/AIT</li> <li>Mentions of bleeding, hemorrhage, thrombus, ischemia, etc., localized in the brain</li> </ul> Details of the used patterns <pre><code># fmt: off\nimport re\n\nfrom edsnlp.utils.resources import get_AVC_care_site\n\nfrom ..terms import BRAIN, HEART, PERIPHERAL\n\nAVC_CARE_SITES_REGEX = [\n    r\"\\b\" + re.escape(cs.strip()) + r\"\\b\" for cs in get_AVC_care_site(prefix=True)\n] + [\n    r\"h[o\u00f4]p\",\n    r\"\\brcp\",\n    r\"service\",\n    r\"\\bsau\",\n    r\"ap.?hp\",\n    r\"\\burg\",\n    r\"finess\",\n    r\"\\bsiret\",\n    r\"[\u00e0a] avc\",\n    r\"consult\",\n]\n\navc = dict(\n    source=\"avc\",\n    regex=[\n        r\"\\bavc\\b\",\n    ],\n    exclude=[\n        dict(\n            regex=AVC_CARE_SITES_REGEX,\n            window=(-5, 5),\n            regex_flags=re.S | re.I,\n            limit_to_sentence=False,\n        ),\n        dict(\n            regex=r\"\\b[a-z]\\.\",\n            window=2,\n            limit_to_sentence=False,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"(hemorr?agie|hematome)\",\n        r\"angiopath\",\n        r\"angioplasti\",\n        r\"infarctus\",\n        r\"occlusion\",\n        r\"saignement\",\n        r\"embol\",\n        r\"vascularite\",\n        r\"\\bhsd\\b\",\n        r\"thrombos\",\n        r\"thrombol[^y]\",\n        r\"thrombophi\",\n        r\"thrombi[^n]\",\n        r\"thrombus\",\n        r\"thrombectomi\",\n        r\"phleb\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=[\n        dict(\n            regex=r\"pulmo|poumon\",\n            window=4,\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"brain_localized\",\n            regex=\"(\" + r\"|\".join(BRAIN) + \")\",\n            window=(-15, 15),\n            limit_to_sentence=False,\n            include_assigned=False,\n        ),\n    ],\n)\n\ngeneral = dict(\n    source=\"general\",\n    regex=[\n        r\"acc?ident.{1,5}\\s*vasculaire?.{1,6}\\s*cereb.{1,5}\",\n        r\"acc?ident.{1,5}\\s*vasculaire?.{1,6}\\s*ischemi\\w+\",\n        r\"acc?ident.{1,5}ischemi\\w+\",\n        r\"moya.?moya\",\n        r\"oc?clusion.{1,5}(arter|veine).{1,20}retine\",\n        r\"vasculo\\s*path\\w+.cerebr?a\\w+.isch\\w+\",\n        r\"maladies?.des.petites.arter\\w+\",\n        r\"maladies?.des.petits.vaisseaux\",\n        r\"thromboly?i?se\",\n        r\"\\bsusac\\b\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bAIC\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nAIT = dict(\n    source=\"AIT\",\n    regex=[\n        r\"\\bAIC\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n        r\"\\bAIT\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nischemia = dict(\n    source=\"ischemia\",\n    regex=[\n        r\"ischemi\",\n    ],\n    exclude=[\n        dict(\n            regex=PERIPHERAL + HEART,\n            window=(-7, 7),\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"brain\",\n            regex=\"(\" + r\"|\".join(BRAIN) + \")\",\n            window=(-10, 15),\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    avc,\n    with_localization,\n    general,\n    acronym,\n    AIT,\n    ischemia,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/factory/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/factory/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.cerebrovascular_accident())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Patient hospitalis\u00e9 \u00e0 AVC.\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Hospitalisation pour un AVC.\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [AVC]\n</code></pre> <pre><code>text = \"Saignement intracranien\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Saignement]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [intracranien]}\n</code></pre> <pre><code>text = \"Thrombose p\u00e9riph\u00e9rique\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Thrombose sylvienne\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Thrombose]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [sylvienne]}\n</code></pre> <pre><code>text = \"Infarctus c\u00e9r\u00e9bral\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [Infarctus]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'brain_localized': [c\u00e9r\u00e9bral]}\n</code></pre> <pre><code>text = \"Soign\u00e9 via un thrombolyse\"\ndoc = nlp(text)\nspans = doc.spans[\"cerebrovascular_accident\"]\n\nspans\n# Out: [thrombolyse]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/factory/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'avc', 'regex': ['\\\\bavc\\\\b'], 'exc...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>cerebrovascular_accident</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'cerebrovascular_accident': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/factory/#edsnlp.pipes.ner.disorders.cerebrovascular_accident.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.cerebrovascular_accident</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/cerebrovascular_accident/patterns/","title":"<code>edsnlp.pipes.ner.disorders.cerebrovascular_accident.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/","title":"<code>edsnlp.pipes.ner.disorders.ckd</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/ckd/","title":"<code>edsnlp.pipes.ner.disorders.ckd.ckd</code>","text":"<p><code>eds.ckd</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/ckd/#edsnlp.pipes.ner.disorders.ckd.ckd.CKDMatcher","title":"<code>CKDMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.CKD</code> pipeline component extracts mentions of CKD (Chronic Kidney Disease). It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Kidney transplantation</li> <li>Chronic dialysis</li> <li>Renal failure from stage 3 to 5. The stage is extracted by trying 3 methods:<ul> <li>Extracting the mentioned stage directly (\"IRC stade IV\")</li> <li>Extracting the severity directly (\"IRC terminale\")</li> <li>Extracting the mentioned GFR (DFG in french) (\"IRC avec DFG estim\u00e9 \u00e0 30   mL/min/1,73m2)\")</li> </ul> </li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/ckd/#edsnlp.pipes.ner.disorders.ckd.ckd.CKDMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>stage</code>: mentioned renal failure stage</li> <li><code>status</code>: mentioned renal failure severity (e.g. mod\u00e9r\u00e9e, s\u00e9v\u00e8re, terminale,   etc.)</li> <li><code>dfg</code>: mentioned DFG</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/ckd/#edsnlp.pipes.ner.disorders.ckd.ckd.CKDMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.ckd())\n</code></pre> <p>Below are a few examples:</p> 1234567891011 <pre><code>text = \"Patient atteint d'une glom\u00e9rulopathie.\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [glom\u00e9rulopathie]\n</code></pre> <pre><code>text = \"Patient atteint d'une tubulopathie aig\u00fce.\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Patient transplant\u00e9 r\u00e9nal\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [transplant\u00e9 r\u00e9nal]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une insuffisance r\u00e9nale aig\u00fce sur chronique\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [insuffisance r\u00e9nale aig\u00fce sur chronique]\n</code></pre> <pre><code>text = \"Le patient a \u00e9t\u00e9 dialys\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Le patient est dialys\u00e9 chaque lundi\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [dialys\u00e9 chaque lundi]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'chronic': [lundi]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC s\u00e9v\u00e8re\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC s\u00e9v\u00e8re]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'status': s\u00e9v\u00e8re}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC au stade IV\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC au stade IV]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': IV}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC avec DFG \u00e0 30\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC avec DFG \u00e0 30]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'dfg': 30}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une maladie r\u00e9nale avec DFG \u00e0 110\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/ckd/#edsnlp.pipes.ner.disorders.ckd.ckd.CKDMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['glomerulo\\\\s*nep...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>ckd</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'ckd': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/ckd/#edsnlp.pipes.ner.disorders.ckd.ckd.CKDMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.ckd</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/factory/","title":"<code>edsnlp.pipes.ner.disorders.ckd.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/factory/#edsnlp.pipes.ner.disorders.ckd.factory.create_component","title":"<code>create_component = registry.factory.register('eds.ckd', assigns=['doc.ents', 'doc.spans'], deprecated=['eds.CKD'])(CKDMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.CKD</code> pipeline component extracts mentions of CKD (Chronic Kidney Disease). It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Kidney transplantation</li> <li>Chronic dialysis</li> <li>Renal failure from stage 3 to 5. The stage is extracted by trying 3 methods:<ul> <li>Extracting the mentioned stage directly (\"IRC stade IV\")</li> <li>Extracting the severity directly (\"IRC terminale\")</li> <li>Extracting the mentioned GFR (DFG in french) (\"IRC avec DFG estim\u00e9 \u00e0 30   mL/min/1,73m2)\")</li> </ul> </li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/factory/#edsnlp.pipes.ner.disorders.ckd.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>stage</code>: mentioned renal failure stage</li> <li><code>status</code>: mentioned renal failure severity (e.g. mod\u00e9r\u00e9e, s\u00e9v\u00e8re, terminale,   etc.)</li> <li><code>dfg</code>: mentioned DFG</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/factory/#edsnlp.pipes.ner.disorders.ckd.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.ckd())\n</code></pre> <p>Below are a few examples:</p> 1234567891011 <pre><code>text = \"Patient atteint d'une glom\u00e9rulopathie.\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [glom\u00e9rulopathie]\n</code></pre> <pre><code>text = \"Patient atteint d'une tubulopathie aig\u00fce.\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Patient transplant\u00e9 r\u00e9nal\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [transplant\u00e9 r\u00e9nal]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une insuffisance r\u00e9nale aig\u00fce sur chronique\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [insuffisance r\u00e9nale aig\u00fce sur chronique]\n</code></pre> <pre><code>text = \"Le patient a \u00e9t\u00e9 dialys\u00e9\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Le patient est dialys\u00e9 chaque lundi\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [dialys\u00e9 chaque lundi]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'chronic': [lundi]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC s\u00e9v\u00e8re\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC s\u00e9v\u00e8re]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'status': s\u00e9v\u00e8re}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC au stade IV\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC au stade IV]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': IV}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une IRC avec DFG \u00e0 30\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: [IRC avec DFG \u00e0 30]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'dfg': 30}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une maladie r\u00e9nale avec DFG \u00e0 110\"\ndoc = nlp(text)\nspans = doc.spans[\"ckd\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/factory/#edsnlp.pipes.ner.disorders.ckd.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['glomerulo\\\\s*nep...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>ckd</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'ckd': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/factory/#edsnlp.pipes.ner.disorders.ckd.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.ckd</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/ckd/patterns/","title":"<code>edsnlp.pipes.ner.disorders.ckd.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/","title":"<code>edsnlp.pipes.ner.disorders.congestive_heart_failure</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/congestive_heart_failure/","title":"<code>edsnlp.pipes.ner.disorders.congestive_heart_failure.congestive_heart_failure</code>","text":"<p><code>eds.congestive_heart_failure</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/congestive_heart_failure/#edsnlp.pipes.ner.disorders.congestive_heart_failure.congestive_heart_failure.CongestiveHeartFailureMatcher","title":"<code>CongestiveHeartFailureMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.congestive_heart_failure</code> pipeline component extracts mentions of congestive heart failure. It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Heart transplantation</li> <li>AF (Atrial Fibrillation)</li> <li>Pacemaker</li> </ul> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import ASYMPTOMATIC\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"defaill?ance.{1,10}cardi\\w+\",\n        r\"(\u0153|oe)deme?.{1,10}pulmon\",\n        r\"decompensation.{1,10}card\",\n        r\"choc.{1,30}cardio\",\n        r\"greff?e.{1,10}c(\u0153|oe)ur\",\n        r\"greff?e.{1,10}cardia\",\n        r\"transplantation.{1,10}c(\u0153|oe)ur\",\n        r\"transplantation.{1,10}cardia\",\n        r\"arret.{1,10}cardi\",\n        r\"c(\u0153|oe)ur pulmo\",\n        r\"foie.card\",\n        r\"pace.?maker\",\n        r\"stimulateur.cardiaque\",\n        r\"valve.{1,30}(meca|artific)\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nsymptomatic = dict(\n    source=\"symptomatic\",\n    regex=[\n        r\"cardio\\s*path\\w+\",\n        r\"cardio\\s*myopath\\w+\",\n        r\"d(i|y)sfonction.{1,15}(ventricul|\\bvg|cardiaque)\",\n        r\"valvulo\\s*path\\w+?\",\n        r\"\\bic\\b.{1,10}(droite|gauche)\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=ASYMPTOMATIC + [r\"(?&lt;!\\bnon.)ischem\"],  # Exclusion of ischemic events\n        window=5,\n    ),\n)\n\nwith_minimum_severity = dict(\n    source=\"min_severity\",\n    regex=[\n        r\"insuf?fisance.{1,10}(\\bcardi|\\bdiasto|\\bventri|\\bmitral|tri.?cusp)\",\n        r\"(retrecissement|stenose).(aortique|mitral)\",\n        r\"\\brac\\b\",\n        r\"\\brm\\b\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=ASYMPTOMATIC + [\"minime\", \"modere\", r\"non.serre\"],\n        window=5,\n    ),\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bOAP\\b\",\n        r\"\\bCMH\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nAF_main_pattern = dict(\n    source=\"AF_main\",\n    regex=[\n        r\"fibrill?ation.{1,3}(atriale|auriculaire|ventriculaire)\",\n        r\"flutter\",\n        r\"brady.?arythmie\",\n        r\"pace.?maker\",\n    ],\n)\n\nAF_acronym = dict(\n    source=\"AF_acronym\",\n    regex=[\n        r\"\\bFA\\b\",\n        r\"\\bAC.?FA\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    symptomatic,\n    acronym,\n    AF_main_pattern,\n    AF_acronym,\n    with_minimum_severity,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/congestive_heart_failure/#edsnlp.pipes.ner.disorders.congestive_heart_failure.congestive_heart_failure.CongestiveHeartFailureMatcher--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.congestive_heart_failure())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Pr\u00e9sence d'un oed\u00e8me pulmonaire\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [oed\u00e8me pulmonaire]\n</code></pre> <pre><code>text = \"Le patient est \u00e9quip\u00e9 d'un pace-maker\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [pace-maker]\n</code></pre> <pre><code>text = \"Un cardiopathie non d\u00e9compens\u00e9e\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Insuffisance cardiaque\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [Insuffisance cardiaque]\n</code></pre> <pre><code>text = \"Insuffisance cardiaque minime\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/congestive_heart_failure/#edsnlp.pipes.ner.disorders.congestive_heart_failure.congestive_heart_failure.CongestiveHeartFailureMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>(str)</code> DEFAULT: <code>'congestive_heart_failure'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['defaill?ance.{1,...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>congestive_heart_failure</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'congestive_heart_failure': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/congestive_heart_failure/#edsnlp.pipes.ner.disorders.congestive_heart_failure.congestive_heart_failure.CongestiveHeartFailureMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.congestive_heart_failure</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/factory/","title":"<code>edsnlp.pipes.ner.disorders.congestive_heart_failure.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/factory/#edsnlp.pipes.ner.disorders.congestive_heart_failure.factory.create_component","title":"<code>create_component = registry.factory.register('eds.congestive_heart_failure', assigns=['doc.ents', 'doc.spans'])(CongestiveHeartFailureMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.congestive_heart_failure</code> pipeline component extracts mentions of congestive heart failure. It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Heart transplantation</li> <li>AF (Atrial Fibrillation)</li> <li>Pacemaker</li> </ul> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import ASYMPTOMATIC\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"defaill?ance.{1,10}cardi\\w+\",\n        r\"(\u0153|oe)deme?.{1,10}pulmon\",\n        r\"decompensation.{1,10}card\",\n        r\"choc.{1,30}cardio\",\n        r\"greff?e.{1,10}c(\u0153|oe)ur\",\n        r\"greff?e.{1,10}cardia\",\n        r\"transplantation.{1,10}c(\u0153|oe)ur\",\n        r\"transplantation.{1,10}cardia\",\n        r\"arret.{1,10}cardi\",\n        r\"c(\u0153|oe)ur pulmo\",\n        r\"foie.card\",\n        r\"pace.?maker\",\n        r\"stimulateur.cardiaque\",\n        r\"valve.{1,30}(meca|artific)\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nsymptomatic = dict(\n    source=\"symptomatic\",\n    regex=[\n        r\"cardio\\s*path\\w+\",\n        r\"cardio\\s*myopath\\w+\",\n        r\"d(i|y)sfonction.{1,15}(ventricul|\\bvg|cardiaque)\",\n        r\"valvulo\\s*path\\w+?\",\n        r\"\\bic\\b.{1,10}(droite|gauche)\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=ASYMPTOMATIC + [r\"(?&lt;!\\bnon.)ischem\"],  # Exclusion of ischemic events\n        window=5,\n    ),\n)\n\nwith_minimum_severity = dict(\n    source=\"min_severity\",\n    regex=[\n        r\"insuf?fisance.{1,10}(\\bcardi|\\bdiasto|\\bventri|\\bmitral|tri.?cusp)\",\n        r\"(retrecissement|stenose).(aortique|mitral)\",\n        r\"\\brac\\b\",\n        r\"\\brm\\b\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=ASYMPTOMATIC + [\"minime\", \"modere\", r\"non.serre\"],\n        window=5,\n    ),\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bOAP\\b\",\n        r\"\\bCMH\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nAF_main_pattern = dict(\n    source=\"AF_main\",\n    regex=[\n        r\"fibrill?ation.{1,3}(atriale|auriculaire|ventriculaire)\",\n        r\"flutter\",\n        r\"brady.?arythmie\",\n        r\"pace.?maker\",\n    ],\n)\n\nAF_acronym = dict(\n    source=\"AF_acronym\",\n    regex=[\n        r\"\\bFA\\b\",\n        r\"\\bAC.?FA\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    symptomatic,\n    acronym,\n    AF_main_pattern,\n    AF_acronym,\n    with_minimum_severity,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/factory/#edsnlp.pipes.ner.disorders.congestive_heart_failure.factory.create_component--usage","title":"Usage","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.congestive_heart_failure())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Pr\u00e9sence d'un oed\u00e8me pulmonaire\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [oed\u00e8me pulmonaire]\n</code></pre> <pre><code>text = \"Le patient est \u00e9quip\u00e9 d'un pace-maker\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [pace-maker]\n</code></pre> <pre><code>text = \"Un cardiopathie non d\u00e9compens\u00e9e\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Insuffisance cardiaque\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: [Insuffisance cardiaque]\n</code></pre> <pre><code>text = \"Insuffisance cardiaque minime\"\ndoc = nlp(text)\nspans = doc.spans[\"congestive_heart_failure\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/factory/#edsnlp.pipes.ner.disorders.congestive_heart_failure.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>(str)</code> DEFAULT: <code>'congestive_heart_failure'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['defaill?ance.{1,...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>congestive_heart_failure</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'congestive_heart_failure': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/factory/#edsnlp.pipes.ner.disorders.congestive_heart_failure.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.congestive_heart_failure</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/congestive_heart_failure/patterns/","title":"<code>edsnlp.pipes.ner.disorders.congestive_heart_failure.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/","title":"<code>edsnlp.pipes.ner.disorders.connective_tissue_disease</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/connective_tissue_disease/","title":"<code>edsnlp.pipes.ner.disorders.connective_tissue_disease.connective_tissue_disease</code>","text":"<p><code>eds.connective_tissue_disease</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/connective_tissue_disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.connective_tissue_disease.ConnectiveTissueDiseaseMatcher","title":"<code>ConnectiveTissueDiseaseMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.connective_tissue_disease</code> pipeline component extracts mentions of connective tissue diseases.</p> Details of the used patterns <pre><code># fmt: off\nTO_EXCLUDE = r\"(?&lt;!a )((\\bacc\\b)|anti.?coag|anti.?corps|buschke|(\\bac\\b)|(\\bbio))\"\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"arth?rites?.{1,5}juveniles?.{1,5}idiopa\\w+\",\n        r\"myosite\",\n        r\"myopath\\w+.{1,5}inflammatoire\",\n        r\"polyarth?rite.{1,5}chroni\\w+.{1,5}evol\",\n        r\"polymyosie\",\n        r\"polyarth?rites?.{1,5}(rhizo|rhuma)\",\n        r\"scleroderm\\w+\",\n        r\"connectivite\",\n        r\"sarcoidose\",\n    ],\n    exclude=dict(\n        regex=[TO_EXCLUDE],\n        window=(-7, 7),\n    ),\n    regex_attr=\"NORM\",\n)\n\nlupus = dict(\n    source=\"lupus\",\n    regex=[\n        r\"\\blupus\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nlupique = dict(\n    source=\"lupique\",\n    regex=[r\"\\blupique\", r\"\\blupic\"],\n    exclude=dict(\n        regex=[TO_EXCLUDE],\n        window=(-7, 7),\n    ),\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronyms\",\n    regex=[\n        r\"\\bAJI\\b\",\n        r\"\\bLED\\b\",\n        r\"\\bPCE\\b\",\n        r\"\\bCREST\\b\",\n        r\"\\bPPR\\b\",\n        r\"\\bMICI\\b\",\n        r\"\\bMNAI\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nnamed_disease = dict(\n    source=\"named_disease\",\n    regex=[\n        r\"libman.?lack\",\n        r\"\\bstill\",\n        r\"felty\",\n        r\"forestier.?certon\",\n        r\"gou(g|j)erot\",\n        r\"raynaud\",\n        r\"thibierge.?weiss\",\n        r\"sjogren\",\n        r\"gou(g|j)erot.?sjogren\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    lupus,\n    lupique,\n    acronym,\n    named_disease,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/connective_tissue_disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.connective_tissue_disease.ConnectiveTissueDiseaseMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/connective_tissue_disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.connective_tissue_disease.ConnectiveTissueDiseaseMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.connective_tissue_disease())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Pr\u00e9sence d'une scl\u00e9rodermie.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [scl\u00e9rodermie]\n</code></pre> <pre><code>text = \"Patient atteint d'un lupus.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [lupus]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'anticoagulants lupiques,\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Il y a une MICI.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [MICI]\n</code></pre> <pre><code>text = \"Syndrome de Raynaud\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [Raynaud]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/connective_tissue_disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.connective_tissue_disease.ConnectiveTissueDiseaseMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'connective_tissue_disease'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['arth?rites?.{1,5...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>connective_tissue_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'connective_tissue_disease': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/connective_tissue_disease/#edsnlp.pipes.ner.disorders.connective_tissue_disease.connective_tissue_disease.ConnectiveTissueDiseaseMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.connective_tissue_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/factory/","title":"<code>edsnlp.pipes.ner.disorders.connective_tissue_disease.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/factory/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component","title":"<code>create_component = registry.factory.register('eds.connective_tissue_disease', assigns=['doc.ents', 'doc.spans'])(ConnectiveTissueDiseaseMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.connective_tissue_disease</code> pipeline component extracts mentions of connective tissue diseases.</p> Details of the used patterns <pre><code># fmt: off\nTO_EXCLUDE = r\"(?&lt;!a )((\\bacc\\b)|anti.?coag|anti.?corps|buschke|(\\bac\\b)|(\\bbio))\"\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"arth?rites?.{1,5}juveniles?.{1,5}idiopa\\w+\",\n        r\"myosite\",\n        r\"myopath\\w+.{1,5}inflammatoire\",\n        r\"polyarth?rite.{1,5}chroni\\w+.{1,5}evol\",\n        r\"polymyosie\",\n        r\"polyarth?rites?.{1,5}(rhizo|rhuma)\",\n        r\"scleroderm\\w+\",\n        r\"connectivite\",\n        r\"sarcoidose\",\n    ],\n    exclude=dict(\n        regex=[TO_EXCLUDE],\n        window=(-7, 7),\n    ),\n    regex_attr=\"NORM\",\n)\n\nlupus = dict(\n    source=\"lupus\",\n    regex=[\n        r\"\\blupus\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nlupique = dict(\n    source=\"lupique\",\n    regex=[r\"\\blupique\", r\"\\blupic\"],\n    exclude=dict(\n        regex=[TO_EXCLUDE],\n        window=(-7, 7),\n    ),\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronyms\",\n    regex=[\n        r\"\\bAJI\\b\",\n        r\"\\bLED\\b\",\n        r\"\\bPCE\\b\",\n        r\"\\bCREST\\b\",\n        r\"\\bPPR\\b\",\n        r\"\\bMICI\\b\",\n        r\"\\bMNAI\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nnamed_disease = dict(\n    source=\"named_disease\",\n    regex=[\n        r\"libman.?lack\",\n        r\"\\bstill\",\n        r\"felty\",\n        r\"forestier.?certon\",\n        r\"gou(g|j)erot\",\n        r\"raynaud\",\n        r\"thibierge.?weiss\",\n        r\"sjogren\",\n        r\"gou(g|j)erot.?sjogren\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    lupus,\n    lupique,\n    acronym,\n    named_disease,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/factory/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/factory/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.connective_tissue_disease())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Pr\u00e9sence d'une scl\u00e9rodermie.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [scl\u00e9rodermie]\n</code></pre> <pre><code>text = \"Patient atteint d'un lupus.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [lupus]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'anticoagulants lupiques,\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Il y a une MICI.\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [MICI]\n</code></pre> <pre><code>text = \"Syndrome de Raynaud\"\ndoc = nlp(text)\nspans = doc.spans[\"connective_tissue_disease\"]\n\nspans\n# Out: [Raynaud]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/factory/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'connective_tissue_disease'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['arth?rites?.{1,5...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>connective_tissue_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'connective_tissue_disease': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/factory/#edsnlp.pipes.ner.disorders.connective_tissue_disease.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.connective_tissue_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/connective_tissue_disease/patterns/","title":"<code>edsnlp.pipes.ner.disorders.connective_tissue_disease.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/copd/","title":"<code>edsnlp.pipes.ner.disorders.copd</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/copd/copd/","title":"<code>edsnlp.pipes.ner.disorders.copd.copd</code>","text":"<p><code>eds.copd</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/copd/#edsnlp.pipes.ner.disorders.copd.copd.COPDMatcher","title":"<code>COPDMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.copd</code> pipeline component extracts mentions of COPD (Chronic obstructive pulmonary disease). It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Pulmonary hypertension</li> <li>Long-term oxygen therapy</li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/copd/#edsnlp.pipes.ner.disorders.copd.copd.COPDMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/copd/#edsnlp.pipes.ner.disorders.copd.copd.COPDMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.copd())\n</code></pre> <p>Below are a few examples:</p> 123456 <pre><code>text = \"Une fibrose interstitielle diffuse idiopathique\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [fibrose interstitielle diffuse idiopathique]\n</code></pre> <pre><code>text = \"Patient atteint de pneumoconiose\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [pneumoconiose]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une HTAP.\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [HTAP]\n</code></pre> <pre><code>text = \"On voit une hypertension pulmonaire minime\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"La patiente a \u00e9t\u00e9 mis sous oxyg\u00e9norequ\u00e9rance\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"La patiente est sous oxyg\u00e9norequ\u00e9rance au long cours\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [oxyg\u00e9norequ\u00e9rance au long cours]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'long': [long cours]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/copd/#edsnlp.pipes.ner.disorders.copd.copd.COPDMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'copd'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['alveolites?.{1,5...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>copd</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'copd': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/copd/#edsnlp.pipes.ner.disorders.copd.copd.COPDMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.copd</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/factory/","title":"<code>edsnlp.pipes.ner.disorders.copd.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/factory/#edsnlp.pipes.ner.disorders.copd.factory.create_component","title":"<code>create_component = registry.factory.register('eds.copd', assigns=['doc.ents', 'doc.spans'], deprecated=['eds.COPD'])(COPDMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.copd</code> pipeline component extracts mentions of COPD (Chronic obstructive pulmonary disease). It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Pulmonary hypertension</li> <li>Long-term oxygen therapy</li> </ul> Details of the used patterns <pre><code># fmt: off\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/factory/#edsnlp.pipes.ner.disorders.copd.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/factory/#edsnlp.pipes.ner.disorders.copd.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.copd())\n</code></pre> <p>Below are a few examples:</p> 123456 <pre><code>text = \"Une fibrose interstitielle diffuse idiopathique\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [fibrose interstitielle diffuse idiopathique]\n</code></pre> <pre><code>text = \"Patient atteint de pneumoconiose\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [pneumoconiose]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'une HTAP.\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [HTAP]\n</code></pre> <pre><code>text = \"On voit une hypertension pulmonaire minime\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"La patiente a \u00e9t\u00e9 mis sous oxyg\u00e9norequ\u00e9rance\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"La patiente est sous oxyg\u00e9norequ\u00e9rance au long cours\"\ndoc = nlp(text)\nspans = doc.spans[\"copd\"]\n\nspans\n# Out: [oxyg\u00e9norequ\u00e9rance au long cours]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'long': [long cours]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/factory/#edsnlp.pipes.ner.disorders.copd.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'copd'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['alveolites?.{1,5...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>copd</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'copd': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/factory/#edsnlp.pipes.ner.disorders.copd.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.copd</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/copd/patterns/","title":"<code>edsnlp.pipes.ner.disorders.copd.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/","title":"<code>edsnlp.pipes.ner.disorders.dementia</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/dementia/","title":"<code>edsnlp.pipes.ner.disorders.dementia.dementia</code>","text":"<p><code>eds.dementia</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/dementia/#edsnlp.pipes.ner.disorders.dementia.dementia.DementiaMatcher","title":"<code>DementiaMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.dementia</code> pipeline component extracts mentions of dementia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"demence\",\n        r\"demense\",\n        r\"dementiel\",\n        r\"corps\\s*de\\s*le[vw]y\",\n        r\"deficits?.chroniques?.cognitifs?\",\n        r\"troubles?.mnesique?\",\n        r\"troubles?.praxique\",\n        r\"troubles?.att?entionel\",\n        r\"troubles?.degeneratifs?.{1,15}fonctions.{1,5}sup\",\n        r\"maladies?.cerebrales?.degen\",\n        r\"troubles?.neurocogn\\w+\",\n        r\"deficits?.cogniti\\w+\",\n        r\"atteinte.{1,7}spheres?cogniti\",\n        r\"syndrome?.{1,10}(frontal|neuro.deg)\",\n        r\"(trouble|d(y|i)sfonction).{1,25}cogni\\w+\",\n        r\"(?&lt;!specialisee)alzheimer\",\n        r\"demence.{1,20}(\\balz|\\bpark)\",\n        r\"binswanger\",\n        r\"gehring\",\n        r\"\\bpick\",\n        r\"de\\s*guam\",\n        r\"[kc]reutzfeld.{1,5}ja[ck]ob\",\n        r\"huntington\",\n        r\"korsako[fv]\",\n        r\"atrophie.{1,10}(cortico|hip?pocamp|cereb|lobe)\",\n    ],\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bSLA\\b\",\n        r\"\\bDFT\\b\",\n        r\"\\bDFT\",\n        r\"\\bTNC\\b\",\n        r\"\\bHTT\\b\",\n        r\"\\bALS\\b\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=r\"\\banti\",  # anticorps\n        window=-15,\n        regex_attr=\"NORM\",\n    ),\n)\n\ncharcot = dict(\n    source=\"charcot\",\n    regex=[\n        r\"maladie.{1,10}charcot\",\n        r\"maladie.{1,10}lou\\s*gehrig\",\n    ],\n    exclude=dict(\n        regex=[\n            \"pied de\",\n            \"marie.?tooth\",\n        ],\n        window=(-3, 3),\n    ),\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    charcot,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/dementia/#edsnlp.pipes.ner.disorders.dementia.dementia.DementiaMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/dementia/#edsnlp.pipes.ner.disorders.dementia.dementia.DementiaMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.dementia())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"D'importants d\u00e9ficits cognitifs\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [d\u00e9ficits cognitifs]\n</code></pre> <pre><code>text = \"Patient atteint de d\u00e9mence\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [d\u00e9mence]\n</code></pre> <pre><code>text = \"On retrouve des anti-SLA\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une maladie de Charcot\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [maladie de Charcot]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/dementia/#edsnlp.pipes.ner.disorders.dementia.dementia.DementiaMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'dementia'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['demence', 'demen...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>dementia</code> </p> <code>span_setter</code> <p>The span setter to use</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'dementia': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/dementia/#edsnlp.pipes.ner.disorders.dementia.dementia.DementiaMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.dementia</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/factory/","title":"<code>edsnlp.pipes.ner.disorders.dementia.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/factory/#edsnlp.pipes.ner.disorders.dementia.factory.create_component","title":"<code>create_component = registry.factory.register('eds.dementia', assigns=['doc.ents', 'doc.spans'])(DementiaMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.dementia</code> pipeline component extracts mentions of dementia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"demence\",\n        r\"demense\",\n        r\"dementiel\",\n        r\"corps\\s*de\\s*le[vw]y\",\n        r\"deficits?.chroniques?.cognitifs?\",\n        r\"troubles?.mnesique?\",\n        r\"troubles?.praxique\",\n        r\"troubles?.att?entionel\",\n        r\"troubles?.degeneratifs?.{1,15}fonctions.{1,5}sup\",\n        r\"maladies?.cerebrales?.degen\",\n        r\"troubles?.neurocogn\\w+\",\n        r\"deficits?.cogniti\\w+\",\n        r\"atteinte.{1,7}spheres?cogniti\",\n        r\"syndrome?.{1,10}(frontal|neuro.deg)\",\n        r\"(trouble|d(y|i)sfonction).{1,25}cogni\\w+\",\n        r\"(?&lt;!specialisee)alzheimer\",\n        r\"demence.{1,20}(\\balz|\\bpark)\",\n        r\"binswanger\",\n        r\"gehring\",\n        r\"\\bpick\",\n        r\"de\\s*guam\",\n        r\"[kc]reutzfeld.{1,5}ja[ck]ob\",\n        r\"huntington\",\n        r\"korsako[fv]\",\n        r\"atrophie.{1,10}(cortico|hip?pocamp|cereb|lobe)\",\n    ],\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bSLA\\b\",\n        r\"\\bDFT\\b\",\n        r\"\\bDFT\",\n        r\"\\bTNC\\b\",\n        r\"\\bHTT\\b\",\n        r\"\\bALS\\b\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=r\"\\banti\",  # anticorps\n        window=-15,\n        regex_attr=\"NORM\",\n    ),\n)\n\ncharcot = dict(\n    source=\"charcot\",\n    regex=[\n        r\"maladie.{1,10}charcot\",\n        r\"maladie.{1,10}lou\\s*gehrig\",\n    ],\n    exclude=dict(\n        regex=[\n            \"pied de\",\n            \"marie.?tooth\",\n        ],\n        window=(-3, 3),\n    ),\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    charcot,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/factory/#edsnlp.pipes.ner.disorders.dementia.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/factory/#edsnlp.pipes.ner.disorders.dementia.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.dementia())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"D'importants d\u00e9ficits cognitifs\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [d\u00e9ficits cognitifs]\n</code></pre> <pre><code>text = \"Patient atteint de d\u00e9mence\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [d\u00e9mence]\n</code></pre> <pre><code>text = \"On retrouve des anti-SLA\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une maladie de Charcot\"\ndoc = nlp(text)\nspans = doc.spans[\"dementia\"]\n\nspans\n# Out: [maladie de Charcot]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/factory/#edsnlp.pipes.ner.disorders.dementia.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'dementia'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['demence', 'demen...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>dementia</code> </p> <code>span_setter</code> <p>The span setter to use</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'dementia': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/factory/#edsnlp.pipes.ner.disorders.dementia.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.dementia</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/dementia/patterns/","title":"<code>edsnlp.pipes.ner.disorders.dementia.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/","title":"<code>edsnlp.pipes.ner.disorders.diabetes</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/diabetes/","title":"<code>edsnlp.pipes.ner.disorders.diabetes.diabetes</code>","text":"<p><code>eds.diabetes</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/diabetes/#edsnlp.pipes.ner.disorders.diabetes.diabetes.DiabetesMatcher","title":"<code>DiabetesMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.diabetes</code> pipeline component extracts mentions of diabetes.</p> Details of the used patterns <pre><code># fmt: off\nCOMPLICATIONS = [\n    r\"nephropat\",\n    r\"neuropat\",\n    r\"retinopat\",\n    r\"glomerulopathi\",\n    r\"glomeruloscleros\",\n    r\"angiopathi\",\n    r\"origine\",\n]\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"\\bds?n?id\\b\",\n        r\"\\bdiabet[^o]\",\n        r\"\\bdiab\",\n        r\"\\bdb\\b\",\n        r\"\\bdt.?(i|ii|1|2)\\b\",\n    ],\n    exclude=dict(\n        regex=[\n            \"insipide\",\n            \"nephrogenique\",\n            \"aigu\",\n            r\"\\bdr\\b\",  # Dr. ...\n            \"endocrino\",  # Section title\n            \"soins aux pieds\",  # Section title\n            \"nutrition\",  # Section title\n            r\"\\s?:\\n+\\W+(?!oui|non|\\W)\",  # General pattern for section title\n        ],\n        window=(-5, 5),\n    ),\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"complicated_before\",\n            regex=r\"(\" + r\"|\".join(COMPLICATIONS + [\"origine\"]) + r\")\",\n            window=-3,\n        ),\n        dict(\n            name=\"complicated_after\",\n            regex=r\"(\"\n            + r\"|\".join([r\"(?&lt;!sans )compli\", r\"(?&lt;!a)symptomatique\"] + COMPLICATIONS)\n            + r\")\",\n            window=12,\n        ),\n        dict(\n            name=\"type\",\n            regex=r\"type.?\\s*(ii|i|1|2)\",\n            window=6,\n        ),\n        dict(\n            name=\"insulin\",\n            regex=r\"insulino.?(dep|req)\",\n            window=6,\n        ),\n        dict(\n            name=\"corticoid\",\n            regex=r\"(\\bctc\\b|cortico(?:.?induit)?)\",\n            window=6,\n        ),\n    ],\n)\n\ncomplicated_pattern = dict(\n    source=\"complicated\",\n    regex=[\n        r\"(mal|maux).perforants?(.plantaire)?\",\n        r\"pieds? diabeti\",\n    ],\n    exclude=dict(\n        regex=\"soins aux\",  # Section title\n        window=-2,\n    ),\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    complicated_pattern,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/diabetes/#edsnlp.pipes.ner.disorders.diabetes.diabetes.DiabetesMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"WITH_COMPLICATION\"</code> if the diabetes is  complicated (e.g., via organ    damages)</li> <li><code>\"WITHOUT_COMPLICATION\"</code> otherwise</li> </ul> </li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>type</code>: type of diabetes (I or II)</li> <li><code>insulin</code>: if the diabetes is insulin-dependent</li> <li><code>corticoid</code>: if the diabetes is corticoid-induced</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/diabetes/#edsnlp.pipes.ner.disorders.diabetes.diabetes.DiabetesMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.diabetes())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Pr\u00e9sence d'un DT2\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [DT2]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un DNID\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [DNID]\n</code></pre> <pre><code>text = \"Patient diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [diab\u00e9tique]\n</code></pre> <pre><code>text = \"Un diab\u00e8te insipide\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Atteinte neurologique d'origine diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [origine diab\u00e9tique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n\nspan._.assigned\n# Out: {'complicated_before': [origine]}\n</code></pre> <pre><code>text = \"Une r\u00e9tinopathie diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [r\u00e9tinopathie diab\u00e9tique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n\nspan._.assigned\n# Out: {'complicated_before': [r\u00e9tinopathie]}\n</code></pre> <pre><code>text = \"Il y a un mal perforant plantaire\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [mal perforant plantaire]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/diabetes/#edsnlp.pipes.ner.disorders.diabetes.diabetes.DiabetesMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['\\\\bds?n?id\\\\b', ...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>diabetes</code> </p> <code>span_setter</code> <p>The span setter to use</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'diabetes': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/diabetes/#edsnlp.pipes.ner.disorders.diabetes.diabetes.DiabetesMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.diabetes</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/diabetes/#edsnlp.pipes.ner.disorders.diabetes.diabetes.DiabetesMatcher.has_far_complications","title":"<code>has_far_complications</code>","text":"<p>Handles the common case where complications are listed as bullet points, sometimes fairly far from the anchor.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/factory/","title":"<code>edsnlp.pipes.ner.disorders.diabetes.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/factory/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component","title":"<code>create_component = registry.factory.register('eds.diabetes', assigns=['doc.ents', 'doc.spans'])(DiabetesMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.diabetes</code> pipeline component extracts mentions of diabetes.</p> Details of the used patterns <pre><code># fmt: off\nCOMPLICATIONS = [\n    r\"nephropat\",\n    r\"neuropat\",\n    r\"retinopat\",\n    r\"glomerulopathi\",\n    r\"glomeruloscleros\",\n    r\"angiopathi\",\n    r\"origine\",\n]\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"\\bds?n?id\\b\",\n        r\"\\bdiabet[^o]\",\n        r\"\\bdiab\",\n        r\"\\bdb\\b\",\n        r\"\\bdt.?(i|ii|1|2)\\b\",\n    ],\n    exclude=dict(\n        regex=[\n            \"insipide\",\n            \"nephrogenique\",\n            \"aigu\",\n            r\"\\bdr\\b\",  # Dr. ...\n            \"endocrino\",  # Section title\n            \"soins aux pieds\",  # Section title\n            \"nutrition\",  # Section title\n            r\"\\s?:\\n+\\W+(?!oui|non|\\W)\",  # General pattern for section title\n        ],\n        window=(-5, 5),\n    ),\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"complicated_before\",\n            regex=r\"(\" + r\"|\".join(COMPLICATIONS + [\"origine\"]) + r\")\",\n            window=-3,\n        ),\n        dict(\n            name=\"complicated_after\",\n            regex=r\"(\"\n            + r\"|\".join([r\"(?&lt;!sans )compli\", r\"(?&lt;!a)symptomatique\"] + COMPLICATIONS)\n            + r\")\",\n            window=12,\n        ),\n        dict(\n            name=\"type\",\n            regex=r\"type.?\\s*(ii|i|1|2)\",\n            window=6,\n        ),\n        dict(\n            name=\"insulin\",\n            regex=r\"insulino.?(dep|req)\",\n            window=6,\n        ),\n        dict(\n            name=\"corticoid\",\n            regex=r\"(\\bctc\\b|cortico(?:.?induit)?)\",\n            window=6,\n        ),\n    ],\n)\n\ncomplicated_pattern = dict(\n    source=\"complicated\",\n    regex=[\n        r\"(mal|maux).perforants?(.plantaire)?\",\n        r\"pieds? diabeti\",\n    ],\n    exclude=dict(\n        regex=\"soins aux\",  # Section title\n        window=-2,\n    ),\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    complicated_pattern,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/factory/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"WITH_COMPLICATION\"</code> if the diabetes is  complicated (e.g., via organ    damages)</li> <li><code>\"WITHOUT_COMPLICATION\"</code> otherwise</li> </ul> </li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>type</code>: type of diabetes (I or II)</li> <li><code>insulin</code>: if the diabetes is insulin-dependent</li> <li><code>corticoid</code>: if the diabetes is corticoid-induced</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/factory/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.diabetes())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Pr\u00e9sence d'un DT2\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [DT2]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un DNID\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [DNID]\n</code></pre> <pre><code>text = \"Patient diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [diab\u00e9tique]\n</code></pre> <pre><code>text = \"Un diab\u00e8te insipide\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Atteinte neurologique d'origine diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [origine diab\u00e9tique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n\nspan._.assigned\n# Out: {'complicated_before': [origine]}\n</code></pre> <pre><code>text = \"Une r\u00e9tinopathie diab\u00e9tique\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [r\u00e9tinopathie diab\u00e9tique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n\nspan._.assigned\n# Out: {'complicated_before': [r\u00e9tinopathie]}\n</code></pre> <pre><code>text = \"Il y a un mal perforant plantaire\"\ndoc = nlp(text)\nspans = doc.spans[\"diabetes\"]\n\nspans\n# Out: [mal perforant plantaire]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: WITH_COMPLICATION\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/factory/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['\\\\bds?n?id\\\\b', ...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>diabetes</code> </p> <code>span_setter</code> <p>The span setter to use</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'diabetes': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/factory/#edsnlp.pipes.ner.disorders.diabetes.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.diabetes</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/diabetes/patterns/","title":"<code>edsnlp.pipes.ner.disorders.diabetes.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/","title":"<code>edsnlp.pipes.ner.disorders.hemiplegia</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/factory/","title":"<code>edsnlp.pipes.ner.disorders.hemiplegia.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/factory/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component","title":"<code>create_component = registry.factory.register('eds.hemiplegia', assigns=['doc.ents', 'doc.spans'])(HemiplegiaMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.hemiplegia</code> pipeline component extracts mentions of hemiplegia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"hemipleg\\w+\",\n        r\"tetrapleg\\w+\",\n        r\"quadripleg\\w+\",\n        r\"parapleg\\w+\",\n        r\"neuropath\\w+.{1,25}motrice.{1,30}type\\s*[5V]\",\n        r\"charcot.?marie.?tooth\",\n        r\"loc?ked.?in\",\n        r\"syndrome?.{1,5}(enfermement|verrouillage)|(desafferen)\",\n        r\"paralysie.{1,10}hemicorps\",\n        r\"paralysie.{1,10}jambe\",\n        r\"paralysie.{1,10}membre\",\n        r\"paralysie.{1,10}cote\",\n        r\"paralysie.{1,5}cerebrale.{1,5}spastique\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLIS\\b\",\n        r\"\\bNMSH\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/factory/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/factory/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.hemiplegia())\n</code></pre> <p>Below are a few examples:</p> 123 <pre><code>text = \"Patient h\u00e9mipl\u00e9gique\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [h\u00e9mipl\u00e9gique]\n</code></pre> <pre><code>text = \"Paralysie des membres inf\u00e9rieurs\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [Paralysie des membres]\n</code></pre> <pre><code>text = \"Patient en LIS\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [LIS]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/factory/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'hemiplegia'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['hemipleg\\\\w+', '...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>hemiplegia</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'hemiplegia': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/factory/#edsnlp.pipes.ner.disorders.hemiplegia.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.hemiplegia</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/hemiplegia/","title":"<code>edsnlp.pipes.ner.disorders.hemiplegia.hemiplegia</code>","text":"<p><code>eds.hemiplegia</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.hemiplegia.HemiplegiaMatcher","title":"<code>HemiplegiaMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.hemiplegia</code> pipeline component extracts mentions of hemiplegia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"hemipleg\\w+\",\n        r\"tetrapleg\\w+\",\n        r\"quadripleg\\w+\",\n        r\"parapleg\\w+\",\n        r\"neuropath\\w+.{1,25}motrice.{1,30}type\\s*[5V]\",\n        r\"charcot.?marie.?tooth\",\n        r\"loc?ked.?in\",\n        r\"syndrome?.{1,5}(enfermement|verrouillage)|(desafferen)\",\n        r\"paralysie.{1,10}hemicorps\",\n        r\"paralysie.{1,10}jambe\",\n        r\"paralysie.{1,10}membre\",\n        r\"paralysie.{1,10}cote\",\n        r\"paralysie.{1,5}cerebrale.{1,5}spastique\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLIS\\b\",\n        r\"\\bNMSH\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.hemiplegia.HemiplegiaMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.hemiplegia.HemiplegiaMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.hemiplegia())\n</code></pre> <p>Below are a few examples:</p> 123 <pre><code>text = \"Patient h\u00e9mipl\u00e9gique\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [h\u00e9mipl\u00e9gique]\n</code></pre> <pre><code>text = \"Paralysie des membres inf\u00e9rieurs\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [Paralysie des membres]\n</code></pre> <pre><code>text = \"Patient en LIS\"\ndoc = nlp(text)\nspans = doc.spans[\"hemiplegia\"]\n\nspans\n# Out: [LIS]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.hemiplegia.HemiplegiaMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'hemiplegia'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['hemipleg\\\\w+', '...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>hemiplegia</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'hemiplegia': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/hemiplegia/#edsnlp.pipes.ner.disorders.hemiplegia.hemiplegia.HemiplegiaMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.hemiplegia</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/hemiplegia/patterns/","title":"<code>edsnlp.pipes.ner.disorders.hemiplegia.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/","title":"<code>edsnlp.pipes.ner.disorders.leukemia</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/factory/","title":"<code>edsnlp.pipes.ner.disorders.leukemia.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/factory/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component","title":"<code>create_component = registry.factory.register('eds.leukemia', assigns=['doc.ents', 'doc.spans'])(LeukemiaMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.leukemia</code> pipeline component extracts mentions of leukemia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"leucemie?\",\n        r\"(syndrome?.)?myelo\\s*proliferatif\",\n        r\"m[yi]eloprolifer\",\n    ],\n    exclude=dict(\n        regex=[\n            \"plasmocyte\",\n            \"benin\",\n            \"benign\",\n        ],\n        window=5,\n    ),\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLAM\\b\",\n        r\"\\bLAM.?[0-9]\",\n        r\"\\bLAL\\b\",\n        r\"\\bLMC\\b\",\n        r\"\\bLCE\\b\",\n        r\"\\bLMM[JC]\\b\",\n        r\"\\bLCN\\b\",\n        r\"\\bAREB\\b\",\n        r\"\\bAPMF\\b\",\n        r\"\\bLLC\\b\",\n        r\"\\bSMD\\b\",\n        r\"LA my[\u00e9\u00e8e]lomonocytaire\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=\"anti\",\n        window=-20,\n    ),\n)\n\nother = dict(\n    source=\"other\",\n    regex=[\n        r\"myelofibrose\",\n        r\"vaquez\",\n        r\"thrombocytem\\w+.{1,3}essentiell?e?\",\n        r\"splenomegal\\w+.{1,3}myeloide\",\n        r\"mastocytose.{1,5}maligne?\",\n        r\"polyglobul\\w+.{1,10}essentiell?e?\",\n        r\"letterer.?siwe\",\n        r\"anemie.refractaire.{1,20}blaste\",\n        r\"m[iy]elod[iy]splasi\",\n        r\"syndrome.myelo.?dysplasique\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    other,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/factory/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/factory/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.leukemia())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Sydrome my\u00e9loprolif\u00e9ratif\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [my\u00e9loprolif\u00e9ratif]\n</code></pre> <pre><code>text = \"Sydrome my\u00e9loprolif\u00e9ratif b\u00e9nin\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Patient atteint d'une LAM\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [LAM]\n</code></pre> <pre><code>text = \"Une maladie de Vaquez\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [Vaquez]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/factory/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'leukemia'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['leucemie?', '(sy...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>leukemia</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'leukemia': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/factory/#edsnlp.pipes.ner.disorders.leukemia.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.leukemia</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/leukemia/","title":"<code>edsnlp.pipes.ner.disorders.leukemia.leukemia</code>","text":"<p><code>eds.leukemia</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/leukemia/#edsnlp.pipes.ner.disorders.leukemia.leukemia.LeukemiaMatcher","title":"<code>LeukemiaMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.leukemia</code> pipeline component extracts mentions of leukemia.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"leucemie?\",\n        r\"(syndrome?.)?myelo\\s*proliferatif\",\n        r\"m[yi]eloprolifer\",\n    ],\n    exclude=dict(\n        regex=[\n            \"plasmocyte\",\n            \"benin\",\n            \"benign\",\n        ],\n        window=5,\n    ),\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLAM\\b\",\n        r\"\\bLAM.?[0-9]\",\n        r\"\\bLAL\\b\",\n        r\"\\bLMC\\b\",\n        r\"\\bLCE\\b\",\n        r\"\\bLMM[JC]\\b\",\n        r\"\\bLCN\\b\",\n        r\"\\bAREB\\b\",\n        r\"\\bAPMF\\b\",\n        r\"\\bLLC\\b\",\n        r\"\\bSMD\\b\",\n        r\"LA my[\u00e9\u00e8e]lomonocytaire\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=\"anti\",\n        window=-20,\n    ),\n)\n\nother = dict(\n    source=\"other\",\n    regex=[\n        r\"myelofibrose\",\n        r\"vaquez\",\n        r\"thrombocytem\\w+.{1,3}essentiell?e?\",\n        r\"splenomegal\\w+.{1,3}myeloide\",\n        r\"mastocytose.{1,5}maligne?\",\n        r\"polyglobul\\w+.{1,10}essentiell?e?\",\n        r\"letterer.?siwe\",\n        r\"anemie.refractaire.{1,20}blaste\",\n        r\"m[iy]elod[iy]splasi\",\n        r\"syndrome.myelo.?dysplasique\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    other,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/leukemia/#edsnlp.pipes.ner.disorders.leukemia.leukemia.LeukemiaMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/leukemia/#edsnlp.pipes.ner.disorders.leukemia.leukemia.LeukemiaMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.leukemia())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Sydrome my\u00e9loprolif\u00e9ratif\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [my\u00e9loprolif\u00e9ratif]\n</code></pre> <pre><code>text = \"Sydrome my\u00e9loprolif\u00e9ratif b\u00e9nin\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Patient atteint d'une LAM\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [LAM]\n</code></pre> <pre><code>text = \"Une maladie de Vaquez\"\ndoc = nlp(text)\nspans = doc.spans[\"leukemia\"]\n\nspans\n# Out: [Vaquez]\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/leukemia/#edsnlp.pipes.ner.disorders.leukemia.leukemia.LeukemiaMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'leukemia'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['leucemie?', '(sy...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>leukemia</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'leukemia': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/leukemia/#edsnlp.pipes.ner.disorders.leukemia.leukemia.LeukemiaMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.leukemia</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/leukemia/patterns/","title":"<code>edsnlp.pipes.ner.disorders.leukemia.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/","title":"<code>edsnlp.pipes.ner.disorders.liver_disease</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/factory/","title":"<code>edsnlp.pipes.ner.disorders.liver_disease.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/factory/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component","title":"<code>create_component = registry.factory.register('eds.liver_disease', assigns=['doc.ents', 'doc.spans'])(LiverDiseaseMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.liver_disease</code> pipeline component extracts mentions of liver disease.</p> Details of the used patterns <pre><code># fmt: off\nmild = dict(\n    source=\"mild\",\n    regex=[\n        r\"cholangites?.{1,10}(sclero|secondaire)\",\n        r\"fibrose.{1,10}(hepatique|foie)\",\n        r\"hepatite.{1,15}chroni\\w+\",\n        r\"hepatopath\\w+\",\n        r\"\\bnash\\b\",\n        r\"(maladie|sydrome?).{1,10}hanot\",\n        r\"surinfections?.{1,5}delta\",\n        r\"\\bcbp\\b\",\n        r\"\\bmaf\\b\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=\"\\bdots?\\b\",\n        window=-5,\n    ),\n)\n\nmoderate_severe = dict(\n    source=\"moderate_severe\",\n    regex=[\n        r\"cirr?hose\",\n        r\"necrose.{1,10}(hepati|foie)\",\n        r\"varice.{1,10}(estomac|oesopha|gastr)\",\n        r\"\\bvo\\b.{1,5}(stade|grade).(1|2|3|i{1,3})\",\n        r\"hypertension.{1,5}portale?\",\n        r\"scleroses?.{1,5}hepato\\s*portale?\",\n        r\"sydrome?.{1,10}hepato.?ren\",\n        r\"insuff?isance.{1,5}hepa\",\n        r\"encephalopath\\w+.{1,5}hepa\",\n        r\"\\btips\\b\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ntransplant = dict(\n    source=\"transplant\",\n    regex=[\n        r\"(?&lt;!pre.?)(gref?fe|transplant).{1,12}(hepatique|foie)\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=\"chc\",\n        window=(-5, 5),\n    ),\n)\n\ndefault_patterns = [\n    mild,\n    moderate_severe,\n    transplant,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/factory/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"MILD\"</code> for mild liver diseases</li> <li><code>\"MODERATE_TO_SEVERE\"</code> else</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/factory/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.liver_disease())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Il y a une fibrose h\u00e9patique\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [fibrose h\u00e9patique]\n</code></pre> <pre><code>text = \"Une h\u00e9patite B chronique\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [h\u00e9patite B chronique]\n</code></pre> <pre><code>text = \"Le patient consulte pour une cirrhose\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [cirrhose]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: MODERATE_TO_SEVERE\n</code></pre> <pre><code>text = \"Greffe h\u00e9patique.\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [Greffe h\u00e9patique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: MODERATE_TO_SEVERE\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/factory/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'mild', 'regex': ['cholangites?.{1,...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>liver_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'liver_disease': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/factory/#edsnlp.pipes.ner.disorders.liver_disease.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.liver_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/liver_disease/","title":"<code>edsnlp.pipes.ner.disorders.liver_disease.liver_disease</code>","text":"<p><code>eds.liver_disease</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/liver_disease/#edsnlp.pipes.ner.disorders.liver_disease.liver_disease.LiverDiseaseMatcher","title":"<code>LiverDiseaseMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.liver_disease</code> pipeline component extracts mentions of liver disease.</p> Details of the used patterns <pre><code># fmt: off\nmild = dict(\n    source=\"mild\",\n    regex=[\n        r\"cholangites?.{1,10}(sclero|secondaire)\",\n        r\"fibrose.{1,10}(hepatique|foie)\",\n        r\"hepatite.{1,15}chroni\\w+\",\n        r\"hepatopath\\w+\",\n        r\"\\bnash\\b\",\n        r\"(maladie|sydrome?).{1,10}hanot\",\n        r\"surinfections?.{1,5}delta\",\n        r\"\\bcbp\\b\",\n        r\"\\bmaf\\b\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=\"\\bdots?\\b\",\n        window=-5,\n    ),\n)\n\nmoderate_severe = dict(\n    source=\"moderate_severe\",\n    regex=[\n        r\"cirr?hose\",\n        r\"necrose.{1,10}(hepati|foie)\",\n        r\"varice.{1,10}(estomac|oesopha|gastr)\",\n        r\"\\bvo\\b.{1,5}(stade|grade).(1|2|3|i{1,3})\",\n        r\"hypertension.{1,5}portale?\",\n        r\"scleroses?.{1,5}hepato\\s*portale?\",\n        r\"sydrome?.{1,10}hepato.?ren\",\n        r\"insuff?isance.{1,5}hepa\",\n        r\"encephalopath\\w+.{1,5}hepa\",\n        r\"\\btips\\b\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ntransplant = dict(\n    source=\"transplant\",\n    regex=[\n        r\"(?&lt;!pre.?)(gref?fe|transplant).{1,12}(hepatique|foie)\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=\"chc\",\n        window=(-5, 5),\n    ),\n)\n\ndefault_patterns = [\n    mild,\n    moderate_severe,\n    transplant,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/liver_disease/#edsnlp.pipes.ner.disorders.liver_disease.liver_disease.LiverDiseaseMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"MILD\"</code> for mild liver diseases</li> <li><code>\"MODERATE_TO_SEVERE\"</code> else</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/liver_disease/#edsnlp.pipes.ner.disorders.liver_disease.liver_disease.LiverDiseaseMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.liver_disease())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Il y a une fibrose h\u00e9patique\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [fibrose h\u00e9patique]\n</code></pre> <pre><code>text = \"Une h\u00e9patite B chronique\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [h\u00e9patite B chronique]\n</code></pre> <pre><code>text = \"Le patient consulte pour une cirrhose\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [cirrhose]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: MODERATE_TO_SEVERE\n</code></pre> <pre><code>text = \"Greffe h\u00e9patique.\"\ndoc = nlp(text)\nspans = doc.spans[\"liver_disease\"]\n\nspans\n# Out: [Greffe h\u00e9patique]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: MODERATE_TO_SEVERE\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/liver_disease/#edsnlp.pipes.ner.disorders.liver_disease.liver_disease.LiverDiseaseMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'mild', 'regex': ['cholangites?.{1,...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>liver_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'liver_disease': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/liver_disease/#edsnlp.pipes.ner.disorders.liver_disease.liver_disease.LiverDiseaseMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.liver_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/liver_disease/patterns/","title":"<code>edsnlp.pipes.ner.disorders.liver_disease.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/","title":"<code>edsnlp.pipes.ner.disorders.lymphoma</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/factory/","title":"<code>edsnlp.pipes.ner.disorders.lymphoma.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/factory/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component","title":"<code>create_component = registry.factory.register('eds.lymphoma', assigns=['doc.ents', 'doc.spans'])(LymphomaMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.lymphoma</code> pipeline component extracts mentions of lymphoma.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"lymphom(?:.{1,10}hodgkin)\",\n        r\"lymphom\",\n        r\"lymphangio\",\n        r\"sezary\",\n        r\"burkitt?\",\n        r\"kaposi\",\n        r\"hodgkin\",\n        r\"amylose\",\n        r\"plasm[ao]cytome\",\n        r\"lympho.{1,3}sarcome\",\n        r\"lympho.?prolif\",\n        r\"hemopathie.{1,10}lymphoide\",\n        r\"macroglobulinemie\",\n        r\"imm?unocytome\",\n        r\"maladie.des.chaines?\",\n        r\"histi?ocytose.{1,5}(maligne|langerhans?)\",\n        r\"waldenst(ro|or)m\",\n        r\"mycos.{1,10}fongoide\",\n        r\"myelome\",\n        r\"maladie.{1,5}imm?uno\\s*proliferative.{1,5}maligne\",\n        r\"leucemie.{1,10}plasmocyte\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLNH\\b\",\n        r\"\\bLH\\b\",\n        r\"\\bEATL\\b\",\n        r\"\\bLAGC\\b\",\n        r\"\\bLDGCB\\b\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=[\"/L\", \"/mL\"],\n        window=10,\n    ),\n)\n\n\ngammapathy = dict(\n    source=\"gammapathy\",\n    regex=[\n        r\"gam?mapath\\w+\\s*monoclonale\",\n    ],\n    exclude=dict(\n        regex=[\n            \"benin\",\n            \"benign\",\n            \"signification.indeter\",\n            \"NMSI\",\n            \"MGUS\",\n        ],\n        window=(0, 5),\n    ),\n    regex_attr=\"NORM\",\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    # gammapathy,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/factory/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul> <p>Monoclonal gammapathy</p> <p>Monoclonal gammapathies are not extracted by this pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/factory/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.lymphoma())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Un lymphome de Hodgkin.\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [lymphome de Hodgkin]\n</code></pre> <pre><code>text = \"Atteint d'un Waldenst\u00f6rm\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [Waldenst\u00f6rm]\n</code></pre> <pre><code>text = \"Un LAGC\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [LAGC]\n</code></pre> <pre><code>text = \"anti LAGC: 10^4/mL\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/factory/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['lymphom(?:.{1,10...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>lymphoma</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'lymphoma': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/factory/#edsnlp.pipes.ner.disorders.lymphoma.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.lymphoma</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/lymphoma/","title":"<code>edsnlp.pipes.ner.disorders.lymphoma.lymphoma</code>","text":"<p><code>eds.lymphoma</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.lymphoma.LymphomaMatcher","title":"<code>LymphomaMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.lymphoma</code> pipeline component extracts mentions of lymphoma.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"lymphom(?:.{1,10}hodgkin)\",\n        r\"lymphom\",\n        r\"lymphangio\",\n        r\"sezary\",\n        r\"burkitt?\",\n        r\"kaposi\",\n        r\"hodgkin\",\n        r\"amylose\",\n        r\"plasm[ao]cytome\",\n        r\"lympho.{1,3}sarcome\",\n        r\"lympho.?prolif\",\n        r\"hemopathie.{1,10}lymphoide\",\n        r\"macroglobulinemie\",\n        r\"imm?unocytome\",\n        r\"maladie.des.chaines?\",\n        r\"histi?ocytose.{1,5}(maligne|langerhans?)\",\n        r\"waldenst(ro|or)m\",\n        r\"mycos.{1,10}fongoide\",\n        r\"myelome\",\n        r\"maladie.{1,5}imm?uno\\s*proliferative.{1,5}maligne\",\n        r\"leucemie.{1,10}plasmocyte\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bLNH\\b\",\n        r\"\\bLH\\b\",\n        r\"\\bEATL\\b\",\n        r\"\\bLAGC\\b\",\n        r\"\\bLDGCB\\b\",\n    ],\n    regex_attr=\"TEXT\",\n    exclude=dict(\n        regex=[\"/L\", \"/mL\"],\n        window=10,\n    ),\n)\n\n\ngammapathy = dict(\n    source=\"gammapathy\",\n    regex=[\n        r\"gam?mapath\\w+\\s*monoclonale\",\n    ],\n    exclude=dict(\n        regex=[\n            \"benin\",\n            \"benign\",\n            \"signification.indeter\",\n            \"NMSI\",\n            \"MGUS\",\n        ],\n        window=(0, 5),\n    ),\n    regex_attr=\"NORM\",\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    # gammapathy,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.lymphoma.LymphomaMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul> <p>Monoclonal gammapathy</p> <p>Monoclonal gammapathies are not extracted by this pipeline</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.lymphoma.LymphomaMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.lymphoma())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Un lymphome de Hodgkin.\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [lymphome de Hodgkin]\n</code></pre> <pre><code>text = \"Atteint d'un Waldenst\u00f6rm\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [Waldenst\u00f6rm]\n</code></pre> <pre><code>text = \"Un LAGC\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: [LAGC]\n</code></pre> <pre><code>text = \"anti LAGC: 10^4/mL\"\ndoc = nlp(text)\nspans = doc.spans[\"lymphoma\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.lymphoma.LymphomaMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['lymphom(?:.{1,10...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>lymphoma</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'lymphoma': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/lymphoma/#edsnlp.pipes.ner.disorders.lymphoma.lymphoma.LymphomaMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.lymphoma</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/lymphoma/patterns/","title":"<code>edsnlp.pipes.ner.disorders.lymphoma.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/","title":"<code>edsnlp.pipes.ner.disorders.myocardial_infarction</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/factory/","title":"<code>edsnlp.pipes.ner.disorders.myocardial_infarction.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/factory/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component","title":"<code>create_component = registry.factory.register('eds.myocardial_infarction', assigns=['doc.ents', 'doc.spans'])(MyocardialInfarctionMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.myocardial_infarction</code> pipeline component extracts mentions of myocardial infarction. It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Mentions of stents with a heart localization</li> </ul> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import HEART\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"coronaropath\\w+\",  # changed\n        r\"angor.{1,5}instable\",\n        r\"cardiopathie(?!.{0,20}non).{0,20}(ischem|arteriosc)\",\n        r\"cardio.?myopathie(?!.{0,20}non).{0,20}(ischem|arteriosc)\",\n        r\"ischemi.{1,15}myocard\",\n        r\"syndrome?.{1,5}corona.{1,10}aigu\",  # changed\n        r\"syndrome?.{1,5}corona.{1,10}st\",  # changed\n        r\"pontage.{1,5}mammaire\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"\\bstent\",\n        r\"endoprothese\",\n        r\"pontage\",\n        r\"anevr[iy]sme\",\n        r\"infa?r?a?ctus\",  # changed\n        r\"angioplast\\w+\",  # changed\n    ],\n    assign=[\n        dict(\n            name=\"heart_localized\",\n            regex=\"(\" + r\"|\".join(HEART) + \")\",\n            window=(-10, 10),\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bidm\\b\",\n        r\"\\bsca\\b\",\n        r\"\\batl\\b\",\n    ],\n    regex_attr=\"NORM\",\n    assign=dict(\n        name=\"segment\",\n        regex=r\"st([+-])\",\n        window=2,\n    ),\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    with_localization,\n    acronym,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/factory/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>heart_localized</code>: localization of the stent or bypass</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/factory/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.myocardial_infarction())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Une cardiopathie isch\u00e9mique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [cardiopathie isch\u00e9mique]\n</code></pre> <pre><code>text = \"Une cardiopathie non-isch\u00e9mique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un stent sur la marginale\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [stent sur la marginale]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'heart_localized': [marginale]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un stent p\u00e9riph\u00e9rique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"infarctus du myocarde\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [infarctus du myocarde]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'heart_localized': [myocarde]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/factory/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'myocardial_infarction'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['coronaropath\\\\w+...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>myocardial_infarction</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'myocardial_infarction': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/factory/#edsnlp.pipes.ner.disorders.myocardial_infarction.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.myocardial_infarction</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/myocardial_infarction/","title":"<code>edsnlp.pipes.ner.disorders.myocardial_infarction.myocardial_infarction</code>","text":"<p><code>eds.myocardial_infarction</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/myocardial_infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.myocardial_infarction.MyocardialInfarctionMatcher","title":"<code>MyocardialInfarctionMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.myocardial_infarction</code> pipeline component extracts mentions of myocardial infarction. It will notably match:</p> <ul> <li>Mentions of various diseases (see below)</li> <li>Mentions of stents with a heart localization</li> </ul> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import HEART\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"coronaropath\\w+\",  # changed\n        r\"angor.{1,5}instable\",\n        r\"cardiopathie(?!.{0,20}non).{0,20}(ischem|arteriosc)\",\n        r\"cardio.?myopathie(?!.{0,20}non).{0,20}(ischem|arteriosc)\",\n        r\"ischemi.{1,15}myocard\",\n        r\"syndrome?.{1,5}corona.{1,10}aigu\",  # changed\n        r\"syndrome?.{1,5}corona.{1,10}st\",  # changed\n        r\"pontage.{1,5}mammaire\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"\\bstent\",\n        r\"endoprothese\",\n        r\"pontage\",\n        r\"anevr[iy]sme\",\n        r\"infa?r?a?ctus\",  # changed\n        r\"angioplast\\w+\",  # changed\n    ],\n    assign=[\n        dict(\n            name=\"heart_localized\",\n            regex=\"(\" + r\"|\".join(HEART) + \")\",\n            window=(-10, 10),\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bidm\\b\",\n        r\"\\bsca\\b\",\n        r\"\\batl\\b\",\n    ],\n    regex_attr=\"NORM\",\n    assign=dict(\n        name=\"segment\",\n        regex=r\"st([+-])\",\n        window=2,\n    ),\n)\n\n\ndefault_patterns = [\n    main_pattern,\n    with_localization,\n    acronym,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/myocardial_infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.myocardial_infarction.MyocardialInfarctionMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>heart_localized</code>: localization of the stent or bypass</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/myocardial_infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.myocardial_infarction.MyocardialInfarctionMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.myocardial_infarction())\n</code></pre> <p>Below are a few examples:</p> 12345 <pre><code>text = \"Une cardiopathie isch\u00e9mique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [cardiopathie isch\u00e9mique]\n</code></pre> <pre><code>text = \"Une cardiopathie non-isch\u00e9mique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un stent sur la marginale\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [stent sur la marginale]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'heart_localized': [marginale]}\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un stent p\u00e9riph\u00e9rique\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"infarctus du myocarde\"\ndoc = nlp(text)\nspans = doc.spans[\"myocardial_infarction\"]\n\nspans\n# Out: [infarctus du myocarde]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'heart_localized': [myocarde]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/myocardial_infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.myocardial_infarction.MyocardialInfarctionMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'myocardial_infarction'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['coronaropath\\\\w+...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>myocardial_infarction</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'myocardial_infarction': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/myocardial_infarction/#edsnlp.pipes.ner.disorders.myocardial_infarction.myocardial_infarction.MyocardialInfarctionMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.myocardial_infarction</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/myocardial_infarction/patterns/","title":"<code>edsnlp.pipes.ner.disorders.myocardial_infarction.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/","title":"<code>edsnlp.pipes.ner.disorders.peptic_ulcer_disease</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/factory/","title":"<code>edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/factory/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component","title":"<code>create_component = registry.factory.register('eds.peptic_ulcer_disease', assigns=['doc.ents', 'doc.spans'])(PepticUlcerDiseaseMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.peptic_ulcer_disease</code> pipeline component extracts mentions of peptic ulcer disease.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"ulcere?.{1,10}gastr\",\n        r\"ulcere?.{1,10}duoden\",\n        r\"ulcere?.{1,10}antra\",\n        r\"ulcere?.{1,10}pept\",\n        r\"ulcere?.{1,10}estomac?\",\n        r\"ulcere?.{1,10}curling\",\n        r\"ulcere?.{1,10}bulb\",\n        r\"(\u0153|oe)sophagites?.{1,5}pepti.{1,10}ulcer\",\n        r\"gastrite.{1,20}ulcer\",\n        r\"antrite.{1,5}ulcer\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bUGD\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ngeneric = dict(\n    source=\"generic\",\n    regex=r\"ulcere?\",\n    regex_attr=\"NORM\",\n    assign=dict(\n        name=\"is_peptic\",\n        regex=r\"\\b(gastr|digest)\",\n        window=(-20, 20),\n        limit_to_sentence=False,\n    ),\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    generic,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/factory/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that matches, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/factory/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.peptic_ulcer_disease())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Beaucoup d'ulc\u00e8res gastriques\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [ulc\u00e8res gastriques]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'UGD\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [UGD]\n</code></pre> <pre><code>text = \"La patient \u00e0 des ulc\u00e8res\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Au niveau gastrique: blabla blabla blabla blabla blabla quelques ulc\u00e8res\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [gastrique: blabla blabla blabla blabla blabla quelques ulc\u00e8res]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'is_peptic': [gastrique]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/factory/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['ulcere?.{1,10}ga...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>peptic_ulcer_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'peptic_ulcer_disease': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/factory/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.peptic_ulcer_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/patterns/","title":"<code>edsnlp.pipes.ner.disorders.peptic_ulcer_disease.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/peptic_ulcer_disease/","title":"<code>edsnlp.pipes.ner.disorders.peptic_ulcer_disease.peptic_ulcer_disease</code>","text":"<p><code>eds.peptic_ulcer_disease</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/peptic_ulcer_disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.peptic_ulcer_disease.PepticUlcerDiseaseMatcher","title":"<code>PepticUlcerDiseaseMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.peptic_ulcer_disease</code> pipeline component extracts mentions of peptic ulcer disease.</p> Details of the used patterns <pre><code># fmt: off\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"ulcere?.{1,10}gastr\",\n        r\"ulcere?.{1,10}duoden\",\n        r\"ulcere?.{1,10}antra\",\n        r\"ulcere?.{1,10}pept\",\n        r\"ulcere?.{1,10}estomac?\",\n        r\"ulcere?.{1,10}curling\",\n        r\"ulcere?.{1,10}bulb\",\n        r\"(\u0153|oe)sophagites?.{1,5}pepti.{1,10}ulcer\",\n        r\"gastrite.{1,20}ulcer\",\n        r\"antrite.{1,5}ulcer\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bUGD\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\ngeneric = dict(\n    source=\"generic\",\n    regex=r\"ulcere?\",\n    regex_attr=\"NORM\",\n    assign=dict(\n        name=\"is_peptic\",\n        regex=r\"\\b(gastr|digest)\",\n        window=(-20, 20),\n        limit_to_sentence=False,\n    ),\n)\n\ndefault_patterns = [\n    main_pattern,\n    acronym,\n    generic,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/peptic_ulcer_disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.peptic_ulcer_disease.PepticUlcerDiseaseMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that matches, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/peptic_ulcer_disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.peptic_ulcer_disease.PepticUlcerDiseaseMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.peptic_ulcer_disease())\n</code></pre> <p>Below are a few examples:</p> 1234 <pre><code>text = \"Beaucoup d'ulc\u00e8res gastriques\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [ulc\u00e8res gastriques]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'UGD\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [UGD]\n</code></pre> <pre><code>text = \"La patient \u00e0 des ulc\u00e8res\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Au niveau gastrique: blabla blabla blabla blabla blabla quelques ulc\u00e8res\"\ndoc = nlp(text)\nspans = doc.spans[\"peptic_ulcer_disease\"]\n\nspans\n# Out: [gastrique: blabla blabla blabla blabla blabla quelques ulc\u00e8res]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'is_peptic': [gastrique]}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/peptic_ulcer_disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.peptic_ulcer_disease.PepticUlcerDiseaseMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'main', 'regex': ['ulcere?.{1,10}ga...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>peptic_ulcer_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'peptic_ulcer_disease': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/peptic_ulcer_disease/peptic_ulcer_disease/#edsnlp.pipes.ner.disorders.peptic_ulcer_disease.peptic_ulcer_disease.PepticUlcerDiseaseMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.peptic_ulcer_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/","title":"<code>edsnlp.pipes.ner.disorders.peripheral_vascular_disease</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/factory/","title":"<code>edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/factory/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component","title":"<code>create_component = registry.factory.register('eds.peripheral_vascular_disease', assigns=['doc.ents', 'doc.spans'])(PeripheralVascularDiseaseMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.peripheral_vascular_disease</code> pipeline component extracts mentions of peripheral vascular disease.</p> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import ASYMPTOMATIC, BRAIN, HEART, PERIPHERAL\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bAOMI\\b\",\n        r\"\\bACOM\\b\",\n        r\"\\bTAO\\b\",\n        r\"\\bSAPL\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n        r\"\\bSCS\\b\",\n        r\"\\bTVP\\b\",\n        r\"\\bCAPS\\b\",\n        r\"\\bMTEV\\b\",\n        r\"\\bPTT\\b\",\n        r\"\\bMAT\\b\",\n        r\"\\bSHU\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nother = dict(\n    source=\"other\",\n    regex=[\n        r\"\\bbuerger\",\n        r\"takayasu\",\n        r\"\\bhorton\",\n        r\"wegener\",\n        r\"churg.{1,10}strauss\",\n        r\"\\bsnedd?on\",\n        r\"budd.chiari\",\n        r\"infa?r?a?ctus.{1,5}(renal|spleni\\w+|polaire|pulmo)\",\n        r\"ulcere?.{1,5}arter\",\n        r\"syndrome?.?hemolytique.{1,8}uremi\\w+\",\n        r\"granulomatose.{1,10}polyangeite\",\n        r\"occlusion.{1,10}(artere?|veine).{1,20}retine\",\n        r\"syndrome?.{1,20}anti.?phospho\",\n        r\"embolie.{1,5}pulm\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"angiopath\\w+\",\n        r\"arteriopathies?.{1,5}obliterante?\",\n        r\"gangren\",\n        r\"claudication\",\n        r\"dissection.{1,10}(aort|arter)\",\n        r\"tromboangeit\",\n        r\"tromboarterit\",\n        r\"(pontage|angioplastie).{1,10}(\\bfem|\\bpop|\\bren|\\bjamb)\",\n        r\"arterite\",\n        r\"(ischemie|infa?r?a?ctus).{1,10}mesenteri\\w+\",\n        r\"endarteriectom\\w+\",\n        r\"vascularite\",\n        r\"occlusion.{1,10}terminaisons?\\s*carotid\",\n        r\"cryoglobulinemie\",\n        r\"colite.{1,5}ischemi\",\n        r\"embole.{1,10}cholesterol\",\n        r\"purpura.?thrombopenique.?idiopa\",\n        r\"micro.?angiopathie.?th?rombotique\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART + ASYMPTOMATIC + [r\"inr\\srecommande\\ssous\\savk\"],\n            window=(-8, 8),\n            limit_to_sentence=False,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nthrombosis = dict(\n    source=\"thrombosis\",\n    regex=[\n        r\"thrombos\",\n        r\"thrombol[^y]\",\n        r\"thrombophi\",\n        r\"thrombi[^n]\",\n        r\"thrombus\",\n        r\"thrombectomi\",\n        r\"thrombo.?embo\",\n        r\"phlebit\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART + [\"superficiel\", \"\\biv\\b\", \"intra.?vein\"],\n            window=(-15, 15),\n            limit_to_sentence=False,\n        ),\n        dict(\n            regex=[\n                \"pre\",\n                \"anti\",\n                \"bilan\",\n            ],\n            window=-4,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\n\nischemia = dict(\n    source=\"ischemia\",\n    regex=[\n        r\"ischemi\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART,\n            window=(-7, 7),\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"peripheral\",\n            regex=\"(\" + r\"|\".join(PERIPHERAL) + \")\",\n            window=15,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nep = dict(\n    source=\"ep\",\n    regex=r\"\\bEP(?![\\w\\./-])\",\n    regex_attr=\"TEXT\",\n    exclude=[\n        dict(\n            regex=[\n                r\"fibreux\",\n                r\"retin\",\n                r\"\\bfove\",\n                r\"\\boct\\b\",\n                r\"\\bmacula\",\n                r\"prosta\",\n                r\"\\bip\\b\",\n                r\"protocole\",\n                r\"seance\",\n                r\"echange\",\n                r\"ritux\",\n                r\"ivig\",\n                r\"ig.?iv\",\n                r\"\\bctc\",\n                r\"corticoide\",\n                r\"serum\",\n                r\"\\bcure\",\n                r\"plasma\",\n                r\"mensuel\",\n                r\"semaine\",\n                r\"serologi\",\n                r\"espaces.porte\",\n                r\"projet\",\n                r\"bolus\",\n            ],\n            window=(-25, 25),\n            limit_to_sentence=False,\n            regex_attr=\"NORM\",\n        ),\n        dict(\n            regex=[r\"rdv\", r\"les\", r\"des\", r\"angine\"],\n            window=(-3, 0),\n            regex_attr=\"NORM\",\n        ),\n    ],\n)\n\nhypertension = dict(\n    source=\"main\",\n    regex=[\n        r\"\\bhta\\b\",\n        r\"hyper.?tension.?arte\",\n        r\"hyper.?tendu\",\n        r\"hyper.?tension.?essenti\",\n        r\"hypertensi\",\n    ],\n    exclude=dict(\n        regex=\"(pulmo|porta)\",\n        window=3,\n    ),\n)\n\ndefault_patterns = [\n    acronym,\n    other,\n    with_localization,\n    thrombosis,\n    ep,\n    ischemia,\n    hypertension,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/factory/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/factory/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.peripheral_vascular_disease())\n</code></pre> <p>Below are a few examples:</p> 12345678910111213 <pre><code>text = \"Un AOMI\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [AOMI]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un infarctus r\u00e9nal\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [infarctus r\u00e9nal]\n</code></pre> <pre><code>text = \"Une angiopathie c\u00e9r\u00e9brale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une angiopathie\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [angiopathie]\n</code></pre> <pre><code>text = \"Une thrombose c\u00e9r\u00e9brale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une thrombose des veines superficielles\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une thrombose\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [thrombose]\n</code></pre> <pre><code>text = \"Effectuer un bilan pre-trombose\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une isch\u00e9mie des MI est remarqu\u00e9e.\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [isch\u00e9mie des MI]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'peripheral': [MI]}\n</code></pre> <pre><code>text = \"Plusieurs cas d'EP\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [EP]\n</code></pre> <pre><code>text = \"Effectuer des cures d'EP\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Le patient est hypertendu\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [hypertendu]\n</code></pre> <pre><code>text = \"Une hypertension portale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/factory/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'acronym', 'regex': ['\\\\bAOMI\\\\b', ...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>peripheral_vascular_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'peripheral_vascular_disease': T...</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/factory/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.peripheral_vascular_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/patterns/","title":"<code>edsnlp.pipes.ner.disorders.peripheral_vascular_disease.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/peripheral_vascular_disease/","title":"<code>edsnlp.pipes.ner.disorders.peripheral_vascular_disease.peripheral_vascular_disease</code>","text":"<p><code>eds.peripheral_vascular_disease</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/peripheral_vascular_disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.peripheral_vascular_disease.PeripheralVascularDiseaseMatcher","title":"<code>PeripheralVascularDiseaseMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.peripheral_vascular_disease</code> pipeline component extracts mentions of peripheral vascular disease.</p> Details of the used patterns <pre><code># fmt: off\nfrom ..terms import ASYMPTOMATIC, BRAIN, HEART, PERIPHERAL\n\nacronym = dict(\n    source=\"acronym\",\n    regex=[\n        r\"\\bAOMI\\b\",\n        r\"\\bACOM\\b\",\n        r\"\\bTAO\\b\",\n        r\"\\bSAPL\\b\",\n        r\"\\bOACR\\b\",\n        r\"\\bOVCR\\b\",\n        r\"\\bSCS\\b\",\n        r\"\\bTVP\\b\",\n        r\"\\bCAPS\\b\",\n        r\"\\bMTEV\\b\",\n        r\"\\bPTT\\b\",\n        r\"\\bMAT\\b\",\n        r\"\\bSHU\\b\",\n    ],\n    regex_attr=\"TEXT\",\n)\n\nother = dict(\n    source=\"other\",\n    regex=[\n        r\"\\bbuerger\",\n        r\"takayasu\",\n        r\"\\bhorton\",\n        r\"wegener\",\n        r\"churg.{1,10}strauss\",\n        r\"\\bsnedd?on\",\n        r\"budd.chiari\",\n        r\"infa?r?a?ctus.{1,5}(renal|spleni\\w+|polaire|pulmo)\",\n        r\"ulcere?.{1,5}arter\",\n        r\"syndrome?.?hemolytique.{1,8}uremi\\w+\",\n        r\"granulomatose.{1,10}polyangeite\",\n        r\"occlusion.{1,10}(artere?|veine).{1,20}retine\",\n        r\"syndrome?.{1,20}anti.?phospho\",\n        r\"embolie.{1,5}pulm\",\n    ],\n    regex_attr=\"NORM\",\n)\n\nwith_localization = dict(\n    source=\"with_localization\",\n    regex=[\n        r\"angiopath\\w+\",\n        r\"arteriopathies?.{1,5}obliterante?\",\n        r\"gangren\",\n        r\"claudication\",\n        r\"dissection.{1,10}(aort|arter)\",\n        r\"tromboangeit\",\n        r\"tromboarterit\",\n        r\"(pontage|angioplastie).{1,10}(\\bfem|\\bpop|\\bren|\\bjamb)\",\n        r\"arterite\",\n        r\"(ischemie|infa?r?a?ctus).{1,10}mesenteri\\w+\",\n        r\"endarteriectom\\w+\",\n        r\"vascularite\",\n        r\"occlusion.{1,10}terminaisons?\\s*carotid\",\n        r\"cryoglobulinemie\",\n        r\"colite.{1,5}ischemi\",\n        r\"embole.{1,10}cholesterol\",\n        r\"purpura.?thrombopenique.?idiopa\",\n        r\"micro.?angiopathie.?th?rombotique\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART + ASYMPTOMATIC + [r\"inr\\srecommande\\ssous\\savk\"],\n            window=(-8, 8),\n            limit_to_sentence=False,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nthrombosis = dict(\n    source=\"thrombosis\",\n    regex=[\n        r\"thrombos\",\n        r\"thrombol[^y]\",\n        r\"thrombophi\",\n        r\"thrombi[^n]\",\n        r\"thrombus\",\n        r\"thrombectomi\",\n        r\"thrombo.?embo\",\n        r\"phlebit\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART + [\"superficiel\", \"\\biv\\b\", \"intra.?vein\"],\n            window=(-15, 15),\n            limit_to_sentence=False,\n        ),\n        dict(\n            regex=[\n                \"pre\",\n                \"anti\",\n                \"bilan\",\n            ],\n            window=-4,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\n\nischemia = dict(\n    source=\"ischemia\",\n    regex=[\n        r\"ischemi\",\n    ],\n    exclude=[\n        dict(\n            regex=BRAIN + HEART,\n            window=(-7, 7),\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"peripheral\",\n            regex=\"(\" + r\"|\".join(PERIPHERAL) + \")\",\n            window=15,\n        ),\n    ],\n    regex_attr=\"NORM\",\n)\n\nep = dict(\n    source=\"ep\",\n    regex=r\"\\bEP(?![\\w\\./-])\",\n    regex_attr=\"TEXT\",\n    exclude=[\n        dict(\n            regex=[\n                r\"fibreux\",\n                r\"retin\",\n                r\"\\bfove\",\n                r\"\\boct\\b\",\n                r\"\\bmacula\",\n                r\"prosta\",\n                r\"\\bip\\b\",\n                r\"protocole\",\n                r\"seance\",\n                r\"echange\",\n                r\"ritux\",\n                r\"ivig\",\n                r\"ig.?iv\",\n                r\"\\bctc\",\n                r\"corticoide\",\n                r\"serum\",\n                r\"\\bcure\",\n                r\"plasma\",\n                r\"mensuel\",\n                r\"semaine\",\n                r\"serologi\",\n                r\"espaces.porte\",\n                r\"projet\",\n                r\"bolus\",\n            ],\n            window=(-25, 25),\n            limit_to_sentence=False,\n            regex_attr=\"NORM\",\n        ),\n        dict(\n            regex=[r\"rdv\", r\"les\", r\"des\", r\"angine\"],\n            window=(-3, 0),\n            regex_attr=\"NORM\",\n        ),\n    ],\n)\n\nhypertension = dict(\n    source=\"main\",\n    regex=[\n        r\"\\bhta\\b\",\n        r\"hyper.?tension.?arte\",\n        r\"hyper.?tendu\",\n        r\"hyper.?tension.?essenti\",\n        r\"hypertensi\",\n    ],\n    exclude=dict(\n        regex=\"(pulmo|porta)\",\n        window=3,\n    ),\n)\n\ndefault_patterns = [\n    acronym,\n    other,\n    with_localization,\n    thrombosis,\n    ep,\n    ischemia,\n    hypertension,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/peripheral_vascular_disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.peripheral_vascular_disease.PeripheralVascularDiseaseMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to None</li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/peripheral_vascular_disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.peripheral_vascular_disease.PeripheralVascularDiseaseMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.peripheral_vascular_disease())\n</code></pre> <p>Below are a few examples:</p> 12345678910111213 <pre><code>text = \"Un AOMI\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [AOMI]\n</code></pre> <pre><code>text = \"Pr\u00e9sence d'un infarctus r\u00e9nal\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [infarctus r\u00e9nal]\n</code></pre> <pre><code>text = \"Une angiopathie c\u00e9r\u00e9brale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une angiopathie\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [angiopathie]\n</code></pre> <pre><code>text = \"Une thrombose c\u00e9r\u00e9brale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une thrombose des veines superficielles\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une thrombose\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [thrombose]\n</code></pre> <pre><code>text = \"Effectuer un bilan pre-trombose\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Une isch\u00e9mie des MI est remarqu\u00e9e.\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [isch\u00e9mie des MI]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'peripheral': [MI]}\n</code></pre> <pre><code>text = \"Plusieurs cas d'EP\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [EP]\n</code></pre> <pre><code>text = \"Effectuer des cures d'EP\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Le patient est hypertendu\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: [hypertendu]\n</code></pre> <pre><code>text = \"Une hypertension portale\"\ndoc = nlp(text)\nspans = doc.spans[\"peripheral_vascular_disease\"]\n\nspans\n# Out: []\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/peripheral_vascular_disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.peripheral_vascular_disease.PeripheralVascularDiseaseMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> DEFAULT: <code>[{'source': 'acronym', 'regex': ['\\\\bAOMI\\\\b', ...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>peripheral_vascular_disease</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'peripheral_vascular_disease': T...</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/peripheral_vascular_disease/peripheral_vascular_disease/#edsnlp.pipes.ner.disorders.peripheral_vascular_disease.peripheral_vascular_disease.PeripheralVascularDiseaseMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.peripheral_vascular_disease</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/","title":"<code>edsnlp.pipes.ner.disorders.solid_tumor</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/factory/","title":"<code>edsnlp.pipes.ner.disorders.solid_tumor.factory</code>","text":"<ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li><li><p><p>Kempf E., Priou S., Lam\u00e9 G., Daniel C., Bellamine A., Sommacale D., Belkacemi y., Bey R., Galula G., Taright N., Tannier X., Rance B., Flicoteaux R., Hemery F., Audureau E., Chatellier G. and Tournigand C., 2022. Impact of two waves of Sars-Cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: A French multicentric cohort study from a large group of University hospitals. {International Journal of Cancer}. 150, pp.1609-1618. 10.1002/ijc.33928</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/factory/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component","title":"<code>create_component = registry.factory.register('eds.solid_tumor', assigns=['doc.ents', 'doc.spans'])(SolidTumorMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.solid_tumor</code> pipeline component extracts mentions of solid tumors. It will notably match:</p> Details of the used patterns <pre><code># fmt: off\nBENINE = r\"benign|benin|(grade.?\\b[i1]\\b)\"\nSTAGE = r\"stade ([^\\s]*)\"\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"carcinom(?!.{0,10}in.?situ)\",\n        r\"seminome\",\n        r\"(?&lt;!lympho)(?&lt;!lympho-)sarcome\",\n        r\"blastome\",\n        r\"cancer([^o]|\\s|\\b)\",\n        r\"adamantinome\",\n        r\"chordome\",\n        r\"cranio\\s*pharyngiome\",\n        r\"melanome\",\n        r\"neoplasie\",\n        r\"neoplasme\",\n        r\"linite\",\n        r\"melanome\",\n        r\"mesoth?eliome\",\n        # r\"mesotheliome\", #removed same as above\n        # r\"seminome\", #removed same as above\n        r\"myxome\",\n        r\"paragangliome\",\n        # r\"craniopharyngiome\",  #removed same as above\n        r\"k\\s*.{0,5}(prostate|sein)\",\n        r\"pancoast.?tobias\",\n        r\"syndrome?.{1,10}lynch\",\n        r\"li.?fraumeni\",\n        r\"germinome\",\n        r\"adeno[\\s-]?k\",\n        r\"thymome\",\n        r\"\\bnut\\b\",\n        r\"\\bgist\\b\",\n        r\"\\bchc\\b\",\n        r\"\\badk\\b\",\n        r\"\\btves\\b\",\n        r\"\\btv.tves\\b\",\n        r\"lesion.{1,20}tumor\",\n        r\"tumeur\",\n        r\"carcinoid\",\n        r\"histiocytome\",\n        r\"ependymome\",\n        # r\"primitif\", Trop de FP\n    ],\n    exclude=dict(\n        regex=BENINE,\n        window=(0, 5),\n    ),\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"metastasis\",\n            regex=r\"(metasta|multinodul)\",\n            window=(-3, 7),\n            reduce_mode=\"keep_last\",\n        ),\n        dict(\n            name=\"stage\",\n            regex=STAGE,\n            window=7,\n            reduce_mode=\"keep_last\",\n        ),\n    ],\n)\n\nmetastasis_pattern = dict(\n    source=\"metastasis\",\n    regex=[\n        r\"cellule.{1,5}tumorale.{1,5}circulantes\",\n        r\"metasta\",\n        r\"multinodul\",\n        r\"carcinose\",\n        r\"ruptures?.{1,5}corticale\",\n        r\"envahissement.{0,15}parties\\s*molle\",\n        r\"(localisation|lesion)s?.{0,20}second\",\n        r\"(lymphangite|meningite).{1,5}carcinomateuse\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=r\"goitre\",\n        window=-3,\n    ),\n)\n\n# Patterns developed for CT-Scan reports\nmetastasis_ct_scan = dict(\n    source=\"metastasis_ct_scan\",\n    regex=[\n        r\"(?i)(m[\u00e9e]tasta(se|tique)s?)\",\n        r\"(diss[\u00e9e]min[\u00e9e]e?s?)\",\n        r\"(carcinose)\",\n        r\"(((allure|l[\u00e9e]sion|localisation|progression)s?\\s)(suspecte?s?)?.{0,50}(secondaire)s?)\",\n        r\"(l(a|\u00e2)ch(\u00e9|e|er)\\sde\\sballons?)\",\n        r\"(l[\u00e9e]sions?\\s(non\\s)?cibles?)\",\n        r\"(rupture.{1,20}corticale)\",\n        r\"(envahissement.{0,15}parties\\smolles)\",\n        r\"((l[i,y]se).{1,20}os)|ost[e\u00e9]ol[i,y]|rupture.{1,20}corticale|envahissement.{1,20}parties\\smolles|ost[e\u00e9]ocondensa.{1,20}(suspect|secondaire|[\u00e9e]volutive)\",\n        r\"(l[\u00e9e]sion|anomalie|image).{1,20}os.{1,30}(suspect|secondaire|[\u00e9e]volutive)\",\n        r\"os.{1,30}(l[\u00e9e]sion|anomalie|image).{1,20}(suspect|secondaire|[\u00e9e]volutive)\",\n        r\"(l[\u00e9e]sion|anomalie|image).{1,20}l[i,y]tique\",\n        r\"(l[\u00e9e]sion|anomalie|image).{1,20}condensant.{1,20}(suspect|secondaire|[\u00e9e]volutive)\",\n        r\"fracture.{1,30}(suspect|secondaire|[\u00e9e]volutive)\",\n        r\"((l[\u00e9e]sion|anomalie|image|nodule).{1,80}(secondaire))\",\n        r\"((l[\u00e9e]sion|anomalie|image|nodule)s.{1,40}suspec?ts?)\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    metastasis_pattern,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/factory/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"METASTASIS\"</code> for tumors at the metastatic stage</li> <li><code>\"LOCALIZED\"</code> else</li> </ul> </li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>stage</code>: stage of the tumor</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/factory/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.solid_tumor())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Pr\u00e9sence d'un carcinome intra-h\u00e9patique.\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [carcinome]\n</code></pre> <pre><code>text = \"Patient avec un K sein.\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [K sein]\n</code></pre> <pre><code>text = \"Il y a une tumeur b\u00e9nigne\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Tumeur m\u00e9tastas\u00e9e\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Tumeur m\u00e9tastas\u00e9e]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n\nspan._.assigned\n# Out: {'metastasis': m\u00e9tastas\u00e9e}\n</code></pre> <pre><code>text = \"Cancer du poumon au stade 4\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Cancer du poumon au stade 4]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n\nspan._.assigned\n# Out: {'stage': 4}\n</code></pre> <pre><code>text = \"Cancer du poumon au stade 2\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Cancer du poumon au stade 2]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': 2}\n</code></pre> <pre><code>text = \"Pr\u00e9sence de nombreuses l\u00e9sions secondaires\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [l\u00e9sions secondaires]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/factory/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'solid_tumor'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['carcinom(?!.{0,1...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>solid_tumor</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'solid_tumor': True}</code> </p> <code>use_tnm</code> <p>Whether to use TNM scores matching as well</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_patterns_metastasis_ct_scan</code> <p>Whether to use the metastasis patterns developed for the CT-Scans</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/factory/#edsnlp.pipes.ner.disorders.solid_tumor.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.solid_tumor</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024 and Kempf et al., 2022.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/patterns/","title":"<code>edsnlp.pipes.ner.disorders.solid_tumor.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/solid_tumor/","title":"<code>edsnlp.pipes.ner.disorders.solid_tumor.solid_tumor</code>","text":"<p><code>eds.solid_tumor</code> pipeline</p> <ol><li><p><p>Petit-Jean T., G\u00e9rardin C., Berthelot E., Chatellier G., Frank M., Tannier X., Kempf E. and Bey R., 2024. Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions. Journal of the American Medical Informatics Association. 31, pp.1280-1290. 10.1093/jamia/ocae069</p></p></li><li><p><p>Kempf E., Priou S., Lam\u00e9 G., Daniel C., Bellamine A., Sommacale D., Belkacemi y., Bey R., Galula G., Taright N., Tannier X., Rance B., Flicoteaux R., Hemery F., Audureau E., Chatellier G. and Tournigand C., 2022. Impact of two waves of Sars-Cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: A French multicentric cohort study from a large group of University hospitals. {International Journal of Cancer}. 150, pp.1609-1618. 10.1002/ijc.33928</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/solid_tumor/#edsnlp.pipes.ner.disorders.solid_tumor.solid_tumor.SolidTumorMatcher","title":"<code>SolidTumorMatcher</code>","text":"<p>           Bases: <code>DisorderMatcher</code></p> <p>The <code>eds.solid_tumor</code> pipeline component extracts mentions of solid tumors. It will notably match:</p> Details of the used patterns <pre><code># fmt: off\nBENINE = r\"benign|benin|(grade.?\\b[i1]\\b)\"\nSTAGE = r\"stade ([^\\s]*)\"\n\nmain_pattern = dict(\n    source=\"main\",\n    regex=[\n        r\"carcinom(?!.{0,10}in.?situ)\",\n        r\"seminome\",\n        r\"(?&lt;!lympho)(?&lt;!lympho-)sarcome\",\n        r\"blastome\",\n        r\"cancer([^o]|\\s|\\b)\",\n        r\"adamantinome\",\n        r\"chordome\",\n        r\"cranio\\s*pharyngiome\",\n        r\"melanome\",\n        r\"neoplasie\",\n        r\"neoplasme\",\n        r\"linite\",\n        r\"melanome\",\n        r\"mesoth?eliome\",\n        # r\"mesotheliome\", #removed same as above\n        # r\"seminome\", #removed same as above\n        r\"myxome\",\n        r\"paragangliome\",\n        # r\"craniopharyngiome\",  #removed same as above\n        r\"k\\s*.{0,5}(prostate|sein)\",\n        r\"pancoast.?tobias\",\n        r\"syndrome?.{1,10}lynch\",\n        r\"li.?fraumeni\",\n        r\"germinome\",\n        r\"adeno[\\s-]?k\",\n        r\"thymome\",\n        r\"\\bnut\\b\",\n        r\"\\bgist\\b\",\n        r\"\\bchc\\b\",\n        r\"\\badk\\b\",\n        r\"\\btves\\b\",\n        r\"\\btv.tves\\b\",\n        r\"lesion.{1,20}tumor\",\n        r\"tumeur\",\n        r\"carcinoid\",\n        r\"histiocytome\",\n        r\"ependymome\",\n        # r\"primitif\", Trop de FP\n    ],\n    exclude=dict(\n        regex=BENINE,\n        window=(0, 5),\n    ),\n    regex_attr=\"NORM\",\n    assign=[\n        dict(\n            name=\"metastasis\",\n            regex=r\"(metasta|multinodul)\",\n            window=(-3, 7),\n            reduce_mode=\"keep_last\",\n        ),\n        dict(\n            name=\"stage\",\n            regex=STAGE,\n            window=7,\n            reduce_mode=\"keep_last\",\n        ),\n    ],\n)\n\nmetastasis_pattern = dict(\n    source=\"metastasis\",\n    regex=[\n        r\"cellule.{1,5}tumorale.{1,5}circulantes\",\n        r\"metasta\",\n        r\"multinodul\",\n        r\"carcinose\",\n        r\"ruptures?.{1,5}corticale\",\n        r\"envahissement.{0,15}parties\\s*molle\",\n        r\"(localisation|lesion)s?.{0,20}second\",\n        r\"(lymphangite|meningite).{1,5}carcinomateuse\",\n    ],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=r\"goitre\",\n        window=-3,\n    ),\n)\n\n# Patterns developed for CT-Scan reports\nmetastasis_ct_scan = dict(\n    source=\"metastasis_ct_scan\",\n    regex=[\n        r\"(?i)(m[\u00e9e]tasta(se|tique)s?)\",\n        r\"(diss[\u00e9e]min[\u00e9e]e?s?)\",\n        r\"(carcinose)\",\n        r\"(((allure|l[\u00e9e]sion|localisation|progression)s?\\s)(suspecte?s?)?.{0,50}(secondaire)s?)\",\n        r\"(l(a|\u00e2)ch(\u00e9|e|er)\\sde\\sballons?)\",\n        r\"(l[\u00e9e]sions?\\s(non\\s)?cibles?)\",\n        r\"(rupture.{1,20}corticale)\",\n        r\"(envahissement.{0,15}parties\\smolles)\",\n        r\"((l[i,y]se).{1,20}os)|ost[e\u00e9]ol[i,y]|rupture.{1,20}corticale|envahissement.{1,20}parties\\smolles|ost[e\u00e9]ocondensa.{1,20}(suspect|secondaire|[\u00e9e]volutive)\",\n        r\"(l[\u00e9e]sion|anomalie|image).{1,20}os.{1,30}(suspect|secondaire|[\u00e9e]volutive)\",\n        r\"os.{1,30}(l[\u00e9e]sion|anomalie|image).{1,20}(suspect|secondaire|[\u00e9e]volutive)\",\n        r\"(l[\u00e9e]sion|anomalie|image).{1,20}l[i,y]tique\",\n        r\"(l[\u00e9e]sion|anomalie|image).{1,20}condensant.{1,20}(suspect|secondaire|[\u00e9e]volutive)\",\n        r\"fracture.{1,30}(suspect|secondaire|[\u00e9e]volutive)\",\n        r\"((l[\u00e9e]sion|anomalie|image|nodule).{1,80}(secondaire))\",\n        r\"((l[\u00e9e]sion|anomalie|image|nodule)s.{1,40}suspec?ts?)\",\n    ],\n    regex_attr=\"NORM\",\n)\n\ndefault_patterns = [\n    main_pattern,\n    metastasis_pattern,\n]\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/solid_tumor/#edsnlp.pipes.ner.disorders.solid_tumor.solid_tumor.SolidTumorMatcher--extensions","title":"Extensions","text":"<p>On each span <code>span</code> that match, the following attributes are available:</p> <ul> <li><code>span._.detailed_status</code>: set to either<ul> <li><code>\"METASTASIS\"</code> for tumors at the metastatic stage</li> <li><code>\"LOCALIZED\"</code> else</li> </ul> </li> <li><code>span._.assigned</code>: dictionary with the following keys, if relevant:<ul> <li><code>stage</code>: stage of the tumor</li> </ul> </li> </ul>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/solid_tumor/#edsnlp.pipes.ner.disorders.solid_tumor.solid_tumor.SolidTumorMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(\n    eds.normalizer(\n        accents=True,\n        lowercase=True,\n        quotes=True,\n        spaces=True,\n        pollution=dict(\n            information=True,\n            bars=True,\n            biology=True,\n            doctors=True,\n            web=True,\n            coding=True,\n            footer=True,\n        ),\n    ),\n)\nnlp.add_pipe(eds.solid_tumor())\n</code></pre> <p>Below are a few examples:</p> 1234567 <pre><code>text = \"Pr\u00e9sence d'un carcinome intra-h\u00e9patique.\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [carcinome]\n</code></pre> <pre><code>text = \"Patient avec un K sein.\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [K sein]\n</code></pre> <pre><code>text = \"Il y a une tumeur b\u00e9nigne\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: []\n</code></pre> <pre><code>text = \"Tumeur m\u00e9tastas\u00e9e\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Tumeur m\u00e9tastas\u00e9e]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n\nspan._.assigned\n# Out: {'metastasis': m\u00e9tastas\u00e9e}\n</code></pre> <pre><code>text = \"Cancer du poumon au stade 4\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Cancer du poumon au stade 4]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n\nspan._.assigned\n# Out: {'stage': 4}\n</code></pre> <pre><code>text = \"Cancer du poumon au stade 2\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [Cancer du poumon au stade 2]\n\nspan = spans[0]\n\nspan._.assigned\n# Out: {'stage': 2}\n</code></pre> <pre><code>text = \"Pr\u00e9sence de nombreuses l\u00e9sions secondaires\"\ndoc = nlp(text)\nspans = doc.spans[\"solid_tumor\"]\n\nspans\n# Out: [l\u00e9sions secondaires]\n\nspan = spans[0]\n\nspan._.detailed_status\n# Out: METASTASIS\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/solid_tumor/#edsnlp.pipes.ner.disorders.solid_tumor.solid_tumor.SolidTumorMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'solid_tumor'</code> </p> <code>patterns</code> <p>The patterns to use for matching</p> <p> TYPE: <code>FullConfig</code> DEFAULT: <code>[{'source': 'main', 'regex': ['carcinom(?!.{0,1...</code> </p> <code>label</code> <p>The label to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>solid_tumor</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'solid_tumor': True}</code> </p> <code>use_tnm</code> <p>Whether to use TNM scores matching as well</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_patterns_metastasis_ct_scan</code> <p>Whether to use the metastasis patterns developed for the CT-Scans</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/ner/disorders/solid_tumor/solid_tumor/#edsnlp.pipes.ner.disorders.solid_tumor.solid_tumor.SolidTumorMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.solid_tumor</code> component was developed by AP-HP's Data Science team with a team of medical experts, following the insights of the algorithm proposed by Petit-Jean et al., 2024 and Kempf et al., 2022.</p>"},{"location":"reference/edsnlp/pipes/ner/disorders/terms/","title":"<code>edsnlp.pipes.ner.disorders.terms</code>","text":""},{"location":"reference/edsnlp/pipes/ner/drugs/","title":"<code>edsnlp.pipes.ner.drugs</code>","text":""},{"location":"reference/edsnlp/pipes/ner/drugs/factory/","title":"<code>edsnlp.pipes.ner.drugs.factory</code>","text":"<ol><li><p><p>Cossin S., Lebrun L., Lobre G., Loustau R., Jouhet V., Griffier R., Mougin F., Diallo G. and Thiessard F., 2019. Romedi: An Open Data Source About French Drugs on the Semantic Web. {Studies in Health Technology and Informatics}. 264, pp.79-82. 10.3233/SHTI190187</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/drugs/factory/#edsnlp.pipes.ner.drugs.factory.create_component","title":"<code>create_component</code>","text":"<p>The <code>eds.drugs</code> pipeline component detects mentions of French drugs (brand names and active ingredients) and adds them to <code>doc.ents</code>. Each drug is mapped to an ATC code through the Romedi terminology (Cossin et al., 2019). The ATC classifies drugs into groups.</p>"},{"location":"reference/edsnlp/pipes/ner/drugs/factory/#edsnlp.pipes.ner.drugs.factory.create_component--examples","title":"Examples","text":"<p>In this example, we are looking for an oral antidiabetic medication (ATC code: A10B).</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.drugs(term_matcher=\"exact\"))\n\ntext = \"Traitement habituel: Kard\u00e9gic, cardensiel (bisoprolol), glucophage, lasilix\"\n\ndoc = nlp(text)\n\ndrugs_detected = [(x.text, x.kb_id_) for x in doc.ents]\n\ndrugs_detected[0]\n# Out: ('Kard\u00e9gic', 'B01AC06')\n\nlen(drugs_detected)\n# Out: 5\n\noral_antidiabetics_detected = list(\n    filter(lambda x: (x[1].startswith(\"A10B\")), drugs_detected)\n)\noral_antidiabetics_detected\n# Out: [('glucophage', 'A10BA02')]\n</code></pre> <p>Glucophage is the brand name of a medication that contains metformine, the first-line medication for the treatment of type 2 diabetes.</p>"},{"location":"reference/edsnlp/pipes/ner/drugs/factory/#edsnlp.pipes.ner.drugs.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'drugs'</code> </p> <code>attr</code> <p>The default attribute to use for matching.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>Literal['exact', 'simstring']</code> DEFAULT: <code>'exact'</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'drug'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'drug': True}</code> </p> RETURNS DESCRIPTION <code>TerminologyMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/drugs/factory/#edsnlp.pipes.ner.drugs.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.drugs</code> pipeline was developed by the IAM team and CHU de Bordeaux's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/ner/drugs/patterns/","title":"<code>edsnlp.pipes.ner.drugs.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/","title":"<code>edsnlp.pipes.ner.scores</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/base_score/","title":"<code>edsnlp.pipes.ner.scores.base_score</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/base_score/#edsnlp.pipes.ner.scores.base_score.SimpleScoreMatcher","title":"<code>SimpleScoreMatcher</code>","text":"<p>           Bases: <code>ContextualMatcher</code></p> <p>Matcher component to extract a numeric score</p>"},{"location":"reference/edsnlp/pipes/ner/scores/base_score/#edsnlp.pipes.ner.scores.base_score.SimpleScoreMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>label</code> <p>The name of the extracted score</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>None</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>score_name</code> <p>Deprecated, use <code>label</code> instead. The name of the extracted score</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>Optional[SpanSetterArg]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/ner/scores/base_score/#edsnlp.pipes.ner.scores.base_score.SimpleScoreMatcher.process","title":"<code>process</code>","text":"<p>Extracts, if available, the value of the score. Normalizes the score via the provided <code>self.score_normalization</code> method.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/base_score/#edsnlp.pipes.ner.scores.base_score.SimpleScoreMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>Document to process</p> <p> TYPE: <code>Doc</code> </p> YIELDS DESCRIPTION <code>Span</code> <p>Matches with, if found, an added <code>score_value</code> extension</p>"},{"location":"reference/edsnlp/pipes/ner/scores/charlson/","title":"<code>edsnlp.pipes.ner.scores.charlson</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/charlson/factory/","title":"<code>edsnlp.pipes.ner.scores.charlson.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/charlson/factory/#edsnlp.pipes.ner.scores.charlson.factory.create_component","title":"<code>create_component</code>","text":"<p>The <code>eds.charlson</code> component extracts the Charlson Comorbidity Index.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/charlson/factory/#edsnlp.pipes.ner.scores.charlson.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.charlson())\n\ntext = \"\"\"\nCharlson \u00e0 l'admission: 7.\nCharlson:\nOMS:\n\"\"\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (Charlson \u00e0 l'admission: 7,)\n</code></pre> <p>We can see that only one occurrence was extracted. The second mention of Charlson in the text doesn't contain any numerical value, so it isn't extracted.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/charlson/factory/#edsnlp.pipes.ner.scores.charlson.factory.create_component--extensions","title":"Extensions","text":"<p>Each extraction exposes 2 extensions:</p> <pre><code>ent = doc.ents[0]\n\nent._.score_name\n# Out: 'charlson'\n\nent._.score_value\n# Out: 7\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/scores/charlson/factory/#edsnlp.pipes.ner.scores.charlson.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'charlson'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'charlson'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'charlson': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/scores/charlson/patterns/","title":"<code>edsnlp.pipes.ner.scores.charlson.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/charlson/patterns/#edsnlp.pipes.ner.scores.charlson.patterns.score_normalization","title":"<code>score_normalization</code>","text":"<p>Charlson score normalization. If available, returns the integer value of the Charlson score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/elston_ellis/","title":"<code>edsnlp.pipes.ner.scores.elston_ellis</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/elston_ellis/factory/","title":"<code>edsnlp.pipes.ner.scores.elston_ellis.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/elston_ellis/factory/#edsnlp.pipes.ner.scores.elston_ellis.factory.create_component","title":"<code>create_component</code>","text":"<p>Matcher for the Elston-Ellis score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/elston_ellis/factory/#edsnlp.pipes.ner.scores.elston_ellis.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.elston_ellis())\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/scores/elston_ellis/factory/#edsnlp.pipes.ner.scores.elston_ellis.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'elston_ellis'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'TEXT'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'elston_ellis'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'elston_ellis': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/scores/elston_ellis/patterns/","title":"<code>edsnlp.pipes.ner.scores.elston_ellis.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/elston_ellis/patterns/#edsnlp.pipes.ner.scores.elston_ellis.patterns.score_normalization","title":"<code>score_normalization</code>","text":"<p>Elston and Ellis score normalization. If available, returns the integer value of the Elston and Ellis score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/","title":"<code>edsnlp.pipes.ner.scores.emergency</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/ccmu/","title":"<code>edsnlp.pipes.ner.scores.emergency.ccmu</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/ccmu/factory/","title":"<code>edsnlp.pipes.ner.scores.emergency.ccmu.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/ccmu/factory/#edsnlp.pipes.ner.scores.emergency.ccmu.factory.create_component","title":"<code>create_component</code>","text":"<p>Matcher for explicit mentions of the French CCMU emergency score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/ccmu/factory/#edsnlp.pipes.ner.scores.emergency.ccmu.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.emergency_ccmu())\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/ccmu/factory/#edsnlp.pipes.ner.scores.emergency.ccmu.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'emergency_ccmu'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value otherwise</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_ccmu'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'emergency_ccmu': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/ccmu/patterns/","title":"<code>edsnlp.pipes.ner.scores.emergency.ccmu.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/ccmu/patterns/#edsnlp.pipes.ner.scores.emergency.ccmu.patterns.score_normalization","title":"<code>score_normalization</code>","text":"<p>CCMU score normalization. If available, returns the integer value of the CCMU score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/gemsa/","title":"<code>edsnlp.pipes.ner.scores.emergency.gemsa</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/gemsa/factory/","title":"<code>edsnlp.pipes.ner.scores.emergency.gemsa.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/gemsa/factory/#edsnlp.pipes.ner.scores.emergency.gemsa.factory.create_component","title":"<code>create_component</code>","text":"<p>Matcher for explicit mentions of the French GEMSA emergency score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/gemsa/factory/#edsnlp.pipes.ner.scores.emergency.gemsa.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.emergency_gemsa())\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/gemsa/factory/#edsnlp.pipes.ner.scores.emergency.gemsa.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_gemsa'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value otherwise</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_gemsa'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'emergency_gemsa': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/gemsa/patterns/","title":"<code>edsnlp.pipes.ner.scores.emergency.gemsa.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/gemsa/patterns/#edsnlp.pipes.ner.scores.emergency.gemsa.patterns.score_normalization","title":"<code>score_normalization</code>","text":"<p>GEMSA score normalization. If available, returns the integer value of the GEMSA score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/priority/","title":"<code>edsnlp.pipes.ner.scores.emergency.priority</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/priority/factory/","title":"<code>edsnlp.pipes.ner.scores.emergency.priority.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/priority/factory/#edsnlp.pipes.ner.scores.emergency.priority.factory.create_component","title":"<code>create_component</code>","text":"<p>Matcher for explicit mentions of the French priority emergency score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/priority/factory/#edsnlp.pipes.ner.scores.emergency.priority.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.emergency_priority())\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/priority/factory/#edsnlp.pipes.ner.scores.emergency.priority.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_priority'</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency_priority'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'emergency_priority': True}</code> </p> RETURNS DESCRIPTION <code>SimpleScoreMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/scores/emergency/priority/patterns/","title":"<code>edsnlp.pipes.ner.scores.emergency.priority.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/emergency/priority/patterns/#edsnlp.pipes.ner.scores.emergency.priority.patterns.score_normalization","title":"<code>score_normalization</code>","text":"<p>Priority score normalization. If available, returns the integer value of the priority score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/factory/","title":"<code>edsnlp.pipes.ner.scores.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/factory/#edsnlp.pipes.ner.scores.factory.create_component","title":"<code>create_component = registry.factory.register('eds.score', assigns=['doc.ents', 'doc.spans'], deprecated=['score'])(SimpleScoreMatcher)</code>  <code>module-attribute</code>","text":"<p>Matcher component to extract a numeric score</p>"},{"location":"reference/edsnlp/pipes/ner/scores/factory/#edsnlp.pipes.ner.scores.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>label</code> <p>The name of the extracted score</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>None</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>score_name</code> <p>Deprecated, use <code>label</code> instead. The name of the extracted score</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>Optional[SpanSetterArg]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/ner/scores/sofa/","title":"<code>edsnlp.pipes.ner.scores.sofa</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/sofa/factory/","title":"<code>edsnlp.pipes.ner.scores.sofa.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/sofa/factory/#edsnlp.pipes.ner.scores.sofa.factory.create_component","title":"<code>create_component</code>","text":"<p>The <code>eds.sofa</code> component extracts Sequential Organ Failure Assessment (SOFA) scores, used to track a person's status during the stay in an intensive care unit to determine the extent of a person's organ function or rate failure.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/sofa/factory/#edsnlp.pipes.ner.scores.sofa.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sofa())\n\ntext = \"\"\"\nSOFA (\u00e0 24H) : 12.\nOMS:\n\"\"\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (SOFA (\u00e0 24H) : 12,)\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/scores/sofa/factory/#edsnlp.pipes.ner.scores.sofa.factory.create_component--extensions","title":"Extensions","text":"<p>Each extraction exposes 3 extensions:</p> <pre><code>ent = doc.ents[0]\n\nent._.score_name\n# Out: 'sofa'\n\nent._.score_value\n# Out: 12\n\nent._.score_method\n# Out: '24H'\n</code></pre> <p>Score method can here be \"24H\", \"Maximum\", \"A l'admission\" or \"Non pr\u00e9cis\u00e9e\"</p>"},{"location":"reference/edsnlp/pipes/ner/scores/sofa/factory/#edsnlp.pipes.ner.scores.sofa.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'sofa'</code> </p> <code>regex</code> <p>A list of regexes to identify the SOFA score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('CUSTOM_NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex to extract the score value</p> <p> TYPE: <code>Dict[str, str]</code> DEFAULT: <code>value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex, and should return - None if no score could be extracted - The desired score value else</p> <p> TYPE: <code>Callable[[Union[str, None]], Any]</code> DEFAULT: <code>score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Flags to pass to the regex</p> <p> TYPE: <code>Union[RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'sofa'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'sofa': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/scores/sofa/patterns/","title":"<code>edsnlp.pipes.ner.scores.sofa.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/sofa/patterns/#edsnlp.pipes.ner.scores.sofa.patterns.score_normalization","title":"<code>score_normalization</code>","text":"<p>Sofa score normalization. If available, returns the integer value of the SOFA score.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/sofa/sofa/","title":"<code>edsnlp.pipes.ner.scores.sofa.sofa</code>","text":""},{"location":"reference/edsnlp/pipes/ner/scores/sofa/sofa/#edsnlp.pipes.ner.scores.sofa.sofa.SofaMatcher","title":"<code>SofaMatcher</code>","text":"<p>           Bases: <code>SimpleScoreMatcher</code></p>"},{"location":"reference/edsnlp/pipes/ner/scores/sofa/sofa/#edsnlp.pipes.ner.scores.sofa.sofa.SofaMatcher.process","title":"<code>process</code>","text":"<p>Extracts, if available, the value of the score. Normalizes the score via the provided <code>self.score_normalization</code> method.</p>"},{"location":"reference/edsnlp/pipes/ner/scores/sofa/sofa/#edsnlp.pipes.ner.scores.sofa.sofa.SofaMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>Document to process</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>ents</code> <p>List of spaCy's spans, with, if found, an added <code>score_value</code> extension</p> <p> TYPE: <code>List[Span]</code> </p>"},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/","title":"<code>edsnlp.pipes.ner.suicide_attempt</code>","text":""},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/factory/","title":"<code>edsnlp.pipes.ner.suicide_attempt.factory</code>","text":"<ol><li><p><p>Bey R., Cohen A., Trebossen V., Dura B., Geoffroy P., Jean C., Landman B., Petit-Jean T., Chatellier G., Sallah K., Tannier X., Bourmaud A. and Delorme R., 2024. Natural language processing of multi-hospital electronic health records for public health surveillance of suicidality. 10.1038/s44184-023-00046-7</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/factory/#edsnlp.pipes.ner.suicide_attempt.factory.create_component","title":"<code>create_component = registry.factory.register('eds.suicide_attempt', assigns=['doc.ents', 'doc.spans'])(SuicideAttemptMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.suicide_attempt</code> pipeline component detects mentions of Suicide Attempt. It can be used with a span qualifier for contextualisation of the entity (history) and to detect false positives as negation, hypothesis or family. We recommend to use a machine learning qualifier to disambiguate polysemic words, as proposed in Bey et al., 2024.</p> <p>Every matched entity will be labelled <code>suicide_attempt</code>.</p>"},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/factory/#edsnlp.pipes.ner.suicide_attempt.factory.create_component--extensions","title":"Extensions","text":"<p>Each entity span will have the suicide attempt modality as an attribute. The available modalities are:</p> <ul> <li><code>suicide_attempt_unspecific</code></li> <li><code>autolysis</code></li> <li><code>intentional_drug_overdose</code></li> <li><code>jumping_from_height</code></li> <li><code>cuts</code></li> <li><code>strangling</code></li> <li><code>self_destructive_behavior</code></li> <li><code>burn_gas_caustic</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/factory/#edsnlp.pipes.ner.suicide_attempt.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.suicide_attempt())\n\ntext = \"J'ai vu le patient \u00e0 cause d'une IMV avec paracetamol\"\ndoc = nlp(text)\ndoc.ents\n# Out: (IMV,)\n\nent = doc.ents[0]\nent._.suicide_attempt_modality\n# Out: 'intentional_drug_overdose'\n</code></pre> Patterns used for the named entity recognition <pre><code># fmt: off\npatterns = {\n    \"suicide_attempt_unspecific\": [\n        r\"\\b(?&lt;!\\.)(?&lt;!Voie\\s\\d\\s\\:\\s)(?&lt;!Voie\\sd.abord\\s\\:\\s)(?&lt;!surface\\s)(?&lt;!d[\u00e9e]sorientation\\s)(?&lt;!abord\\s)(?&lt;!ECG\\s:\\s)(?&lt;!volume\\s)(?&lt;!\\d\\s[mc]m\\sde\\sla\\s)(?&lt;!\\d[mc]m\\sde\\sla\\s)(?&lt;!au\\scontact\\sde\\sla\\s)T\\.?S\\.?(?![\\.A-Za-z])(?!\\sapyr[e\u00e9]tique)(?!.+TRANSSEPTAL)(?!.+T[34])(?!.+en\\sr.gression)\\b\",\n        r\"(?&lt;!\\.)T\\.S\\.(?![A-Za-z])\",\n        r\"\\b(?&lt;!.)TS\\.\\B\",\n        r\"(?i)tentative[s]?\\s+de\\s+sui?cide\",\n        r\"(?i)tent[\u00e9e]\\s+de\\s+((se\\s+(suicider|tuer))|(mettre\\s+fin\\s+[\u00e0a]\\s+((ses\\s+jours?)|(sa\\s+vie))))\",\n    ],\n    \"autolysis\": [r\"(?i)tentative\\s+d'autolyse\", r\"(?i)autolyse\"],\n    \"intentional_drug_overdose\": [\n        r\"(?i)(intoxication|ingestion)\\s+m[\u00e9e]dicamenteuse\\s+volontaire\",\n        r\"(?i)\\b(i\\.?m\\.?v\\.?)\\b\",\n        r\"(?i)(intoxication|ingestion)\\s*([a-zA-Z0-9_\u00e9\u00e0\u00e8\u00f4\u00ea\\-]+\\s*){0,3}\\s*volontaire\",\n        r\"TS\\s+med\\s+polymedicamenteuse\",\n        r\"TS\\s+(poly)?([\\s-])?m[\u00e9e]dicamenteuse\",\n    ],\n    \"jumping_from_height\": [\n        r\"(?i)tentative[s]?\\s+de\\s+d[\u00e9e]fenestration\",\n        r\"(?i)(?&lt;!id[\u00e9e]es?\\sde\\s)d[\u00e9e]fenestration(?!\\saccidentelle)\",\n        r\"(?i)d[\u00e9e]fenestration\\s+volontaire\",\n        r\"(?i)d[\u00e9e]fenestration\\s+intentionnelle\",\n        r\"(?i)jet.r?\\sd.un\\spont\",\n    ],\n    \"cuts\": [r\"(?i)phl[\u00e9e]botomie\"],\n    \"strangling\": [r\"(?i)pendaison\"],\n    \"self_destructive_behavior\": [r\"(?i)autodestruction\"],\n    \"burn_gas_caustic\": [r\"(?i)ing[e\u00e9]stion\\sde\\s(produit\\s)?caustique\"],\n}\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/factory/#edsnlp.pipes.ner.suicide_attempt.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'eds.suicide_attempt'</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'TEXT'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>patterns</code> <p>The regex pattern to use</p> <p> TYPE: <code>List[str]</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'suicide_attempt'</code> </p> RETURNS DESCRIPTION <code>SuicideAttemptMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/factory/#edsnlp.pipes.ner.suicide_attempt.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.suicide_attempt</code> component was developed by AP-HP's Data Science team, following the insights of the algorithm proposed by Bey et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/patterns/","title":"<code>edsnlp.pipes.ner.suicide_attempt.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/suicide_attempt/","title":"<code>edsnlp.pipes.ner.suicide_attempt.suicide_attempt</code>","text":"<ol><li><p><p>Bey R., Cohen A., Trebossen V., Dura B., Geoffroy P., Jean C., Landman B., Petit-Jean T., Chatellier G., Sallah K., Tannier X., Bourmaud A. and Delorme R., 2024. Natural language processing of multi-hospital electronic health records for public health surveillance of suicidality. 10.1038/s44184-023-00046-7</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/suicide_attempt/#edsnlp.pipes.ner.suicide_attempt.suicide_attempt.SuicideAttemptMatcher","title":"<code>SuicideAttemptMatcher</code>","text":"<p>           Bases: <code>BaseNERComponent</code></p> <p>The <code>eds.suicide_attempt</code> pipeline component detects mentions of Suicide Attempt. It can be used with a span qualifier for contextualisation of the entity (history) and to detect false positives as negation, hypothesis or family. We recommend to use a machine learning qualifier to disambiguate polysemic words, as proposed in Bey et al., 2024.</p> <p>Every matched entity will be labelled <code>suicide_attempt</code>.</p>"},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/suicide_attempt/#edsnlp.pipes.ner.suicide_attempt.suicide_attempt.SuicideAttemptMatcher--extensions","title":"Extensions","text":"<p>Each entity span will have the suicide attempt modality as an attribute. The available modalities are:</p> <ul> <li><code>suicide_attempt_unspecific</code></li> <li><code>autolysis</code></li> <li><code>intentional_drug_overdose</code></li> <li><code>jumping_from_height</code></li> <li><code>cuts</code></li> <li><code>strangling</code></li> <li><code>self_destructive_behavior</code></li> <li><code>burn_gas_caustic</code></li> </ul>"},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/suicide_attempt/#edsnlp.pipes.ner.suicide_attempt.suicide_attempt.SuicideAttemptMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.suicide_attempt())\n\ntext = \"J'ai vu le patient \u00e0 cause d'une IMV avec paracetamol\"\ndoc = nlp(text)\ndoc.ents\n# Out: (IMV,)\n\nent = doc.ents[0]\nent._.suicide_attempt_modality\n# Out: 'intentional_drug_overdose'\n</code></pre> Patterns used for the named entity recognition <pre><code># fmt: off\npatterns = {\n    \"suicide_attempt_unspecific\": [\n        r\"\\b(?&lt;!\\.)(?&lt;!Voie\\s\\d\\s\\:\\s)(?&lt;!Voie\\sd.abord\\s\\:\\s)(?&lt;!surface\\s)(?&lt;!d[\u00e9e]sorientation\\s)(?&lt;!abord\\s)(?&lt;!ECG\\s:\\s)(?&lt;!volume\\s)(?&lt;!\\d\\s[mc]m\\sde\\sla\\s)(?&lt;!\\d[mc]m\\sde\\sla\\s)(?&lt;!au\\scontact\\sde\\sla\\s)T\\.?S\\.?(?![\\.A-Za-z])(?!\\sapyr[e\u00e9]tique)(?!.+TRANSSEPTAL)(?!.+T[34])(?!.+en\\sr.gression)\\b\",\n        r\"(?&lt;!\\.)T\\.S\\.(?![A-Za-z])\",\n        r\"\\b(?&lt;!.)TS\\.\\B\",\n        r\"(?i)tentative[s]?\\s+de\\s+sui?cide\",\n        r\"(?i)tent[\u00e9e]\\s+de\\s+((se\\s+(suicider|tuer))|(mettre\\s+fin\\s+[\u00e0a]\\s+((ses\\s+jours?)|(sa\\s+vie))))\",\n    ],\n    \"autolysis\": [r\"(?i)tentative\\s+d'autolyse\", r\"(?i)autolyse\"],\n    \"intentional_drug_overdose\": [\n        r\"(?i)(intoxication|ingestion)\\s+m[\u00e9e]dicamenteuse\\s+volontaire\",\n        r\"(?i)\\b(i\\.?m\\.?v\\.?)\\b\",\n        r\"(?i)(intoxication|ingestion)\\s*([a-zA-Z0-9_\u00e9\u00e0\u00e8\u00f4\u00ea\\-]+\\s*){0,3}\\s*volontaire\",\n        r\"TS\\s+med\\s+polymedicamenteuse\",\n        r\"TS\\s+(poly)?([\\s-])?m[\u00e9e]dicamenteuse\",\n    ],\n    \"jumping_from_height\": [\n        r\"(?i)tentative[s]?\\s+de\\s+d[\u00e9e]fenestration\",\n        r\"(?i)(?&lt;!id[\u00e9e]es?\\sde\\s)d[\u00e9e]fenestration(?!\\saccidentelle)\",\n        r\"(?i)d[\u00e9e]fenestration\\s+volontaire\",\n        r\"(?i)d[\u00e9e]fenestration\\s+intentionnelle\",\n        r\"(?i)jet.r?\\sd.un\\spont\",\n    ],\n    \"cuts\": [r\"(?i)phl[\u00e9e]botomie\"],\n    \"strangling\": [r\"(?i)pendaison\"],\n    \"self_destructive_behavior\": [r\"(?i)autodestruction\"],\n    \"burn_gas_caustic\": [r\"(?i)ing[e\u00e9]stion\\sde\\s(produit\\s)?caustique\"],\n}\n\n# fmt: on\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/suicide_attempt/#edsnlp.pipes.ner.suicide_attempt.suicide_attempt.SuicideAttemptMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'eds.suicide_attempt'</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'TEXT'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>patterns</code> <p>The regex pattern to use</p> <p> TYPE: <code>List[str]</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'suicide_attempt'</code> </p> RETURNS DESCRIPTION <code>SuicideAttemptMatcher</code>"},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/suicide_attempt/#edsnlp.pipes.ner.suicide_attempt.suicide_attempt.SuicideAttemptMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.suicide_attempt</code> component was developed by AP-HP's Data Science team, following the insights of the algorithm proposed by Bey et al., 2024.</p>"},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/suicide_attempt/#edsnlp.pipes.ner.suicide_attempt.suicide_attempt.SuicideAttemptMatcher.process","title":"<code>process</code>","text":"<p>Find matching spans in doc.</p>"},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/suicide_attempt/#edsnlp.pipes.ner.suicide_attempt.suicide_attempt.SuicideAttemptMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>Doc object.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>spans</code> <p>List of Spans returned by the matchers.</p> <p> TYPE: <code>List[Span]</code> </p>"},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/suicide_attempt/#edsnlp.pipes.ner.suicide_attempt.suicide_attempt.SuicideAttemptMatcher.__call__","title":"<code>__call__</code>","text":"<p>Adds spans to document.</p>"},{"location":"reference/edsnlp/pipes/ner/suicide_attempt/suicide_attempt/#edsnlp.pipes.ner.suicide_attempt.suicide_attempt.SuicideAttemptMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>Doc object, annotated with suicide attempts entities.</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/ner/tnm/","title":"<code>edsnlp.pipes.ner.tnm</code>","text":""},{"location":"reference/edsnlp/pipes/ner/tnm/factory/","title":"<code>edsnlp.pipes.ner.tnm.factory</code>","text":"<ol><li><p><p>Kempf E., Priou S., Lam\u00e9 G., Daniel C., Bellamine A., Sommacale D., Belkacemi y., Bey R., Galula G., Taright N., Tannier X., Rance B., Flicoteaux R., Hemery F., Audureau E., Chatellier G. and Tournigand C., 2022. Impact of two waves of Sars-Cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: A French multicentric cohort study from a large group of University hospitals. {International Journal of Cancer}. 150, pp.1609-1618. 10.1002/ijc.33928</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/tnm/factory/#edsnlp.pipes.ner.tnm.factory.create_component","title":"<code>create_component = registry.factory.register('eds.tnm', assigns=['doc.ents', 'doc.spans'], deprecated=['eds.TNM'])(TNMMatcher)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.tnm</code> component extracts TNM mentions from clinical documents.</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/factory/#edsnlp.pipes.ner.tnm.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.tnm())\n\ntext = \"TNM: pTx N1 M1\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (pTx N1 M1,)\n\nent = doc.ents[0]\nent._.tnm.dict()\n# {'modifier': 'p',\n#  'tumour': None,\n#  'tumour_specification': 'x',\n#  'node': '1',\n#  'node_specification': None,\n#  'metastasis': '1',\n#  'resection_completeness': None,\n#  'version': None,\n#  'version_year': None}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/tnm/factory/#edsnlp.pipes.ner.tnm.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'tnm'</code> </p> <code>pattern</code> <p>The regex pattern to use for matching ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>(?:\\b|^)(?&lt;=\\(?(?P&lt;version&gt;uicc|accj|tnm|UICC|A...</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>tnm</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'tnm': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/tnm/factory/#edsnlp.pipes.ner.tnm.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The TNM score is based on the development of S. Priou, B. Rance and E. Kempf (Kempf et al., 2022).</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/model/","title":"<code>edsnlp.pipes.ner.tnm.model</code>","text":""},{"location":"reference/edsnlp/pipes/ner/tnm/model/#edsnlp.pipes.ner.tnm.model.TNM","title":"<code>TNM</code>","text":"<p>           Bases: <code>BaseModel</code></p>"},{"location":"reference/edsnlp/pipes/ner/tnm/model/#edsnlp.pipes.ner.tnm.model.TNM.dict","title":"<code>dict</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/patterns/","title":"<code>edsnlp.pipes.ner.tnm.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/","title":"<code>edsnlp.pipes.ner.tnm.tnm</code>","text":"<p><code>eds.tnm</code> pipeline.</p> <ol><li><p><p>Kempf E., Priou S., Lam\u00e9 G., Daniel C., Bellamine A., Sommacale D., Belkacemi y., Bey R., Galula G., Taright N., Tannier X., Rance B., Flicoteaux R., Hemery F., Audureau E., Chatellier G. and Tournigand C., 2022. Impact of two waves of Sars-Cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: A French multicentric cohort study from a large group of University hospitals. {International Journal of Cancer}. 150, pp.1609-1618. 10.1002/ijc.33928</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher","title":"<code>TNMMatcher</code>","text":"<p>           Bases: <code>BaseNERComponent</code></p> <p>The <code>eds.tnm</code> component extracts TNM mentions from clinical documents.</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.tnm())\n\ntext = \"TNM: pTx N1 M1\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (pTx N1 M1,)\n\nent = doc.ents[0]\nent._.tnm.dict()\n# {'modifier': 'p',\n#  'tumour': None,\n#  'tumour_specification': 'x',\n#  'node': '1',\n#  'node_specification': None,\n#  'metastasis': '1',\n#  'resection_completeness': None,\n#  'version': None,\n#  'version_year': None}\n</code></pre>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'tnm'</code> </p> <code>pattern</code> <p>The regex pattern to use for matching ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>(?:\\b|^)(?&lt;=\\(?(?P&lt;version&gt;uicc|accj|tnm|UICC|A...</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>TEXT</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>tnm</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'tnm': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher--authors-and-citation","title":"Authors and citation","text":"<p>The TNM score is based on the development of S. Priou, B. Rance and E. Kempf (Kempf et al., 2022).</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher.set_extensions","title":"<code>set_extensions</code>","text":"<p>Set spaCy extensions</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher.process","title":"<code>process</code>","text":"<p>Find TNM mentions in doc.</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher.process--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>spans</code> <p>list of tnm spans</p> <p> TYPE: <code>List[Span]</code> </p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher.parse","title":"<code>parse</code>","text":"<p>Parse dates using the groupdict returned by the matcher.</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher.parse--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>spans</code> <p>List of tuples containing the spans and groupdict returned by the matcher.</p> <p> TYPE: <code>List[Tuple[Span, Dict[str, str]]]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>List of processed spans, with the date parsed.</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher.__call__","title":"<code>__call__</code>","text":"<p>Tags TNM mentions.</p>"},{"location":"reference/edsnlp/pipes/ner/tnm/tnm/#edsnlp.pipes.ner.tnm.tnm.TNMMatcher.__call__--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for TNM</p> <p> TYPE: <code>Doc</code> </p>"},{"location":"reference/edsnlp/pipes/ner/umls/","title":"<code>edsnlp.pipes.ner.umls</code>","text":""},{"location":"reference/edsnlp/pipes/ner/umls/factory/","title":"<code>edsnlp.pipes.ner.umls.factory</code>","text":""},{"location":"reference/edsnlp/pipes/ner/umls/factory/#edsnlp.pipes.ner.umls.factory.create_component","title":"<code>create_component</code>","text":"<p>The <code>eds.umls</code> pipeline component matches the UMLS (Unified Medical Language System from NIH) terminology.</p> <p>Very low recall</p> <p>When using the <code>exact</code> matching mode, this component has a very poor recall performance. We can use the <code>simstring</code> mode to retrieve approximate matches, albeit at the cost of a significantly higher computation time.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/factory/#edsnlp.pipes.ner.umls.factory.create_component--examples","title":"Examples","text":"<p><code>eds.umls</code> is an additional module that needs to be setup by:</p> <ol> <li><code>pip install -U umls_downloader</code></li> <li>Signing up for a UMLS Terminology    Services Account. After filling a short form, you will receive your token API    within a few days.</li> <li>Set <code>UMLS_API_KEY</code> locally: <code>export UMLS_API_KEY=your_api_key</code></li> </ol> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.umls())\n\ntext = \"Grosse toux: le malade a \u00e9t\u00e9 mordu par des Amphibiens \" \"sous le genou\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (toux, a, par, Amphibiens, genou)\n\nent = doc.ents[0]\n\nent.label_\n# Out: umls\n\nent._.umls\n# Out: C0010200\n</code></pre> <p>You can easily change the default languages and sources with the <code>pattern_config</code> argument:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\n# Load the French MeSH\npattern_config = dict(languages=[\"FRE\"], sources=[\"MSHFRE\"])\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.umls(pattern_config=pattern_config))\n</code></pre> <p>See more options of languages and sources here.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/factory/#edsnlp.pipes.ner.umls.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'umls'</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The term matcher to use, either \"exact\" or \"simstring\"</p> <p> TYPE: <code>TerminologyTermMatcher</code> DEFAULT: <code>'exact'</code> </p> <code>term_matcher_config</code> <p>The configuration for the term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>pattern_config</code> <p>The pattern retriever configuration</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>dict(languages=['FRE'], sources=None)</code> </p> <code>label</code> <p>Label name to use for the <code>Span</code> object and the extension</p> <p> TYPE: <code>str</code> DEFAULT: <code>'umls'</code> </p> <code>span_setter</code> <p>How to set matches on the doc</p> <p> TYPE: <code>SpanSetterArg</code> DEFAULT: <code>{'ents': True, 'umls': True}</code> </p>"},{"location":"reference/edsnlp/pipes/ner/umls/factory/#edsnlp.pipes.ner.umls.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.umls</code> pipeline was developed by AP-HP's Data Science team and INRIA SODA's team.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/","title":"<code>edsnlp.pipes.ner.umls.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_patterns","title":"<code>get_patterns</code>","text":"<p>Load the UMLS terminology patterns.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_patterns--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>config</code> <p>Languages and sources to select from the whole terminology. For both keys, None will select all values.</p> <p> TYPE: <code>dict[list]</code> </p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_patterns--return","title":"Return","text":"<p>patterns : dict[list]     The mapping between CUI codes and their synonyms.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_patterns--notes","title":"Notes","text":"<p>When run for the first time, this method will download the entire UMLS file and store it at ~/.data/bio/umls/2022AA/. Therefore the second run will be significantly faster than the first one.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_path","title":"<code>get_path</code>","text":"<p>Get the path, module and filename of the UMLS file.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_path--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>config</code> <p>Languages and sources to select from the whole terminology. For both keys, None will select all values.</p> <p> TYPE: <code>dict[list]</code> </p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_path--return","title":"Return","text":"<p>path, module, filename : pathlib.Path, pystow.module, str</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_path--notes","title":"Notes","text":"<p><code>get_path</code> will convert the config dict into a pretty filename.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.get_path--examples","title":"Examples","text":"<p>config = {\"languages\": [\"FRE\", \"ENG\"], \"sources\": None} print(get_path(config)) .data/bio/umls/2022AA/languagesFRE-ENG_sourcesNone.pkl\"</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.download_and_agg_umls","title":"<code>download_and_agg_umls</code>","text":"<p>Download the UMLS if not exist and create a mapping between CUI code and synonyms.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.download_and_agg_umls--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>config</code> <p>Languages and sources to select from the whole terminology. For both keys, None will select all values.</p> <p> TYPE: <code>dict[list]</code> </p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.download_and_agg_umls--return","title":"Return","text":"<p>patterns : dict[list]     The mapping between CUI codes and their synonyms.</p>"},{"location":"reference/edsnlp/pipes/ner/umls/patterns/#edsnlp.pipes.ner.umls.patterns.download_and_agg_umls--notes","title":"Notes","text":"<p>Performs filtering on the returned mapping only, not the downloaded resource.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/","title":"<code>edsnlp.pipes.qualifiers</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/base/","title":"<code>edsnlp.pipes.qualifiers.base</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/base/#edsnlp.pipes.qualifiers.base.BaseTokenQualifierResults","title":"<code>BaseTokenQualifierResults</code>  <code>dataclass</code>","text":"<p>Base dataclass to store qualification informations for each token. Specific qualifier pipes should inherit from this class and add specific fields as needed</p>"},{"location":"reference/edsnlp/pipes/qualifiers/base/#edsnlp.pipes.qualifiers.base.BaseEntQualifierResults","title":"<code>BaseEntQualifierResults</code>  <code>dataclass</code>","text":"<p>Base dataclass to store qualification informations for each entity. Specific qualifier pipes should inherit from this class and add specific fields as needed</p>"},{"location":"reference/edsnlp/pipes/qualifiers/base/#edsnlp.pipes.qualifiers.base.BaseQualifierResults","title":"<code>BaseQualifierResults</code>  <code>dataclass</code>","text":"<p>Base dataclass to store qualification informations for all tokens and entities.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/base/#edsnlp.pipes.qualifiers.base.RuleBasedQualifier","title":"<code>RuleBasedQualifier</code>","text":"<p>           Bases: <code>BaseSpanAttributeClassifierComponent</code></p> <p>Implements the ConText algorithm (eq. NegEx for negations) for detecting contextual attributes text.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/base/#edsnlp.pipes.qualifiers.base.RuleBasedQualifier--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> </p> <code>on_ents_only</code> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> </p> <code>**terms</code> <p>Terms to look for.</p> <p> TYPE: <code>Dict[str, Optional[List[str]]]</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/base/#edsnlp.pipes.qualifiers.base.RuleBasedQualifier.get_cues","title":"<code>get_cues</code>","text":"<p>Extract cues (ex: ne/pas for negations) from the document.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/base/#edsnlp.pipes.qualifiers.base.RuleBasedQualifier.get_cues--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p>The document to process.</p> <p> TYPE: <code>Doc</code> </p> <code>spans</code> <p>Optional list of spans to limit the search around. If None, will search in the whole document.</p> <p> TYPE: <code>Optional[List[Span]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>List of detected cues</p>"},{"location":"reference/edsnlp/pipes/qualifiers/family/","title":"<code>edsnlp.pipes.qualifiers.family</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/family/factory/","title":"<code>edsnlp.pipes.qualifiers.family.factory</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/family/factory/#edsnlp.pipes.qualifiers.family.factory.create_component","title":"<code>create_component = registry.factory.register('eds.family', assigns=['span._.family'], deprecated=['family'])(FamilyContextQualifier)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.family</code> component uses a simple rule-based algorithm to detect spans that describe a family member (or family history) of the patient rather than the patient themself.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/family/factory/#edsnlp.pipes.qualifiers.family.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks the family context of the extracted entities. It is complete, and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(\n    eds.matcher(terms=dict(douleur=\"douleur\", osteoporose=\"ost\u00e9oporose\")),\n)\nnlp.add_pipe(eds.family())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Il a des ant\u00e9c\u00e9dents familiaux d'ost\u00e9oporose\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, ost\u00e9oporose)\n\ndoc.ents[0]._.family\n# Out: False\n\ndoc.ents[1]._.family\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/pipes/qualifiers/family/factory/#edsnlp.pipes.qualifiers.family.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.family</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>family</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token relates to a family member.</li> <li>The <code>family_</code> property is a human-readable string, computed from the <code>family</code>    attribute. It implements a simple getter function that outputs <code>PATIENT</code> or    <code>FAMILY</code>, depending on the value of <code>family</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/family/factory/#edsnlp.pipes.qualifiers.family.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>family</code> <p>List of terms indicating family reference.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of syntagms termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_sections</code> <p>Whether to use annotated sections (namely <code>ant\u00e9c\u00e9dents familiaux</code>).</p> <p> TYPE: <code>bool, by default `False`</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/family/factory/#edsnlp.pipes.qualifiers.family.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.family</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/family/family/","title":"<code>edsnlp.pipes.qualifiers.family.family</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/family/family/#edsnlp.pipes.qualifiers.family.family.FamilyContextQualifier","title":"<code>FamilyContextQualifier</code>","text":"<p>           Bases: <code>RuleBasedQualifier</code></p> <p>The <code>eds.family</code> component uses a simple rule-based algorithm to detect spans that describe a family member (or family history) of the patient rather than the patient themself.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/family/family/#edsnlp.pipes.qualifiers.family.family.FamilyContextQualifier--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks the family context of the extracted entities. It is complete, and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(\n    eds.matcher(terms=dict(douleur=\"douleur\", osteoporose=\"ost\u00e9oporose\")),\n)\nnlp.add_pipe(eds.family())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Il a des ant\u00e9c\u00e9dents familiaux d'ost\u00e9oporose\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, ost\u00e9oporose)\n\ndoc.ents[0]._.family\n# Out: False\n\ndoc.ents[1]._.family\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/pipes/qualifiers/family/family/#edsnlp.pipes.qualifiers.family.family.FamilyContextQualifier--extensions","title":"Extensions","text":"<p>The <code>eds.family</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>family</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token relates to a family member.</li> <li>The <code>family_</code> property is a human-readable string, computed from the <code>family</code>    attribute. It implements a simple getter function that outputs <code>PATIENT</code> or    <code>FAMILY</code>, depending on the value of <code>family</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/family/family/#edsnlp.pipes.qualifiers.family.family.FamilyContextQualifier--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>family</code> <p>List of terms indicating family reference.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of syntagms termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_sections</code> <p>Whether to use annotated sections (namely <code>ant\u00e9c\u00e9dents familiaux</code>).</p> <p> TYPE: <code>bool, by default `False`</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/family/family/#edsnlp.pipes.qualifiers.family.family.FamilyContextQualifier--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.family</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/family/patterns/","title":"<code>edsnlp.pipes.qualifiers.family.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/history/","title":"<code>edsnlp.pipes.qualifiers.history</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/history/factory/","title":"<code>edsnlp.pipes.qualifiers.history.factory</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/history/factory/#edsnlp.pipes.qualifiers.history.factory.create_component","title":"<code>create_component = registry.factory.register('eds.history', assigns=['span._.history'], deprecated=['history', 'antecedents', 'eds.antecedents'])(HistoryQualifier)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.history</code> pipeline uses a simple rule-based algorithm to detect spans that describe medical history rather than the diagnostic of a given visit.</p> <p>The mere definition of a medical history is not straightforward. Hence, this component only tags entities that are explicitly described as part of the medical history, e.g., preceded by a synonym of \"medical history\".</p> <p>This component may also use the output of:</p> <ul> <li>the <code>eds.sections</code> component In that case, the entire <code>ant\u00e9c\u00e9dent</code> section is tagged as a medical history.</li> </ul> <p>Sections</p> <p>Be careful, the <code>eds.sections</code> component may oversize the <code>ant\u00e9c\u00e9dents</code> section. Indeed, it detects section titles and tags the entire text between a title and the next as a section. Hence, should a section title goes undetected after the <code>ant\u00e9c\u00e9dents</code> title, some parts of the document will erroneously be tagged as a medical history.</p> <p>To curb that possibility, using the output of the <code>eds.sections</code> component is deactivated by default.</p> <ul> <li>the <code>eds.dates</code> component. In that case, it will take the   dates into account to tag extracted entities as a medical history or not.</li> </ul> <p>Dates</p> <p>To take the most of the <code>eds.dates</code> component, you may set a value for <code>doc._.note_datetime</code>, either directly:</p> <pre><code>doc = nlp.make_doc(text)\ndoc._.note_datetime = datetime.datetime(2022, 8, 28)\nnlp(doc)\n</code></pre> <p>or using a converter such as the <code>omop</code> converter</p> <p>It allows the component to compute the duration of absolute dates (e.g., le 28 ao\u00fbt 2022/August 28, 2022). The <code>birth_datetime</code> context allows the component to exclude the birthdate from the extracted dates.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/history/factory/#edsnlp.pipes.qualifiers.history.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are history or not. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sections())\nnlp.add_pipe(eds.dates())\nnlp.add_pipe(eds.matcher(terms=dict(douleur=\"douleur\", malaise=\"malaises\")))\nnlp.add_pipe(\n    eds.history(\n        use_sections=True,\n        use_dates=True,\n    ),\n)\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Il a des ant\u00e9c\u00e9dents de malaises.\"\n    \"ANT\u00c9C\u00c9DENTS : \"\n    \"- le patient a d\u00e9j\u00e0 eu des malaises. \"\n    \"- le patient a eu une douleur \u00e0 la jambe il y a 10 jours\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, malaises, malaises, douleur)\n\ndoc.ents[0]._.history\n# Out: False\n\ndoc.ents[1]._.history\n# Out: True\n\ndoc.ents[2]._.history  # (1)\n# Out: True\n\ndoc.ents[3]._.history  # (2)\n# Out: False\n</code></pre> <ol> <li>The entity is in the section <code>ant\u00e9c\u00e9dent</code>.</li> <li>The entity is in the section <code>ant\u00e9c\u00e9dent</code>, however the extracted <code>relative_date</code> refers to an event that took place within 14 days.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/history/factory/#edsnlp.pipes.qualifiers.history.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.history</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>history</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is a medical history.</li> <li>The <code>history_</code> property is a human-readable string, computed from the <code>history</code>    attribute. It implements a simple getter function that outputs <code>CURRENT</code> or    <code>ATCD</code>, depending on the value of <code>history</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/history/factory/#edsnlp.pipes.qualifiers.history.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'history'</code> </p> <code>history</code> <p>List of terms indicating medical history reference.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of syntagms termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>use_sections</code> <p>Whether to use section pipeline to detect medical history section.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_dates</code> <p>Whether to use dates pipeline to detect if the event occurs  a long time before the document date.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>history_limit</code> <p>The number of days after which the event is considered as history.</p> <p> TYPE: <code>Union[int, timedelta]</code> DEFAULT: <code>14</code> </p> <code>exclude_birthdate</code> <p>Whether to exclude the birthdate from history dates.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>closest_dates_only</code> <p>Whether to include the closest dates only.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>tz</code> <p>The timezone to use. Defaults to \"Europe/Paris\".</p> <p> TYPE: <code>Optional[Union[str, tzinfo]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/history/factory/#edsnlp.pipes.qualifiers.history.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.history</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/history/history/","title":"<code>edsnlp.pipes.qualifiers.history.history</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/history/history/#edsnlp.pipes.qualifiers.history.history.HistoryQualifier","title":"<code>HistoryQualifier</code>","text":"<p>           Bases: <code>RuleBasedQualifier</code></p> <p>The <code>eds.history</code> pipeline uses a simple rule-based algorithm to detect spans that describe medical history rather than the diagnostic of a given visit.</p> <p>The mere definition of a medical history is not straightforward. Hence, this component only tags entities that are explicitly described as part of the medical history, e.g., preceded by a synonym of \"medical history\".</p> <p>This component may also use the output of:</p> <ul> <li>the <code>eds.sections</code> component In that case, the entire <code>ant\u00e9c\u00e9dent</code> section is tagged as a medical history.</li> </ul> <p>Sections</p> <p>Be careful, the <code>eds.sections</code> component may oversize the <code>ant\u00e9c\u00e9dents</code> section. Indeed, it detects section titles and tags the entire text between a title and the next as a section. Hence, should a section title goes undetected after the <code>ant\u00e9c\u00e9dents</code> title, some parts of the document will erroneously be tagged as a medical history.</p> <p>To curb that possibility, using the output of the <code>eds.sections</code> component is deactivated by default.</p> <ul> <li>the <code>eds.dates</code> component. In that case, it will take the   dates into account to tag extracted entities as a medical history or not.</li> </ul> <p>Dates</p> <p>To take the most of the <code>eds.dates</code> component, you may set a value for <code>doc._.note_datetime</code>, either directly:</p> <pre><code>doc = nlp.make_doc(text)\ndoc._.note_datetime = datetime.datetime(2022, 8, 28)\nnlp(doc)\n</code></pre> <p>or using a converter such as the <code>omop</code> converter</p> <p>It allows the component to compute the duration of absolute dates (e.g., le 28 ao\u00fbt 2022/August 28, 2022). The <code>birth_datetime</code> context allows the component to exclude the birthdate from the extracted dates.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/history/history/#edsnlp.pipes.qualifiers.history.history.HistoryQualifier--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are history or not. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\nnlp.add_pipe(eds.normalizer())\nnlp.add_pipe(eds.sections())\nnlp.add_pipe(eds.dates())\nnlp.add_pipe(eds.matcher(terms=dict(douleur=\"douleur\", malaise=\"malaises\")))\nnlp.add_pipe(\n    eds.history(\n        use_sections=True,\n        use_dates=True,\n    ),\n)\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Il a des ant\u00e9c\u00e9dents de malaises.\"\n    \"ANT\u00c9C\u00c9DENTS : \"\n    \"- le patient a d\u00e9j\u00e0 eu des malaises. \"\n    \"- le patient a eu une douleur \u00e0 la jambe il y a 10 jours\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, malaises, malaises, douleur)\n\ndoc.ents[0]._.history\n# Out: False\n\ndoc.ents[1]._.history\n# Out: True\n\ndoc.ents[2]._.history  # (1)\n# Out: True\n\ndoc.ents[3]._.history  # (2)\n# Out: False\n</code></pre> <ol> <li>The entity is in the section <code>ant\u00e9c\u00e9dent</code>.</li> <li>The entity is in the section <code>ant\u00e9c\u00e9dent</code>, however the extracted <code>relative_date</code> refers to an event that took place within 14 days.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/history/history/#edsnlp.pipes.qualifiers.history.history.HistoryQualifier--extensions","title":"Extensions","text":"<p>The <code>eds.history</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>history</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is a medical history.</li> <li>The <code>history_</code> property is a human-readable string, computed from the <code>history</code>    attribute. It implements a simple getter function that outputs <code>CURRENT</code> or    <code>ATCD</code>, depending on the value of <code>history</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/history/history/#edsnlp.pipes.qualifiers.history.history.HistoryQualifier--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'history'</code> </p> <code>history</code> <p>List of terms indicating medical history reference.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of syntagms termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>use_sections</code> <p>Whether to use section pipeline to detect medical history section.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_dates</code> <p>Whether to use dates pipeline to detect if the event occurs  a long time before the document date.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>history_limit</code> <p>The number of days after which the event is considered as history.</p> <p> TYPE: <code>Union[int, timedelta]</code> DEFAULT: <code>14</code> </p> <code>exclude_birthdate</code> <p>Whether to exclude the birthdate from history dates.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>closest_dates_only</code> <p>Whether to include the closest dates only.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>tz</code> <p>The timezone to use. Defaults to \"Europe/Paris\".</p> <p> TYPE: <code>Optional[Union[str, tzinfo]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/history/history/#edsnlp.pipes.qualifiers.history.history.HistoryQualifier--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.history</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/history/patterns/","title":"<code>edsnlp.pipes.qualifiers.history.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/","title":"<code>edsnlp.pipes.qualifiers.hypothesis</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/factory/","title":"<code>edsnlp.pipes.qualifiers.hypothesis.factory</code>","text":"<ol><li><p><p>Dalloux C., Claveau V. and Grabar N., 2017. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. https://hal.archives-ouvertes.fr/hal-01659637</p></p></li><li><p><p>Grabar N., Claveau V. and Dalloux C., 2018. CAS: French Corpus with Clinical Cases. https://hal.archives-ouvertes.fr/hal-01937096</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/factory/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component","title":"<code>create_component = registry.factory.register('eds.hypothesis', assigns=['span._.hypothesis'], deprecated=['hypothesis'])(HypothesisQualifier)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.hypothesis</code> pipeline uses a simple rule-based algorithm to detect spans that are speculations rather than certain statements.</p> <p>The component looks for five kinds of expressions in the text :</p> <ul> <li>preceding hypothesis, ie cues that precede a hypothetical expression</li> <li>following hypothesis, ie cues that follow a hypothetical expression</li> <li>pseudo hypothesis : contain a hypothesis cue, but are not hypothesis   (eg \"pas de doute\"/\"no doubt\")</li> <li>hypothetical verbs : verbs indicating hypothesis (eg \"douter\")</li> <li>classic verbs conjugated to the conditional, thus indicating hypothesis</li> </ul>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/factory/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are part of a speculation. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(douleur=\"douleur\", fracture=\"fracture\")))\nnlp.add_pipe(eds.hypothesis())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Possible fracture du radius.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, fracture)\n\ndoc.ents[0]._.hypothesis\n# Out: False\n\ndoc.ents[1]._.hypothesis\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/factory/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.hypothesis</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>hypothesis</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is a speculation.</li> <li>The <code>hypothesis_</code> property is a human-readable string, computed from the    <code>hypothesis</code> attribute. It implements a simple getter function that outputs    <code>HYP</code> or <code>CERT</code>, depending on the value of <code>hypothesis</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/factory/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--performance","title":"Performance","text":"<p>The component's performance is measured on three datasets :</p> <ul> <li>The ESSAI (Dalloux et al., 2017) and CAS (Grabar et al., 2018) datasets were developed   at the CNRS. The two are concatenated.</li> <li>The NegParHyp corpus was specifically developed at APHP's CDW to test the   component on actual clinical notes, using pseudonymised notes from the APHP's CDW.</li> </ul> Dataset Hypothesis F1 CAS/ESSAI 49% NegParHyp 52% <p>NegParHyp corpus</p> <p>The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for hypothesis, speculation and hypothesis context.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/factory/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'hypothesis'</code> </p> <code>attr</code> <p>spaCy's attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>pseudo</code> <p>List of pseudo hypothesis cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of preceding hypothesis cues</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of following hypothesis cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs_hyp</code> <p>List of hypothetical verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs_eds</code> <p>List of mainstream verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/factory/#edsnlp.pipes.qualifiers.hypothesis.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.hypothesis</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/","title":"<code>edsnlp.pipes.qualifiers.hypothesis.hypothesis</code>","text":"<ol><li><p><p>Dalloux C., Claveau V. and Grabar N., 2017. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. https://hal.archives-ouvertes.fr/hal-01659637</p></p></li><li><p><p>Grabar N., Claveau V. and Dalloux C., 2018. CAS: French Corpus with Clinical Cases. https://hal.archives-ouvertes.fr/hal-01937096</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier","title":"<code>HypothesisQualifier</code>","text":"<p>           Bases: <code>RuleBasedQualifier</code></p> <p>The <code>eds.hypothesis</code> pipeline uses a simple rule-based algorithm to detect spans that are speculations rather than certain statements.</p> <p>The component looks for five kinds of expressions in the text :</p> <ul> <li>preceding hypothesis, ie cues that precede a hypothetical expression</li> <li>following hypothesis, ie cues that follow a hypothetical expression</li> <li>pseudo hypothesis : contain a hypothesis cue, but are not hypothesis   (eg \"pas de doute\"/\"no doubt\")</li> <li>hypothetical verbs : verbs indicating hypothesis (eg \"douter\")</li> <li>classic verbs conjugated to the conditional, thus indicating hypothesis</li> </ul>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are part of a speculation. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(douleur=\"douleur\", fracture=\"fracture\")))\nnlp.add_pipe(eds.hypothesis())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Possible fracture du radius.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, fracture)\n\ndoc.ents[0]._.hypothesis\n# Out: False\n\ndoc.ents[1]._.hypothesis\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier--extensions","title":"Extensions","text":"<p>The <code>eds.hypothesis</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>hypothesis</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is a speculation.</li> <li>The <code>hypothesis_</code> property is a human-readable string, computed from the    <code>hypothesis</code> attribute. It implements a simple getter function that outputs    <code>HYP</code> or <code>CERT</code>, depending on the value of <code>hypothesis</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier--performance","title":"Performance","text":"<p>The component's performance is measured on three datasets :</p> <ul> <li>The ESSAI (Dalloux et al., 2017) and CAS (Grabar et al., 2018) datasets were developed   at the CNRS. The two are concatenated.</li> <li>The NegParHyp corpus was specifically developed at APHP's CDW to test the   component on actual clinical notes, using pseudonymised notes from the APHP's CDW.</li> </ul> Dataset Hypothesis F1 CAS/ESSAI 49% NegParHyp 52% <p>NegParHyp corpus</p> <p>The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for hypothesis, speculation and hypothesis context.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'hypothesis'</code> </p> <code>attr</code> <p>spaCy's attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>pseudo</code> <p>List of pseudo hypothesis cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of preceding hypothesis cues</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of following hypothesis cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs_hyp</code> <p>List of hypothetical verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs_eds</code> <p>List of mainstream verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.hypothesis</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier.load_verbs","title":"<code>load_verbs</code>","text":"<p>Conjugate \"classic\" verbs to conditional, and add hypothesis verbs conjugated to all tenses.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/hypothesis/#edsnlp.pipes.qualifiers.hypothesis.hypothesis.HypothesisQualifier.load_verbs--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>verbs_hyp</code> <p> TYPE: <code>List[str]</code> </p> <code>verbs_eds</code> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>list of hypothesis verbs conjugated at all tenses and classic</code> <code>verbs conjugated to conditional.</code>"},{"location":"reference/edsnlp/pipes/qualifiers/hypothesis/patterns/","title":"<code>edsnlp.pipes.qualifiers.hypothesis.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/negation/","title":"<code>edsnlp.pipes.qualifiers.negation</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/negation/factory/","title":"<code>edsnlp.pipes.qualifiers.negation.factory</code>","text":"<ol><li><p><p>Chapman W.W., Bridewell W., Hanbury P., Cooper G.F. and Buchanan B.G., 2001. A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge Summaries. Journal of Biomedical Informatics. 34, pp.301--310. 10.1006/jbin.2001.1029</p></p></li><li><p><p>Dalloux C., Claveau V. and Grabar N., 2017. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. https://hal.archives-ouvertes.fr/hal-01659637</p></p></li><li><p><p>Grabar N., Claveau V. and Dalloux C., 2018. CAS: French Corpus with Clinical Cases. https://hal.archives-ouvertes.fr/hal-01937096</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/factory/#edsnlp.pipes.qualifiers.negation.factory.create_component","title":"<code>create_component = registry.factory.register('eds.negation', assigns=['span._.negation'], deprecated=['negation'])(NegationQualifier)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.negation</code> component uses a simple rule-based algorithm to detect negated spans. It was designed at AP-HP's EDS, following the insights of the NegEx algorithm by Chapman et al., 2001.</p> <p>The component looks for five kinds of expressions in the text :</p> <ul> <li>preceding negations, i.e., cues that precede a negated expression</li> <li>following negations, i.e., cues that follow a negated expression</li> <li>pseudo negations : contain a negation cue, but are not negations   (eg \"pas de doute\"/\"no doubt\")</li> <li>negation verbs, i.e., verbs that indicate a negation</li> <li>terminations, i.e., words that delimit propositions.   The negation spans from the preceding cue to the termination.</li> </ul>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/factory/#edsnlp.pipes.qualifiers.negation.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks the polarity of the extracted entities. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(patient=\"patient\", fracture=\"fracture\")))\nnlp.add_pipe(eds.negation())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Le scanner ne d\u00e9tecte aucune fracture.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (patient, fracture)\n\ndoc.ents[0]._.negation  # (1)\n# Out: False\n\ndoc.ents[1]._.negation\n# Out: True\n</code></pre> <ol> <li>The result of the component is kept in the <code>negation</code> custom extension.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/factory/#edsnlp.pipes.qualifiers.negation.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.negation</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>negation</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is negated.</li> <li>The <code>negation_</code> property is a human-readable string, computed from the <code>negation</code>    attribute. It implements a simple getter function that outputs <code>AFF</code> or <code>NEG</code>,    depending on the value of <code>negation</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/factory/#edsnlp.pipes.qualifiers.negation.factory.create_component--performance","title":"Performance","text":"<p>The component's performance is measured on three datasets :</p> <ul> <li>The ESSAI (Dalloux et al., 2017) and CAS (Grabar et al., 2018) datasets were developed   at the CNRS. The two are concatenated.</li> <li>The NegParHyp corpus was specifically developed at AP-HP to test the component   on actual clinical notes, using pseudonymised notes from the AP-HP.</li> </ul> Dataset Negation F1 CAS/ESSAI 71% NegParHyp 88% <p>NegParHyp corpus</p> <p>The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/factory/#edsnlp.pipes.qualifiers.negation.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>attr</code> <p>spaCy's attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>pseudo</code> <p>List of pseudo negation cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of preceding negation cues</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding_regex</code> <p>List of preceding negation cues, but as regexes.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of following negation cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs</code> <p>List of negation verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/factory/#edsnlp.pipes.qualifiers.negation.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.negation</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/","title":"<code>edsnlp.pipes.qualifiers.negation.negation</code>","text":"<ol><li><p><p>Chapman W.W., Bridewell W., Hanbury P., Cooper G.F. and Buchanan B.G., 2001. A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge Summaries. Journal of Biomedical Informatics. 34, pp.301--310. 10.1006/jbin.2001.1029</p></p></li><li><p><p>Dalloux C., Claveau V. and Grabar N., 2017. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. https://hal.archives-ouvertes.fr/hal-01659637</p></p></li><li><p><p>Grabar N., Claveau V. and Dalloux C., 2018. CAS: French Corpus with Clinical Cases. https://hal.archives-ouvertes.fr/hal-01937096</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/#edsnlp.pipes.qualifiers.negation.negation.NegationQualifier","title":"<code>NegationQualifier</code>","text":"<p>           Bases: <code>RuleBasedQualifier</code></p> <p>The <code>eds.negation</code> component uses a simple rule-based algorithm to detect negated spans. It was designed at AP-HP's EDS, following the insights of the NegEx algorithm by Chapman et al., 2001.</p> <p>The component looks for five kinds of expressions in the text :</p> <ul> <li>preceding negations, i.e., cues that precede a negated expression</li> <li>following negations, i.e., cues that follow a negated expression</li> <li>pseudo negations : contain a negation cue, but are not negations   (eg \"pas de doute\"/\"no doubt\")</li> <li>negation verbs, i.e., verbs that indicate a negation</li> <li>terminations, i.e., words that delimit propositions.   The negation spans from the preceding cue to the termination.</li> </ul>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/#edsnlp.pipes.qualifiers.negation.negation.NegationQualifier--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks the polarity of the extracted entities. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(patient=\"patient\", fracture=\"fracture\")))\nnlp.add_pipe(eds.negation())\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Le scanner ne d\u00e9tecte aucune fracture.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (patient, fracture)\n\ndoc.ents[0]._.negation  # (1)\n# Out: False\n\ndoc.ents[1]._.negation\n# Out: True\n</code></pre> <ol> <li>The result of the component is kept in the <code>negation</code> custom extension.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/#edsnlp.pipes.qualifiers.negation.negation.NegationQualifier--extensions","title":"Extensions","text":"<p>The <code>eds.negation</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>negation</code> attribute is a boolean, set to <code>True</code> if the component predicts    that the span/token is negated.</li> <li>The <code>negation_</code> property is a human-readable string, computed from the <code>negation</code>    attribute. It implements a simple getter function that outputs <code>AFF</code> or <code>NEG</code>,    depending on the value of <code>negation</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/#edsnlp.pipes.qualifiers.negation.negation.NegationQualifier--performance","title":"Performance","text":"<p>The component's performance is measured on three datasets :</p> <ul> <li>The ESSAI (Dalloux et al., 2017) and CAS (Grabar et al., 2018) datasets were developed   at the CNRS. The two are concatenated.</li> <li>The NegParHyp corpus was specifically developed at AP-HP to test the component   on actual clinical notes, using pseudonymised notes from the AP-HP.</li> </ul> Dataset Negation F1 CAS/ESSAI 71% NegParHyp 88% <p>NegParHyp corpus</p> <p>The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/#edsnlp.pipes.qualifiers.negation.negation.NegationQualifier--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>attr</code> <p>spaCy's attribute to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>pseudo</code> <p>List of pseudo negation cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of preceding negation cues</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>preceding_regex</code> <p>List of preceding negation cues, but as regexes.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of following negation cues.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbs</code> <p>List of negation verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>termination</code> <p>List of termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Deprecated, use <code>span_getter</code> instead.</p> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/#edsnlp.pipes.qualifiers.negation.negation.NegationQualifier--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.negation</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/#edsnlp.pipes.qualifiers.negation.negation.NegationQualifier.load_verbs","title":"<code>load_verbs</code>","text":"<p>Conjugate negating verbs to specific tenses.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/negation/#edsnlp.pipes.qualifiers.negation.negation.NegationQualifier.load_verbs--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>verbs</code> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>list_neg_verbs_preceding</code> <p> TYPE: <code>List of conjugated negating verbs preceding entities.</code> </p> <code>list_neg_verbs_following</code> <p> TYPE: <code>List of conjugated negating verbs following entities.</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/negation/patterns/","title":"<code>edsnlp.pipes.qualifiers.negation.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/","title":"<code>edsnlp.pipes.qualifiers.reported_speech</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/factory/","title":"<code>edsnlp.pipes.qualifiers.reported_speech.factory</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/factory/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component","title":"<code>create_component = registry.factory.register('eds.reported_speech', assigns=['span._.reported_speech'], deprecated=['reported_speech', 'rspeech'])(ReportedSpeechQualifier)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.reported_speech</code> component uses a simple rule-based algorithm to detect spans that relate to reported speech (eg when the doctor quotes the patient). It was designed at AP-HP's EDS.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/factory/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are part of a reported speech. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(patient=\"patient\", alcool=\"alcoolis\u00e9\")))\nnlp.add_pipe(eds.reported_speech())\n\ntext = (\n    \"Le patient est admis aux urgences ce soir pour une douleur au bras. \"\n    \"Il nie \u00eatre alcoolis\u00e9.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (patient, alcoolis\u00e9)\n\ndoc.ents[0]._.reported_speech\n# Out: False\n\ndoc.ents[1]._.reported_speech\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/factory/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component--extensions","title":"Extensions","text":"<p>The <code>eds.reported_speech</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>reported_speech</code> attribute is a boolean, set to <code>True</code> if the component    predicts that the span/token is reported.</li> <li>The <code>reported_speech_</code> property is a human-readable string, computed from the    <code>reported_speech</code> attribute. It implements a simple getter function that outputs    <code>DIRECT</code> or <code>REPORTED</code>, depending on the value of <code>reported_speech</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/factory/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>quotation</code> <p>String gathering all quotation cues.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>verbs</code> <p>List of reported speech verbs.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of terms following a reported speech.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of terms preceding a reported speech.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/factory/#edsnlp.pipes.qualifiers.reported_speech.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.reported_speech</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/patterns/","title":"<code>edsnlp.pipes.qualifiers.reported_speech.patterns</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/reported_speech/","title":"<code>edsnlp.pipes.qualifiers.reported_speech.reported_speech</code>","text":""},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/reported_speech/#edsnlp.pipes.qualifiers.reported_speech.reported_speech.ReportedSpeechQualifier","title":"<code>ReportedSpeechQualifier</code>","text":"<p>           Bases: <code>RuleBasedQualifier</code></p> <p>The <code>eds.reported_speech</code> component uses a simple rule-based algorithm to detect spans that relate to reported speech (eg when the doctor quotes the patient). It was designed at AP-HP's EDS.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/reported_speech/#edsnlp.pipes.qualifiers.reported_speech.reported_speech.ReportedSpeechQualifier--examples","title":"Examples","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are part of a reported speech. It is complete and can be run as is.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# Dummy matcher\nnlp.add_pipe(eds.matcher(terms=dict(patient=\"patient\", alcool=\"alcoolis\u00e9\")))\nnlp.add_pipe(eds.reported_speech())\n\ntext = (\n    \"Le patient est admis aux urgences ce soir pour une douleur au bras. \"\n    \"Il nie \u00eatre alcoolis\u00e9.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (patient, alcoolis\u00e9)\n\ndoc.ents[0]._.reported_speech\n# Out: False\n\ndoc.ents[1]._.reported_speech\n# Out: True\n</code></pre>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/reported_speech/#edsnlp.pipes.qualifiers.reported_speech.reported_speech.ReportedSpeechQualifier--extensions","title":"Extensions","text":"<p>The <code>eds.reported_speech</code> component declares two extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>reported_speech</code> attribute is a boolean, set to <code>True</code> if the component    predicts that the span/token is reported.</li> <li>The <code>reported_speech_</code> property is a human-readable string, computed from the    <code>reported_speech</code> attribute. It implements a simple getter function that outputs    <code>DIRECT</code> or <code>REPORTED</code>, depending on the value of <code>reported_speech</code>.</li> </ol>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/reported_speech/#edsnlp.pipes.qualifiers.reported_speech.reported_speech.ReportedSpeechQualifier--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The component name.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>quotation</code> <p>String gathering all quotation cues.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>verbs</code> <p>List of reported speech verbs.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>following</code> <p>List of terms following a reported speech.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>preceding</code> <p>List of terms preceding a reported speech.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> DEFAULT: <code>NORM</code> </p> <code>span_getter</code> <p>Which entities should be classified. By default, <code>doc.ents</code></p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>on_ents_only</code> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <ul> <li>If True, will look in all ents located in <code>doc.ents</code> only</li> <li>If an iterable of string is passed, will additionally look in <code>doc.spans[key]</code> for each key in the iterable</li> </ul> <p> TYPE: <code>Union[bool, str, List[str], Set[str]]</code> DEFAULT: <code>None</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/reported_speech/#edsnlp.pipes.qualifiers.reported_speech.reported_speech.ReportedSpeechQualifier--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.reported_speech</code> component was developed by AP-HP's Data Science team.</p>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/reported_speech/#edsnlp.pipes.qualifiers.reported_speech.reported_speech.ReportedSpeechQualifier.load_verbs","title":"<code>load_verbs</code>","text":"<p>Conjugate reporting verbs to specific tenses (trhid person)</p>"},{"location":"reference/edsnlp/pipes/qualifiers/reported_speech/reported_speech/#edsnlp.pipes.qualifiers.reported_speech.reported_speech.ReportedSpeechQualifier.load_verbs--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>verbs</code> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>list_rep_verbs</code> <p> TYPE: <code>List of reporting verbs conjugated to specific tenses.</code> </p>"},{"location":"reference/edsnlp/pipes/terminations/","title":"<code>edsnlp.pipes.terminations</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/","title":"<code>edsnlp.pipes.trainable</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/biaffine_dep_parser/","title":"<code>edsnlp.pipes.trainable.biaffine_dep_parser</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/biaffine_dep_parser/biaffine_dep_parser/","title":"<code>edsnlp.pipes.trainable.biaffine_dep_parser.biaffine_dep_parser</code>","text":"<ol><li><p><p>Dozat T. and Manning C.D., 2017. Deep Biaffine Attention for Neural Dependency Parsing. https://arxiv.org/abs/1611.01734</p></p></li><li><p><p>Grobol L. and Crabb\u00e9 B., 2021. Analyse en d\u00e9pendances du fran\u00e7ais avec des plongements contextualis\u00e9s. https://hal.archives-ouvertes.fr/hal-03223424</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/trainable/biaffine_dep_parser/biaffine_dep_parser/#edsnlp.pipes.trainable.biaffine_dep_parser.biaffine_dep_parser.TrainableBiaffineDependencyParser","title":"<code>TrainableBiaffineDependencyParser</code>","text":"<p>           Bases: <code>TorchComponent[BatchOutput, BatchInput]</code></p> <p>The <code>eds.biaffine_dep_parser</code> component is a trainable dependency parser based on a biaffine model (Dozat and Manning, 2017). For each token, the model predicts a score for each possible head in the document, and a score for each possible label for each head. The results are then decoded either greedily by picking the best scoring head for each token independently, or holistically by computing the Maximum Spanning Tree (MST) over the graph of token \u2192 head scores.</p> <p>Experimental</p> <p>This component is experimental. In particular, it expects the input to be sentences and not full documents, as it has not been optimized for memory efficiency yet and computed the full matrix of scores for all pairs of tokens in a document.</p> <p>At the moment, it is mostly used for benchmarking and research purposes.</p>"},{"location":"reference/edsnlp/pipes/trainable/biaffine_dep_parser/biaffine_dep_parser/#edsnlp.pipes.trainable.biaffine_dep_parser.biaffine_dep_parser.TrainableBiaffineDependencyParser--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.biaffine_dep_parser(\n        embedding=eds.transformer(model=\"prajjwal1/bert-tiny\"),\n        hidden_size=128,\n        dropout_p=0.1,\n        # labels unset, will be inferred from the data in `post_init`\n        decoding_mode=\"mst\",\n    ),\n    name=\"dep_parser\"\n)\n</code></pre> <p>Dependency parsers are typically trained on CoNLL-formatted Universal Dependencies corpora, which you can load using the <code>edsnlp.data.read_conll</code> function.</p> <p>To train the model, you can adapt the the Training NER tutorial.</p>"},{"location":"reference/edsnlp/pipes/trainable/biaffine_dep_parser/biaffine_dep_parser/#edsnlp.pipes.trainable.biaffine_dep_parser.biaffine_dep_parser.TrainableBiaffineDependencyParser--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'biaffine_dep_parser'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). For example <code>{\"section\": \"conclusion\"}</code> to predict dependencies in the conclusion section of documents.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>use_attrs</code> <p>The attributes to use as features for the model (ex. <code>[\"pos_\"]</code> to use the POS tag). By default, no attributes are used.</p> <p>Note that if you train a model with attributes, you will need to provide the same attributes during inference, and the model might not work well if the attributes were not annotated accurately on the test data.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>attr_size</code> <p>The size of the attribute embeddings.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>hidden_size</code> <p>The size of the hidden layer in the MLP.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>dropout_p</code> <p>The dropout probability to use in the MLP.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>labels</code> <p>The labels to predict. The labels can also be inferred from the data during <code>nlp.post_init(...)</code>.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>['root']</code> </p> <code>decoding_mode</code> <p>Whether to decode the dependencies greedily or using the Maximum Spanning Tree algorithm.</p> <p> TYPE: <code>Literal['greedy', 'mst']</code> DEFAULT: <code>mst</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/biaffine_dep_parser/biaffine_dep_parser/#edsnlp.pipes.trainable.biaffine_dep_parser.biaffine_dep_parser.TrainableBiaffineDependencyParser--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.biaffine_dep_parser</code> trainable pipe was developed by AP-HP's Data Science team, and heavily inspired by the implementation of Grobol and Crabb\u00e9, 2021. The biaffine architecture is based on the biaffine parser of Dozat and Manning, 2017.</p>"},{"location":"reference/edsnlp/pipes/trainable/biaffine_dep_parser/biaffine_dep_parser/#edsnlp.pipes.trainable.biaffine_dep_parser.biaffine_dep_parser.chuliu_edmonds_one_root","title":"<code>chuliu_edmonds_one_root</code>","text":"<p>Shamelessly copied from https://github.com/hopsparser/hopsparser/blob/main/hopsparser/mst.py#L63 All credits, Loic Grobol at Universit\u00e9 Paris Nanterre, France, the original author of this implementation. Find the license of the hopsparser software below:</p> <p>Copyright 2020 Beno\u00eet Crabb\u00e9 benoit.crabbe@linguist.univ-paris-diderot.fr</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>Repeatedly Use the Chu\u2011Liu/Edmonds algorithm to find a maximum spanning dependency tree from the weight matrix of a rooted weighted directed graph.</p> <p>ATTENTION: this modifies <code>scores</code> in place.</p>"},{"location":"reference/edsnlp/pipes/trainable/biaffine_dep_parser/biaffine_dep_parser/#edsnlp.pipes.trainable.biaffine_dep_parser.biaffine_dep_parser.chuliu_edmonds_one_root--input","title":"Input","text":"<ul> <li><code>scores</code>: A 2d numeric array such that <code>scores[i][j]</code> is the weight of the <code>$j\u2192i$</code> edge in the graph and the 0-th node is the root.</li> </ul>"},{"location":"reference/edsnlp/pipes/trainable/biaffine_dep_parser/biaffine_dep_parser/#edsnlp.pipes.trainable.biaffine_dep_parser.biaffine_dep_parser.chuliu_edmonds_one_root--output","title":"Output","text":"<ul> <li><code>tree</code>: A 1d integer array such that <code>tree[i]</code> is the head of the <code>i</code>-th node</li> </ul>"},{"location":"reference/edsnlp/pipes/trainable/biaffine_dep_parser/factory/","title":"<code>edsnlp.pipes.trainable.biaffine_dep_parser.factory</code>","text":"<ol><li><p><p>Dozat T. and Manning C.D., 2017. Deep Biaffine Attention for Neural Dependency Parsing. https://arxiv.org/abs/1611.01734</p></p></li><li><p><p>Grobol L. and Crabb\u00e9 B., 2021. Analyse en d\u00e9pendances du fran\u00e7ais avec des plongements contextualis\u00e9s. https://hal.archives-ouvertes.fr/hal-03223424</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/trainable/biaffine_dep_parser/factory/#edsnlp.pipes.trainable.biaffine_dep_parser.factory.create_component","title":"<code>create_component = registry.factory.register('eds.biaffine_dep_parser', assigns=['token.head', 'token.dep'])(TrainableBiaffineDependencyParser)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.biaffine_dep_parser</code> component is a trainable dependency parser based on a biaffine model (Dozat and Manning, 2017). For each token, the model predicts a score for each possible head in the document, and a score for each possible label for each head. The results are then decoded either greedily by picking the best scoring head for each token independently, or holistically by computing the Maximum Spanning Tree (MST) over the graph of token \u2192 head scores.</p> <p>Experimental</p> <p>This component is experimental. In particular, it expects the input to be sentences and not full documents, as it has not been optimized for memory efficiency yet and computed the full matrix of scores for all pairs of tokens in a document.</p> <p>At the moment, it is mostly used for benchmarking and research purposes.</p>"},{"location":"reference/edsnlp/pipes/trainable/biaffine_dep_parser/factory/#edsnlp.pipes.trainable.biaffine_dep_parser.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.biaffine_dep_parser(\n        embedding=eds.transformer(model=\"prajjwal1/bert-tiny\"),\n        hidden_size=128,\n        dropout_p=0.1,\n        # labels unset, will be inferred from the data in `post_init`\n        decoding_mode=\"mst\",\n    ),\n    name=\"dep_parser\"\n)\n</code></pre> <p>Dependency parsers are typically trained on CoNLL-formatted Universal Dependencies corpora, which you can load using the <code>edsnlp.data.read_conll</code> function.</p> <p>To train the model, you can adapt the the Training NER tutorial.</p>"},{"location":"reference/edsnlp/pipes/trainable/biaffine_dep_parser/factory/#edsnlp.pipes.trainable.biaffine_dep_parser.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'biaffine_dep_parser'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). For example <code>{\"section\": \"conclusion\"}</code> to predict dependencies in the conclusion section of documents.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>use_attrs</code> <p>The attributes to use as features for the model (ex. <code>[\"pos_\"]</code> to use the POS tag). By default, no attributes are used.</p> <p>Note that if you train a model with attributes, you will need to provide the same attributes during inference, and the model might not work well if the attributes were not annotated accurately on the test data.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>attr_size</code> <p>The size of the attribute embeddings.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>hidden_size</code> <p>The size of the hidden layer in the MLP.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>dropout_p</code> <p>The dropout probability to use in the MLP.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>labels</code> <p>The labels to predict. The labels can also be inferred from the data during <code>nlp.post_init(...)</code>.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>['root']</code> </p> <code>decoding_mode</code> <p>Whether to decode the dependencies greedily or using the Maximum Spanning Tree algorithm.</p> <p> TYPE: <code>Literal['greedy', 'mst']</code> DEFAULT: <code>mst</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/biaffine_dep_parser/factory/#edsnlp.pipes.trainable.biaffine_dep_parser.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.biaffine_dep_parser</code> trainable pipe was developed by AP-HP's Data Science team, and heavily inspired by the implementation of Grobol and Crabb\u00e9, 2021. The biaffine architecture is based on the biaffine parser of Dozat and Manning, 2017.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/","title":"<code>edsnlp.pipes.trainable.embeddings</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/","title":"<code>edsnlp.pipes.trainable.embeddings.span_pooler</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/factory/","title":"<code>edsnlp.pipes.trainable.embeddings.span_pooler.factory</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/factory/#edsnlp.pipes.trainable.embeddings.span_pooler.factory.create_component","title":"<code>create_component = registry.factory.register('eds.span_pooler', assigns=[], deprecated=[])(SpanPooler)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.span_pooler</code> component is a trainable span embedding component. It generates span embeddings from a word embedding component and a span getter. It can be used to train a span classifier, as in <code>eds.span_classifier</code>.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/factory/#edsnlp.pipes.trainable.embeddings.span_pooler.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[Pipeline]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'span_pooler'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>pooling_mode</code> <p>How word embeddings are aggregated into a single embedding per span.</p> <p> TYPE: <code>Literal['max', 'sum', 'mean']</code> DEFAULT: <code>mean</code> </p> <code>hidden_size</code> <p>The size of the hidden layer. If None, no projection is done and the output of the span pooler is used directly.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/span_pooler/","title":"<code>edsnlp.pipes.trainable.embeddings.span_pooler.span_pooler</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/span_pooler/#edsnlp.pipes.trainable.embeddings.span_pooler.span_pooler.SpanPoolerBatchInput","title":"<code>SpanPoolerBatchInput = TypedDict('SpanPoolerBatchInput', {'embedding': BatchInput, 'begins': ft.FoldedTensor, 'ends': ft.FoldedTensor, 'sequence_idx': torch.Tensor, 'stats': TypedDict('SpanPoolerBatchStats', {'spans': int})})</code>  <code>module-attribute</code>","text":"<p>embeds: torch.FloatTensor     Token embeddings to predict the tags from begins: torch.LongTensor     Begin offsets of the spans ends: torch.LongTensor     End offsets of the spans sequence_idx: torch.LongTensor     Sequence (cf Embedding spans) index of the spans</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/span_pooler/#edsnlp.pipes.trainable.embeddings.span_pooler.span_pooler.SpanPooler","title":"<code>SpanPooler</code>","text":"<p>           Bases: <code>SpanEmbeddingComponent</code>, <code>BaseComponent</code></p> <p>The <code>eds.span_pooler</code> component is a trainable span embedding component. It generates span embeddings from a word embedding component and a span getter. It can be used to train a span classifier, as in <code>eds.span_classifier</code>.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/span_pooler/#edsnlp.pipes.trainable.embeddings.span_pooler.span_pooler.SpanPooler--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>Optional[Pipeline]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'span_pooler'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>pooling_mode</code> <p>How word embeddings are aggregated into a single embedding per span.</p> <p> TYPE: <code>Literal['max', 'sum', 'mean']</code> DEFAULT: <code>mean</code> </p> <code>hidden_size</code> <p>The size of the hidden layer. If None, no projection is done and the output of the span pooler is used directly.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/span_pooler/#edsnlp.pipes.trainable.embeddings.span_pooler.span_pooler.SpanPooler.forward","title":"<code>forward</code>","text":"<p>Apply the span classifier module to the document embeddings and given spans to: - compute the loss - and/or predict the labels of spans If labels are predicted, they are assigned to the <code>additional_outputs</code> dictionary.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/span_pooler/span_pooler/#edsnlp.pipes.trainable.embeddings.span_pooler.span_pooler.SpanPooler.forward--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <p>The input batch</p> <p> TYPE: <code>SpanPoolerBatchInput</code> </p> RETURNS DESCRIPTION <code>BatchOutput</code>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/","title":"<code>edsnlp.pipes.trainable.embeddings.text_cnn</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/factory/","title":"<code>edsnlp.pipes.trainable.embeddings.text_cnn.factory</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/factory/#edsnlp.pipes.trainable.embeddings.text_cnn.factory.create_component","title":"<code>create_component = registry.factory.register('eds.text_cnn', assigns=[], deprecated=[])(TextCnnEncoder)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.text_cnn</code> component is a simple 1D convolutional network to contextualize word embeddings (as computed by the <code>embedding</code> component passed as argument).</p> <p>To be memory efficient when handling batches of variable-length sequences, this module employs sequence packing, while taking care of avoiding contamination between the different docs.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/factory/#edsnlp.pipes.trainable.embeddings.text_cnn.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> </p> <code>embedding</code> <p>Embedding module to apply to the input</p> <p> TYPE: <code>TorchComponent[WordEmbeddingBatchOutput, BatchInput]</code> </p> <code>output_size</code> <p>Size of the output embeddings Defaults to the <code>input_size</code></p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>out_channels</code> <p>Number of channels</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>kernel_sizes</code> <p>Window size of each kernel</p> <p> TYPE: <code>Sequence[int]</code> DEFAULT: <code>(3, 4, 5)</code> </p> <code>activation</code> <p>Activation function to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>relu</code> </p> <code>residual</code> <p>Whether to use residual connections</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>normalize</code> <p>Whether to normalize before or after the residual connection</p> <p> TYPE: <code>Literal['pre', 'post', 'none']</code> DEFAULT: <code>pre</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/text_cnn/","title":"<code>edsnlp.pipes.trainable.embeddings.text_cnn.text_cnn</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/text_cnn/#edsnlp.pipes.trainable.embeddings.text_cnn.text_cnn.TextCnnEncoder","title":"<code>TextCnnEncoder</code>","text":"<p>           Bases: <code>WordContextualizerComponent</code></p> <p>The <code>eds.text_cnn</code> component is a simple 1D convolutional network to contextualize word embeddings (as computed by the <code>embedding</code> component passed as argument).</p> <p>To be memory efficient when handling batches of variable-length sequences, this module employs sequence packing, while taking care of avoiding contamination between the different docs.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/text_cnn/#edsnlp.pipes.trainable.embeddings.text_cnn.text_cnn.TextCnnEncoder--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> </p> <code>embedding</code> <p>Embedding module to apply to the input</p> <p> TYPE: <code>TorchComponent[WordEmbeddingBatchOutput, BatchInput]</code> </p> <code>output_size</code> <p>Size of the output embeddings Defaults to the <code>input_size</code></p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>out_channels</code> <p>Number of channels</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>kernel_sizes</code> <p>Window size of each kernel</p> <p> TYPE: <code>Sequence[int]</code> DEFAULT: <code>(3, 4, 5)</code> </p> <code>activation</code> <p>Activation function to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>relu</code> </p> <code>residual</code> <p>Whether to use residual connections</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>normalize</code> <p>Whether to normalize before or after the residual connection</p> <p> TYPE: <code>Literal['pre', 'post', 'none']</code> DEFAULT: <code>pre</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/text_cnn/#edsnlp.pipes.trainable.embeddings.text_cnn.text_cnn.TextCnnEncoder.forward","title":"<code>forward</code>","text":"<p>Encode embeddings with a 1d convolutional network</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/text_cnn/text_cnn/#edsnlp.pipes.trainable.embeddings.text_cnn.text_cnn.TextCnnEncoder.forward--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <ul> <li>embeddings: embeddings of shape (batch_size, seq_len, input_size)</li> <li>mask: mask of shape (batch_size, seq_len)</li> </ul> <p> TYPE: <code>BatchInput</code> </p> RETURNS DESCRIPTION <code>WordEmbeddingBatchOutput</code> <ul> <li>embeddings: encoded embeddings of shape (batch_size, seq_len, input_size)</li> <li>mask: (same) mask of shape (batch_size, seq_len)</li> </ul>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/","title":"<code>edsnlp.pipes.trainable.embeddings.transformer</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/factory/","title":"<code>edsnlp.pipes.trainable.embeddings.transformer.factory</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/factory/#edsnlp.pipes.trainable.embeddings.transformer.factory.create_component","title":"<code>create_component = registry.factory.register('eds.transformer', assigns=[], deprecated=[])(Transformer)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.transformer</code> component is a wrapper around HuggingFace's transformers library. If you are not familiar with transformers, a good way to start is the Illustrated Transformer tutorial.</p> <p>Compared to using the raw Huggingface model, we offer a simple mechanism to split long documents into strided windows before feeding them to the model.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/factory/#edsnlp.pipes.trainable.embeddings.transformer.factory.create_component--windowing","title":"Windowing","text":"<p>EDS-NLP's Transformer component splits long documents into smaller windows before feeding them to the model. This is done to avoid hitting the maximum number of tokens that can be processed by the model on a single device. The window size and stride can be configured using the <code>window</code> and <code>stride</code> parameters. The default values are 512 and 256 respectively, which means that the model will process windows of 512 tokens, each separated by 256 tokens. Whenever a token appears in multiple windows, the embedding of the \"most contextualized\" occurrence is used, i.e. the occurrence that is the closest to the center of its window.</p> <p>Here is an overview how this works to produce embeddings (shown in red) for each word of the document :</p> <p></p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/factory/#edsnlp.pipes.trainable.embeddings.transformer.factory.create_component--examples","title":"Examples","text":"<p>Here is an example of how to define a pipeline with a Transformer component:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.transformer(\n        model=\"prajjwal1/bert-tiny\",\n        window=128,\n        stride=96,\n    ),\n)\n</code></pre> <p>You can then compose this embedding with a task specific component such as <code>eds.ner_crf</code>.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/factory/#edsnlp.pipes.trainable.embeddings.transformer.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline instance</p> <p> </p> <code>name</code> <p>The component name</p> <p> </p> <code>model</code> <p>The Huggingface model name or path</p> <p> </p> <code>window</code> <p>The window size to use when splitting long documents into smaller windows before feeding them to the Transformer model (default: 512 = 512 - 2)</p> <p> DEFAULT: <code>128</code> </p> <code>stride</code> <p>The stride (distance between windows) to use when splitting long documents into smaller windows: (default: 96)</p> <p> DEFAULT: <code>96</code> </p> <code>training_stride</code> <p>If False, the stride will be set to the window size during training, meaning that there will be no overlap between windows. If True, the stride will be set to the <code>stride</code> parameter during training, just like during inference.</p> <p> DEFAULT: <code>True</code> </p> <code>max_tokens_per_device</code> <p>The maximum number of tokens that can be processed by the model on a single device. This does not affect the results but can be used to reduce the memory usage of the model, at the cost of a longer processing time.</p> <p>If \"auto\", the component will try to estimate the maximum number of tokens that can be processed by the model on the current device at a given time.</p> <p> DEFAULT: <code>auto</code> </p> <code>span_getter</code> <p>Which spans of the document should be embedded. Defaults to the full document if None.</p> <p> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/","title":"<code>edsnlp.pipes.trainable.embeddings.transformer.transformer</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/#edsnlp.pipes.trainable.embeddings.transformer.transformer.TransformerBatchInput","title":"<code>TransformerBatchInput = TypedDict('TransformerBatchInput', {'input_ids': ft.FoldedTensor, 'word_indices': torch.Tensor, 'word_offsets': ft.FoldedTensor, 'empty_word_indices': torch.Tensor})</code>  <code>module-attribute</code>","text":"<p>input_ids: FoldedTensor     Tokenized input (prompt + text) to embed word_indices: torch.LongTensor     Flattened indices of the word's wordpieces in the flattened input_ids word_offsets: FoldedTensor     Offsets of the word's wordpieces in the flattened input_ids empty_word_indices: torch.LongTensor     Indices of empty words in the flattened input_ids</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/#edsnlp.pipes.trainable.embeddings.transformer.transformer.TransformerBatchOutput","title":"<code>TransformerBatchOutput = TypedDict('TransformerBatchOutput', {'embeddings': ft.FoldedTensor})</code>  <code>module-attribute</code>","text":"<p>embeddings: FoldedTensor     The embeddings of the words</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/#edsnlp.pipes.trainable.embeddings.transformer.transformer.Transformer","title":"<code>Transformer</code>","text":"<p>           Bases: <code>WordEmbeddingComponent[TransformerBatchInput]</code></p> <p>The <code>eds.transformer</code> component is a wrapper around HuggingFace's transformers library. If you are not familiar with transformers, a good way to start is the Illustrated Transformer tutorial.</p> <p>Compared to using the raw Huggingface model, we offer a simple mechanism to split long documents into strided windows before feeding them to the model.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/#edsnlp.pipes.trainable.embeddings.transformer.transformer.Transformer--windowing","title":"Windowing","text":"<p>EDS-NLP's Transformer component splits long documents into smaller windows before feeding them to the model. This is done to avoid hitting the maximum number of tokens that can be processed by the model on a single device. The window size and stride can be configured using the <code>window</code> and <code>stride</code> parameters. The default values are 512 and 256 respectively, which means that the model will process windows of 512 tokens, each separated by 256 tokens. Whenever a token appears in multiple windows, the embedding of the \"most contextualized\" occurrence is used, i.e. the occurrence that is the closest to the center of its window.</p> <p>Here is an overview how this works to produce embeddings (shown in red) for each word of the document :</p> <p></p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/#edsnlp.pipes.trainable.embeddings.transformer.transformer.Transformer--examples","title":"Examples","text":"<p>Here is an example of how to define a pipeline with a Transformer component:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.transformer(\n        model=\"prajjwal1/bert-tiny\",\n        window=128,\n        stride=96,\n    ),\n)\n</code></pre> <p>You can then compose this embedding with a task specific component such as <code>eds.ner_crf</code>.</p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/#edsnlp.pipes.trainable.embeddings.transformer.transformer.Transformer--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline instance</p> <p> </p> <code>name</code> <p>The component name</p> <p> </p> <code>model</code> <p>The Huggingface model name or path</p> <p> </p> <code>window</code> <p>The window size to use when splitting long documents into smaller windows before feeding them to the Transformer model (default: 512 = 512 - 2)</p> <p> DEFAULT: <code>128</code> </p> <code>stride</code> <p>The stride (distance between windows) to use when splitting long documents into smaller windows: (default: 96)</p> <p> DEFAULT: <code>96</code> </p> <code>training_stride</code> <p>If False, the stride will be set to the window size during training, meaning that there will be no overlap between windows. If True, the stride will be set to the <code>stride</code> parameter during training, just like during inference.</p> <p> DEFAULT: <code>True</code> </p> <code>max_tokens_per_device</code> <p>The maximum number of tokens that can be processed by the model on a single device. This does not affect the results but can be used to reduce the memory usage of the model, at the cost of a longer processing time.</p> <p>If \"auto\", the component will try to estimate the maximum number of tokens that can be processed by the model on the current device at a given time.</p> <p> DEFAULT: <code>auto</code> </p> <code>span_getter</code> <p>Which spans of the document should be embedded. Defaults to the full document if None.</p> <p> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/#edsnlp.pipes.trainable.embeddings.transformer.transformer.Transformer.collate","title":"<code>collate</code>","text":"<p>How this works: 1. Iterate over samples, and in each sample over spans of text to embed    independently, and extract their input ids (and optionally prompts)    in a list <code>input_ids</code> that will be passed to the transformer.</p> <p><code>embeds = self.embedding(input_ids)</code> 2. Since we want to aggregate over words, and have overlapping spans, we need    to process indices carefully. Once the individual spans are embedded, we    will flatten them...</p> <p><code>flat_embeds = embeds.view(-1, embeds.size(2))[indexer]</code></p>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/transformer/transformer/#edsnlp.pipes.trainable.embeddings.transformer.transformer.Transformer.collate--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code>"},{"location":"reference/edsnlp/pipes/trainable/embeddings/typing/","title":"<code>edsnlp.pipes.trainable.embeddings.typing</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/extractive_qa/","title":"<code>edsnlp.pipes.trainable.extractive_qa</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/extractive_qa/extractive_qa/","title":"<code>edsnlp.pipes.trainable.extractive_qa.extractive_qa</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/extractive_qa/extractive_qa/#edsnlp.pipes.trainable.extractive_qa.extractive_qa.TrainableExtractiveQA","title":"<code>TrainableExtractiveQA</code>","text":"<p>           Bases: <code>TrainableNerCrf</code></p> <p>The <code>eds.extractive_qa</code> component is a trainable extractive question answering component. This can be seen as a Named Entity Recognition (NER) component where the types of entities predicted by the model are not pre-defined during the training but are provided as prompts (i.e., questions) at inference time.</p> <p>The <code>eds.extractive_qa</code> shares a lot of similarities with the <code>eds.ner_crf</code> component, and therefore most of the arguments are the same.</p> <p>Extractive vs Abstractive Question Answering</p> <p>Extractive Question Answering differs from Abstractive Question Answering in that the answer is extracted from the text, rather than generated (\u00e0 la ChatGPT) from scratch. To normalize the answers, you can use the <code>eds.span_linker</code> component in <code>synonym</code> mode and search for the closest <code>synonym</code> in a predefined list.</p>"},{"location":"reference/edsnlp/pipes/trainable/extractive_qa/extractive_qa/#edsnlp.pipes.trainable.extractive_qa.extractive_qa.TrainableExtractiveQA--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.extractive_qa(\n        embedding=eds.transformer(\n            model=\"prajjwal1/bert-tiny\",\n            window=128,\n            stride=96,\n        ),\n        mode=\"joint\",\n        target_span_getter=\"ner-gold\",\n        span_setter=\"ents\",\n        questions={\n            \"disease\": \"What disease does the patient have?\",\n            \"drug\": \"What drug is the patient taking?\",\n        },  # (1)!\n    ),\n    name=\"qa\",\n)\n</code></pre> <p>To train the model, refer to the Training tutorial.</p> <p>Once the model is trained, you can use the questions attribute (next section) on the document you run the model on, or you can change the global questions attribute:</p> <pre><code>nlp.pipes.qa.questions = {\n    \"disease\": \"When did the patient get sick?\",\n}\n</code></pre>"},{"location":"reference/edsnlp/pipes/trainable/extractive_qa/extractive_qa/#edsnlp.pipes.trainable.extractive_qa.extractive_qa.TrainableExtractiveQA--dynamic-questions","title":"Dynamic Questions","text":"<p>You can also provide</p> <pre><code>eds.extractive_qa(..., questions_attribute=\"questions\")\n</code></pre> <p>to get the questions dynamically from an attribute on the Doc or Span objects (e.g., <code>doc._.questions</code>). This is useful when you want to have different questions depending on the document.</p> <p>To provide questions from a dataframe, you can use the following code:</p> <pre><code>dataframe = pd.DataFrame({\"questions\": ..., \"note_text\": ..., \"note_id\": ...})\nstream = edsnlp.data.from_pandas(\n    dataframe,\n    converter=\"omop\",\n    doc_attributes={\"questions\": \"questions\"},\n)\nstream.map_pipeline(nlp)\nstream.set_processing(backend=\"multiprocessing\")\nout = stream.to_pandas(converters=\"ents\")\n</code></pre>"},{"location":"reference/edsnlp/pipes/trainable/extractive_qa/extractive_qa/#edsnlp.pipes.trainable.extractive_qa.extractive_qa.TrainableExtractiveQA--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'extractive_qa'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>questions</code> <p>The questions to ask, as a mapping between the entity type and the list of questions to ask for this entity type (or single string if only one question).</p> <p> TYPE: <code>Dict[str, AsList[str]]</code> DEFAULT: <code>{}</code> </p> <code>questions_attribute</code> <p>The attribute to use to get the questions dynamically from the Doc or Span objects (as returned by the <code>context_getter</code> argument). If None, the questions will be fixed and only taken from the <code>questions</code> argument.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>questions</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). For example <code>{\"section\": \"conclusion\"}</code> to only extract the entities from the conclusion.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>target_span_getter</code> <p>Method to call to get the gold spans from a document, for scoring or training. By default, takes all entities in <code>doc.ents</code>, but we recommend you specify a given span group name instead.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use to set the predicted spans on the Doc object. If None, the component will infer the span setter from the target_span_getter config.</p> <p> TYPE: <code>Optional[SpanSetterArg]</code> DEFAULT: <code>None</code> </p> <code>infer_span_setter</code> <p>Whether to complete the span setter from the target_span_getter config. False by default, unless the span_setter is None.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>The CRF mode to use : independent, joint or marginal</p> <p> TYPE: <code>Literal['independent', 'joint', 'marginal']</code> DEFAULT: <code>joint</code> </p> <code>window</code> <p>The window size to use for the CRF. If 0, will use the whole document, at the cost of a longer computation time. If 1, this is equivalent to assuming that the tags are independent and will the component be faster, but with degraded performance. Empirically, we found that a window size of 10 or 20 works well.</p> <p> TYPE: <code>int</code> DEFAULT: <code>40</code> </p> <code>stride</code> <p>The stride to use for the CRF windows. Defaults to <code>window // 2</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/extractive_qa/factory/","title":"<code>edsnlp.pipes.trainable.extractive_qa.factory</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/extractive_qa/factory/#edsnlp.pipes.trainable.extractive_qa.factory.create_component","title":"<code>create_component = registry.factory.register('eds.extractive_qa', assigns=[], deprecated=[])(TrainableExtractiveQA)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.extractive_qa</code> component is a trainable extractive question answering component. This can be seen as a Named Entity Recognition (NER) component where the types of entities predicted by the model are not pre-defined during the training but are provided as prompts (i.e., questions) at inference time.</p> <p>The <code>eds.extractive_qa</code> shares a lot of similarities with the <code>eds.ner_crf</code> component, and therefore most of the arguments are the same.</p> <p>Extractive vs Abstractive Question Answering</p> <p>Extractive Question Answering differs from Abstractive Question Answering in that the answer is extracted from the text, rather than generated (\u00e0 la ChatGPT) from scratch. To normalize the answers, you can use the <code>eds.span_linker</code> component in <code>synonym</code> mode and search for the closest <code>synonym</code> in a predefined list.</p>"},{"location":"reference/edsnlp/pipes/trainable/extractive_qa/factory/#edsnlp.pipes.trainable.extractive_qa.factory.create_component--examples","title":"Examples","text":"<pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.extractive_qa(\n        embedding=eds.transformer(\n            model=\"prajjwal1/bert-tiny\",\n            window=128,\n            stride=96,\n        ),\n        mode=\"joint\",\n        target_span_getter=\"ner-gold\",\n        span_setter=\"ents\",\n        questions={\n            \"disease\": \"What disease does the patient have?\",\n            \"drug\": \"What drug is the patient taking?\",\n        },  # (1)!\n    ),\n    name=\"qa\",\n)\n</code></pre> <p>To train the model, refer to the Training tutorial.</p> <p>Once the model is trained, you can use the questions attribute (next section) on the document you run the model on, or you can change the global questions attribute:</p> <pre><code>nlp.pipes.qa.questions = {\n    \"disease\": \"When did the patient get sick?\",\n}\n</code></pre>"},{"location":"reference/edsnlp/pipes/trainable/extractive_qa/factory/#edsnlp.pipes.trainable.extractive_qa.factory.create_component--dynamic-questions","title":"Dynamic Questions","text":"<p>You can also provide</p> <pre><code>eds.extractive_qa(..., questions_attribute=\"questions\")\n</code></pre> <p>to get the questions dynamically from an attribute on the Doc or Span objects (e.g., <code>doc._.questions</code>). This is useful when you want to have different questions depending on the document.</p> <p>To provide questions from a dataframe, you can use the following code:</p> <pre><code>dataframe = pd.DataFrame({\"questions\": ..., \"note_text\": ..., \"note_id\": ...})\nstream = edsnlp.data.from_pandas(\n    dataframe,\n    converter=\"omop\",\n    doc_attributes={\"questions\": \"questions\"},\n)\nstream.map_pipeline(nlp)\nstream.set_processing(backend=\"multiprocessing\")\nout = stream.to_pandas(converters=\"ents\")\n</code></pre>"},{"location":"reference/edsnlp/pipes/trainable/extractive_qa/factory/#edsnlp.pipes.trainable.extractive_qa.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'extractive_qa'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>questions</code> <p>The questions to ask, as a mapping between the entity type and the list of questions to ask for this entity type (or single string if only one question).</p> <p> TYPE: <code>Dict[str, AsList[str]]</code> DEFAULT: <code>{}</code> </p> <code>questions_attribute</code> <p>The attribute to use to get the questions dynamically from the Doc or Span objects (as returned by the <code>context_getter</code> argument). If None, the questions will be fixed and only taken from the <code>questions</code> argument.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>questions</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). For example <code>{\"section\": \"conclusion\"}</code> to only extract the entities from the conclusion.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>target_span_getter</code> <p>Method to call to get the gold spans from a document, for scoring or training. By default, takes all entities in <code>doc.ents</code>, but we recommend you specify a given span group name instead.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use to set the predicted spans on the Doc object. If None, the component will infer the span setter from the target_span_getter config.</p> <p> TYPE: <code>Optional[SpanSetterArg]</code> DEFAULT: <code>None</code> </p> <code>infer_span_setter</code> <p>Whether to complete the span setter from the target_span_getter config. False by default, unless the span_setter is None.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>The CRF mode to use : independent, joint or marginal</p> <p> TYPE: <code>Literal['independent', 'joint', 'marginal']</code> DEFAULT: <code>joint</code> </p> <code>window</code> <p>The window size to use for the CRF. If 0, will use the whole document, at the cost of a longer computation time. If 1, this is equivalent to assuming that the tags are independent and will the component be faster, but with degraded performance. Empirically, we found that a window size of 10 or 20 works well.</p> <p> TYPE: <code>int</code> DEFAULT: <code>40</code> </p> <code>stride</code> <p>The stride to use for the CRF windows. Defaults to <code>window // 2</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/layers/","title":"<code>edsnlp.pipes.trainable.layers</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/layers/crf/","title":"<code>edsnlp.pipes.trainable.layers.crf</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.LinearChainCRF","title":"<code>LinearChainCRF</code>","text":"<p>           Bases: <code>Module</code></p> <p>A linear chain CRF in Pytorch</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.LinearChainCRF--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>forbidden_transitions</code> <p>Shape: n_tags * n_tags Impossible transitions (1 means impossible) from position n to position n+1</p> <p> </p> <code>start_forbidden_transitions</code> <p>Shape: n_tags Impossible transitions at the start of a sequence</p> <p> DEFAULT: <code>None</code> </p> <code>end_forbidden_transitions</code> <p>Shape: n_tags Impossible transitions at the end of a sequence</p> <p> DEFAULT: <code>None</code> </p> <code>learnable_transitions</code> <p>Should we learn transition scores to complete the constraints ?</p> <p> DEFAULT: <code>True</code> </p> <code>with_start_end_transitions</code> <p>Should we apply start-end transitions. If learnable_transitions is True, learn start/end transition scores</p> <p> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.LinearChainCRF.decode","title":"<code>decode</code>","text":"<p>Decodes a sequence of tag scores using the Viterbi algorithm</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.LinearChainCRF.decode--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>emissions</code> <p>Shape: ... * n_tokens * n_tags</p> <p> </p> <code>mask</code> <p>Shape: ... * n_tokens</p> <p> </p> RETURNS DESCRIPTION <code>LongTensor</code> <p>Backtrack indices (= argmax), ie best tag sequence</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.LinearChainCRF.marginal","title":"<code>marginal</code>","text":"<p>Compute the marginal log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF</p> <p>We could use the <code>propagate</code> method but this implementation is faster.</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.LinearChainCRF.marginal--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>emissions</code> <p>Shape: ... * n_tokens * n_tags</p> <p> </p> <code>mask</code> <p>Shape: ... * n_tokens</p> <p> </p> RETURNS DESCRIPTION <code>FloatTensor</code> <p>Shape: ... * n_tokens * n_tags</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.LinearChainCRF.forward","title":"<code>forward</code>","text":"<p>Compute the posterior reduced log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF, ie the loss.</p> <p>We could use the <code>propagate</code> method but this implementation is faster.</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.LinearChainCRF.forward--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>emissions</code> <p>Shape: n_samples * n_tokens * ... * n_tags</p> <p> </p> <code>mask</code> <p>Shape: n_samples * n_tokens * ...</p> <p> </p> <code>target</code> <p>Shape: n_samples * n_tokens * ... * n_tags The target tags represented with 1-hot encoding We use 1-hot instead of long format to handle cases when multiple tags at a given position are allowed during training.</p> <p> </p> RETURNS DESCRIPTION <code>FloatTensor</code> <p>Shape: ... The loss</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.MultiLabelBIOULDecoder","title":"<code>MultiLabelBIOULDecoder</code>","text":"<p>           Bases: <code>LinearChainCRF</code></p> <p>Create a linear chain CRF with hard constraints to enforce the BIOUL tagging scheme</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.MultiLabelBIOULDecoder--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>num_labels</code> <code>with_start_end_transitions</code> <p> DEFAULT: <code>True</code> </p> <code>learnable_transitions</code> <p> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.MultiLabelBIOULDecoder.tags_to_spans","title":"<code>tags_to_spans</code>  <code>staticmethod</code>","text":"<p>Convert a sequence of multiple label BIOUL tags to a sequence of spans</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/crf/#edsnlp.pipes.trainable.layers.crf.MultiLabelBIOULDecoder.tags_to_spans--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>tags</code> <p>Shape: n_samples * n_tokens * n_labels</p> <p> </p> RETURNS DESCRIPTION <code>LongTensor</code> <p>Shape: n_spans *  4 (doc_idx, begin, end, label_idx)</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/metric/","title":"<code>edsnlp.pipes.trainable.layers.metric</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/layers/metric/#edsnlp.pipes.trainable.layers.metric.Metric","title":"<code>Metric</code>","text":"<p>           Bases: <code>Module</code></p> <p>Metric layer, used for computing similarities between two sets of vectors. A typical use case is to compute the similarity between a set of query vectors (input embeddings) and a set of concept vectors (output embeddings).</p>"},{"location":"reference/edsnlp/pipes/trainable/layers/metric/#edsnlp.pipes.trainable.layers.metric.Metric--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>in_features</code> <p>Size of the input embeddings</p> <p> TYPE: <code>int</code> </p> <code>out_features</code> <p>Size of the output embeddings</p> <p> TYPE: <code>int</code> </p> <code>num_groups</code> <p>Number of groups for the output embeddings, that can be used to filter out certain concepts that are not relevant for a given query (e.g. do not compare a drug with concepts for diseases)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>metric</code> <p>Whether to compute the cosine similarity between the input and output embeddings or the dot product.</p> <p> TYPE: <code>Literal['cosine', 'dot']</code> DEFAULT: <code>'cosine'</code> </p> <code>rescale</code> <p>Rescale the output cosine similarities by a constant factor.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/layers/text_cnn/","title":"<code>edsnlp.pipes.trainable.layers.text_cnn</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/layers/text_cnn/#edsnlp.pipes.trainable.layers.text_cnn.TextCnn","title":"<code>TextCnn</code>","text":"<p>           Bases: <code>Module</code></p>"},{"location":"reference/edsnlp/pipes/trainable/layers/text_cnn/#edsnlp.pipes.trainable.layers.text_cnn.TextCnn--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>input_size</code> <p>Size of the input embeddings</p> <p> TYPE: <code>int</code> </p> <code>output_size</code> <p>Size of the output embeddings Defaults to the <code>input_size</code></p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>out_channels</code> <p>Number of channels</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>kernel_sizes</code> <p>Window size of each kernel</p> <p> TYPE: <code>Sequence[int]</code> DEFAULT: <code>(3, 4, 5)</code> </p> <code>activation</code> <p>Activation function to use</p> <p> TYPE: <code>ActivationFunction</code> DEFAULT: <code>'relu'</code> </p> <code>residual</code> <p>Whether to use residual connections</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>normalize</code> <p>Whether to normalize before or after the residual connection</p> <p> TYPE: <code>Literal['pre', 'post', 'none']</code> DEFAULT: <code>'pre'</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/","title":"<code>edsnlp.pipes.trainable.ner_crf</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/ner_crf/factory/","title":"<code>edsnlp.pipes.trainable.ner_crf.factory</code>","text":"<ol><li><p><p>Wajsb\u00fcrt P., 2021. Extraction and normalization of simple and structured entities in medical documents. https://hal.archives-ouvertes.fr/tel-03624928</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/factory/#edsnlp.pipes.trainable.ner_crf.factory.create_component","title":"<code>create_component = registry.factory.register('eds.ner_crf', assigns=['doc.ents', 'doc.spans'], deprecated=['eds.nested_ner', 'nested_ner'])(TrainableNerCrf)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.ner_crf</code> component is a general purpose trainable named entity recognizer. It can extract:</p> <ul> <li>flat entities</li> <li>overlapping entities of different labels</li> </ul> <p>However, at the moment, the model cannot currently extract entities that are nested inside larger entities of the same label.</p> <p>It is based on a CRF (Conditional Random Field) layer and should therefore work well on dataset composed of entities will ill-defined boundaries. We offer a compromise between speed and performance by allowing the user to specify a window size for the CRF layer. The smaller the window, the faster the model will be, but at the cost of degraded performance.</p> <p>The pipeline assigns both <code>doc.ents</code> (in which overlapping entities are filtered out) and <code>doc.spans</code>. These destinations can be inferred from the <code>target_span_getter</code> parameter, combined with the <code>post_init</code> step.</p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/factory/#edsnlp.pipes.trainable.ner_crf.factory.create_component--architecture","title":"Architecture","text":"<p>The model performs token classification using the BIOUL (Begin, Inside, Outside, Unary, Last) tagging scheme. To extract overlapping entities, each label has its own tag sequence, so the model predicts <code>n_labels</code> sequences of O, I, B, L, U tags. The architecture is displayed in the figure below.</p> <p>To enforce the tagging scheme, (ex: I cannot follow O but only B, ...), we use a stack of CRF (Conditional Random Fields) layers, one per label during both training and prediction.</p> <p> </p> Nested NER architecture"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/factory/#edsnlp.pipes.trainable.ner_crf.factory.create_component--examples","title":"Examples","text":"<p>Let us define a pipeline composed of a transformer, and a NER component.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.ner_crf(\n        embedding=eds.transformer(\n            model=\"prajjwal1/bert-tiny\",\n            window=128,\n            stride=96,\n        ),\n        mode=\"joint\",\n        target_span_getter=\"ner-gold\",\n        span_setter=\"ents\",\n        window=10,\n    ),\n    name=\"ner\"\n)\n</code></pre> <p>To train the model, refer to the Training tutorial.</p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/factory/#edsnlp.pipes.trainable.ner_crf.factory.create_component--extensions","title":"Extensions","text":"<p>Experimental Confidence Score</p> <p>The NER confidence score feature is experimental and the API and underlying algorithm may change.</p> <p>The <code>eds.ner_crf</code> pipeline declares one extension on the <code>Span</code> object:</p> <ul> <li><code>span._.ner_confidence_score</code>: The confidence score of the Named Entity Recognition (NER) model for the given span.</li> </ul> <p>The <code>ner_confidence_score</code> is computed based on the Average Entity Confidence Score using the following formula:</p> <p>$$ \\text{Average Entity Confidence Score} = \\frac{1}{n} \\sum_{i \\in \\text{tokens}} (1 - p(O)_i) $$</p> <p>Where:</p> <ul> <li>$n$ is the number of tokens.</li> <li>$\\text{tokens}$ refers to the tokens within the span.</li> <li>$p(O)_i$ represents the probability of token $i$ belonging to class 'O' (Outside entity).</li> </ul> <p>Confidence score is not computed by default</p> <p>By default, the confidence score is not computed, as it adds around 5% to inference time. You can enable its computation with: <pre><code>nlp.pipes.ner.compute_confidence_score = True\n</code></pre></p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/factory/#edsnlp.pipes.trainable.ner_crf.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'ner_crf'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>target_span_getter</code> <p>Method to call to get the gold spans from a document, for scoring or training. By default, takes all entities in <code>doc.ents</code>, but we recommend you specify a given span group name instead.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>labels</code> <p>The labels to predict. The labels can also be inferred from the data during <code>nlp.post_init(...)</code></p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use to set the predicted spans on the Doc object. If None, the component will infer the span setter from the target_span_getter config.</p> <p> TYPE: <code>Optional[SpanSetterArg]</code> DEFAULT: <code>None</code> </p> <code>infer_span_setter</code> <p>Whether to complete the span setter from the target_span_getter config. False by default, unless the span_setter is None.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). For example <code>{\"section\": \"conclusion\"}</code> to only extract the entities from the conclusion.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>The CRF mode to use : independent, joint or marginal</p> <p> TYPE: <code>Literal['independent', 'joint', 'marginal']</code> </p> <code>window</code> <p>The window size to use for the CRF. If 0, will use the whole document, at the cost of a longer computation time. If 1, this is equivalent to assuming that the tags are independent and will the component be faster, but with degraded performance. Empirically, we found that a window size of 10 or 20 works well.</p> <p> TYPE: <code>int</code> DEFAULT: <code>40</code> </p> <code>stride</code> <p>The stride to use for the CRF windows. Defaults to <code>window // 2</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/factory/#edsnlp.pipes.trainable.ner_crf.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.ner_crf</code> pipeline was developed by AP-HP's Data Science team.</p> <p>The deep learning model was adapted from Wajsb\u00fcrt, 2021.</p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/","title":"<code>edsnlp.pipes.trainable.ner_crf.ner_crf</code>","text":"<ol><li><p><p>Wajsb\u00fcrt P., 2021. Extraction and normalization of simple and structured entities in medical documents. https://hal.archives-ouvertes.fr/tel-03624928</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/#edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf","title":"<code>TrainableNerCrf</code>","text":"<p>           Bases: <code>TorchComponent[NERBatchOutput, NERBatchInput]</code>, <code>BaseNERComponent</code></p> <p>The <code>eds.ner_crf</code> component is a general purpose trainable named entity recognizer. It can extract:</p> <ul> <li>flat entities</li> <li>overlapping entities of different labels</li> </ul> <p>However, at the moment, the model cannot currently extract entities that are nested inside larger entities of the same label.</p> <p>It is based on a CRF (Conditional Random Field) layer and should therefore work well on dataset composed of entities will ill-defined boundaries. We offer a compromise between speed and performance by allowing the user to specify a window size for the CRF layer. The smaller the window, the faster the model will be, but at the cost of degraded performance.</p> <p>The pipeline assigns both <code>doc.ents</code> (in which overlapping entities are filtered out) and <code>doc.spans</code>. These destinations can be inferred from the <code>target_span_getter</code> parameter, combined with the <code>post_init</code> step.</p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/#edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf--architecture","title":"Architecture","text":"<p>The model performs token classification using the BIOUL (Begin, Inside, Outside, Unary, Last) tagging scheme. To extract overlapping entities, each label has its own tag sequence, so the model predicts <code>n_labels</code> sequences of O, I, B, L, U tags. The architecture is displayed in the figure below.</p> <p>To enforce the tagging scheme, (ex: I cannot follow O but only B, ...), we use a stack of CRF (Conditional Random Fields) layers, one per label during both training and prediction.</p> <p> </p> Nested NER architecture"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/#edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf--examples","title":"Examples","text":"<p>Let us define a pipeline composed of a transformer, and a NER component.</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.ner_crf(\n        embedding=eds.transformer(\n            model=\"prajjwal1/bert-tiny\",\n            window=128,\n            stride=96,\n        ),\n        mode=\"joint\",\n        target_span_getter=\"ner-gold\",\n        span_setter=\"ents\",\n        window=10,\n    ),\n    name=\"ner\"\n)\n</code></pre> <p>To train the model, refer to the Training tutorial.</p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/#edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf--extensions","title":"Extensions","text":"<p>Experimental Confidence Score</p> <p>The NER confidence score feature is experimental and the API and underlying algorithm may change.</p> <p>The <code>eds.ner_crf</code> pipeline declares one extension on the <code>Span</code> object:</p> <ul> <li><code>span._.ner_confidence_score</code>: The confidence score of the Named Entity Recognition (NER) model for the given span.</li> </ul> <p>The <code>ner_confidence_score</code> is computed based on the Average Entity Confidence Score using the following formula:</p> <p>$$ \\text{Average Entity Confidence Score} = \\frac{1}{n} \\sum_{i \\in \\text{tokens}} (1 - p(O)_i) $$</p> <p>Where:</p> <ul> <li>$n$ is the number of tokens.</li> <li>$\\text{tokens}$ refers to the tokens within the span.</li> <li>$p(O)_i$ represents the probability of token $i$ belonging to class 'O' (Outside entity).</li> </ul> <p>Confidence score is not computed by default</p> <p>By default, the confidence score is not computed, as it adds around 5% to inference time. You can enable its computation with: <pre><code>nlp.pipes.ner.compute_confidence_score = True\n</code></pre></p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/#edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'ner_crf'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>WordEmbeddingComponent</code> </p> <code>target_span_getter</code> <p>Method to call to get the gold spans from a document, for scoring or training. By default, takes all entities in <code>doc.ents</code>, but we recommend you specify a given span group name instead.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>{'ents': True}</code> </p> <code>labels</code> <p>The labels to predict. The labels can also be inferred from the data during <code>nlp.post_init(...)</code></p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>span_setter</code> <p>The span setter to use to set the predicted spans on the Doc object. If None, the component will infer the span setter from the target_span_getter config.</p> <p> TYPE: <code>Optional[SpanSetterArg]</code> DEFAULT: <code>None</code> </p> <code>infer_span_setter</code> <p>Whether to complete the span setter from the target_span_getter config. False by default, unless the span_setter is None.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). For example <code>{\"section\": \"conclusion\"}</code> to only extract the entities from the conclusion.</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>The CRF mode to use : independent, joint or marginal</p> <p> TYPE: <code>Literal['independent', 'joint', 'marginal']</code> </p> <code>window</code> <p>The window size to use for the CRF. If 0, will use the whole document, at the cost of a longer computation time. If 1, this is equivalent to assuming that the tags are independent and will the component be faster, but with degraded performance. Empirically, we found that a window size of 10 or 20 works well.</p> <p> TYPE: <code>int</code> DEFAULT: <code>40</code> </p> <code>stride</code> <p>The stride to use for the CRF windows. Defaults to <code>window // 2</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/#edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.ner_crf</code> pipeline was developed by AP-HP's Data Science team.</p> <p>The deep learning model was adapted from Wajsb\u00fcrt, 2021.</p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/#edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf.set_extensions","title":"<code>set_extensions</code>","text":"<p>Set spaCy extensions</p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/#edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf.post_init","title":"<code>post_init</code>","text":"<p>Update the labels based on the data and the span getter, and fills in the to_ents and to_span_groups if necessary</p>"},{"location":"reference/edsnlp/pipes/trainable/ner_crf/ner_crf/#edsnlp.pipes.trainable.ner_crf.ner_crf.TrainableNerCrf.post_init--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>docs</code> <p>The documents to use to infer the labels</p> <p> TYPE: <code>Iterable[Doc]</code> </p> <code>exclude</code> <p>Components to exclude from the post initialization</p> <p> TYPE: <code>Set[str]</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/","title":"<code>edsnlp.pipes.trainable.span_classifier</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/span_classifier/factory/","title":"<code>edsnlp.pipes.trainable.span_classifier.factory</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/span_classifier/factory/#edsnlp.pipes.trainable.span_classifier.factory.create_component","title":"<code>create_component = registry.factory.register('eds.span_classifier', assigns=[], deprecated=['eds.span_qualifier'])(TrainableSpanClassifier)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.span_classifier</code> component is a trainable attribute predictor. In this context, the span classification task consists in assigning values (boolean, strings or any object) to attributes/extensions of spans such as:</p> <ul> <li><code>span._.negation</code>,</li> <li><code>span._.date.mode</code></li> <li><code>span._.cui</code></li> </ul> <p>In the rest of this page, we will refer to a pair of (attribute, value) as a \"binding\". For instance, the binding <code>(\"_.negation\", True)</code> means that the attribute <code>negation</code> of the span is (or should be, when predicted) set to <code>True</code>.</p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/factory/#edsnlp.pipes.trainable.span_classifier.factory.create_component--architecture","title":"Architecture","text":"<p>The model performs span classification by:</p> <ol> <li>Calling a word pooling embedding such as <code>eds.span_pooler</code> to compute a single embedding for each span</li> <li>Computing logits for each possible binding using a linear layer</li> <li> <p>Splitting these bindings into groups of exclusive values such as</p> <ul> <li><code>event=start</code> and <code>event=stop</code></li> <li><code>negated=False</code> and <code>negated=True</code></li> </ul> <p>Note that the above groups are not exclusive, but the values within each group are.</p> </li> <li> <p>Applying the best scoring binding in each group to each span</p> </li> </ol>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/factory/#edsnlp.pipes.trainable.span_classifier.factory.create_component--examples","title":"Examples","text":"<p>To create a span classifier component, you can use the following code:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.span_classifier(\n        # To embed the spans, we will use a span pooler\n        embedding=eds.span_pooler(\n            pooling_mode=\"mean\",  # mean pooling\n            # that will use a transformer to embed the doc words\n            embedding=eds.transformer(\n                model=\"prajjwal1/bert-tiny\",\n                window=128,\n                stride=96,\n            ),\n        ),\n        span_getter=[\"ents\", \"sc\"],\n        # For every span embedded by the span pooler\n        # (doc.ents and doc.spans[\"sc\"]), we will predict both\n        # span._.negation and span._.event_type\n        attributes=[\"_.negation\", \"_.event_type\"],\n    ),\n    name=\"span_classifier\",\n)\n</code></pre> <p>To infer the values of the attributes, you can use the pipeline <code>post_init</code> method:</p> <pre><code>nlp.post_init(gold_data)\n</code></pre> <p>To train the model, refer to the Training tutorial.</p> <p>You can inspect the bindings that will be used for training and prediction <pre><code>print(nlp.pipes.attr.bindings)\n# list of (attr name, span labels or True if all, values)\n# Out: [\n#   ('_.negation', True, [True, False]),\n#   ('_.event_type', True, ['start', 'stop'])\n# ]\n</code></pre></p> <p>You can also change these values and update the bindings by calling the <code>update_bindings</code> method. Don't forget to retrain the model if new values are added !</p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/factory/#edsnlp.pipes.trainable.span_classifier.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'span_classifier'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>SpanEmbeddingComponent</code> </p> <code>label_weights</code> <p>The weight of each label for each attribute. The keys are the attribute names and the values are dictionaries with the labels as keys and the weights as values. For instance, <code>{\"_.negation\": {True: 1, False: 2}}</code> will give a weight of 1 to the <code>True</code> value of the <code>negation</code> attribute and 2 to the <code>False</code> value.</p> <p> TYPE: <code>Dict[str, Dict[Any, float]]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>How to extract the candidate spans and the attributes to predict or train on.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). This can be:</p> <ul> <li>a <code>SpanGetterArg</code> to retrieve contexts from a whole document. For example   <code>{\"section\": \"conclusion\"}</code> to only use the conclusion as context (you   must ensure that all spans produced by the <code>span_getter</code> argument do fall   in the conclusion in this case)</li> <li>a callable, that gets a span and should return a context for this span.   For instance, <code>lambda span: span.sent</code> to use the sentence as context.</li> </ul> <p> TYPE: <code>Optional[Union[Callable, SpanGetterArg]]</code> DEFAULT: <code>None</code> </p> <code>attributes</code> <p>The attributes to predict or train on. If a dict is given, keys are the attributes and values are the labels for which the attr is allowed, or True if the attr is allowed for all labels.</p> <p> TYPE: <code>AttributesArg</code> DEFAULT: <code>None</code> </p> <code>keep_none</code> <p>If False, skip spans for which a attr returns None. If True (default), the None values will be learned and predicted, just as any other value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/","title":"<code>edsnlp.pipes.trainable.span_classifier.span_classifier</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/#edsnlp.pipes.trainable.span_classifier.span_classifier.SpanClassifierBatchInput","title":"<code>SpanClassifierBatchInput = TypedDict('SpanClassifierBatchInput', {'embedding': BatchInput, 'targets': NotRequired[torch.Tensor]})</code>  <code>module-attribute</code>","text":"<p>embeds: torch.FloatTensor     Token embeddings to predict the tags from mask: torch.BoolTensor     Mask of the sequences spans: torch.Tensor     2d tensor of n_spans * (doc_idx, ner_label_idx, begin, end) targets: NotRequired[List[torch.Tensor]]     list of 2d tensor of n_spans * n_combinations (1 hot)</p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/#edsnlp.pipes.trainable.span_classifier.span_classifier.SpanClassifierBatchOutput","title":"<code>SpanClassifierBatchOutput = TypedDict('SpanClassifierBatchOutput', {'loss': Optional[torch.Tensor], 'labels': Optional[List[torch.Tensor]]})</code>  <code>module-attribute</code>","text":"<p>loss: Optional[torch.Tensor]     The loss of the model labels: Optional[List[torch.Tensor]]     The predicted labels</p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/#edsnlp.pipes.trainable.span_classifier.span_classifier.TrainableSpanClassifier","title":"<code>TrainableSpanClassifier</code>","text":"<p>           Bases: <code>TorchComponent[BatchOutput, SpanClassifierBatchInput]</code>, <code>BaseSpanAttributeClassifierComponent</code></p> <p>The <code>eds.span_classifier</code> component is a trainable attribute predictor. In this context, the span classification task consists in assigning values (boolean, strings or any object) to attributes/extensions of spans such as:</p> <ul> <li><code>span._.negation</code>,</li> <li><code>span._.date.mode</code></li> <li><code>span._.cui</code></li> </ul> <p>In the rest of this page, we will refer to a pair of (attribute, value) as a \"binding\". For instance, the binding <code>(\"_.negation\", True)</code> means that the attribute <code>negation</code> of the span is (or should be, when predicted) set to <code>True</code>.</p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/#edsnlp.pipes.trainable.span_classifier.span_classifier.TrainableSpanClassifier--architecture","title":"Architecture","text":"<p>The model performs span classification by:</p> <ol> <li>Calling a word pooling embedding such as <code>eds.span_pooler</code> to compute a single embedding for each span</li> <li>Computing logits for each possible binding using a linear layer</li> <li> <p>Splitting these bindings into groups of exclusive values such as</p> <ul> <li><code>event=start</code> and <code>event=stop</code></li> <li><code>negated=False</code> and <code>negated=True</code></li> </ul> <p>Note that the above groups are not exclusive, but the values within each group are.</p> </li> <li> <p>Applying the best scoring binding in each group to each span</p> </li> </ol>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/#edsnlp.pipes.trainable.span_classifier.span_classifier.TrainableSpanClassifier--examples","title":"Examples","text":"<p>To create a span classifier component, you can use the following code:</p> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.span_classifier(\n        # To embed the spans, we will use a span pooler\n        embedding=eds.span_pooler(\n            pooling_mode=\"mean\",  # mean pooling\n            # that will use a transformer to embed the doc words\n            embedding=eds.transformer(\n                model=\"prajjwal1/bert-tiny\",\n                window=128,\n                stride=96,\n            ),\n        ),\n        span_getter=[\"ents\", \"sc\"],\n        # For every span embedded by the span pooler\n        # (doc.ents and doc.spans[\"sc\"]), we will predict both\n        # span._.negation and span._.event_type\n        attributes=[\"_.negation\", \"_.event_type\"],\n    ),\n    name=\"span_classifier\",\n)\n</code></pre> <p>To infer the values of the attributes, you can use the pipeline <code>post_init</code> method:</p> <pre><code>nlp.post_init(gold_data)\n</code></pre> <p>To train the model, refer to the Training tutorial.</p> <p>You can inspect the bindings that will be used for training and prediction <pre><code>print(nlp.pipes.attr.bindings)\n# list of (attr name, span labels or True if all, values)\n# Out: [\n#   ('_.negation', True, [True, False]),\n#   ('_.event_type', True, ['start', 'stop'])\n# ]\n</code></pre></p> <p>You can also change these values and update the bindings by calling the <code>update_bindings</code> method. Don't forget to retrain the model if new values are added !</p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/#edsnlp.pipes.trainable.span_classifier.span_classifier.TrainableSpanClassifier--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline object</p> <p> TYPE: <code>PipelineProtocol</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'span_classifier'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>SpanEmbeddingComponent</code> </p> <code>label_weights</code> <p>The weight of each label for each attribute. The keys are the attribute names and the values are dictionaries with the labels as keys and the weights as values. For instance, <code>{\"_.negation\": {True: 1, False: 2}}</code> will give a weight of 1 to the <code>True</code> value of the <code>negation</code> attribute and 2 to the <code>False</code> value.</p> <p> TYPE: <code>Dict[str, Dict[Any, float]]</code> DEFAULT: <code>None</code> </p> <code>span_getter</code> <p>How to extract the candidate spans and the attributes to predict or train on.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the whole document). This can be:</p> <ul> <li>a <code>SpanGetterArg</code> to retrieve contexts from a whole document. For example   <code>{\"section\": \"conclusion\"}</code> to only use the conclusion as context (you   must ensure that all spans produced by the <code>span_getter</code> argument do fall   in the conclusion in this case)</li> <li>a callable, that gets a span and should return a context for this span.   For instance, <code>lambda span: span.sent</code> to use the sentence as context.</li> </ul> <p> TYPE: <code>Optional[Union[Callable, SpanGetterArg]]</code> DEFAULT: <code>None</code> </p> <code>attributes</code> <p>The attributes to predict or train on. If a dict is given, keys are the attributes and values are the labels for which the attr is allowed, or True if the attr is allowed for all labels.</p> <p> TYPE: <code>AttributesArg</code> DEFAULT: <code>None</code> </p> <code>keep_none</code> <p>If False, skip spans for which a attr returns None. If True (default), the None values will be learned and predicted, just as any other value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/#edsnlp.pipes.trainable.span_classifier.span_classifier.TrainableSpanClassifier.forward","title":"<code>forward</code>","text":"<p>Apply the span classifier module to the document embeddings and given spans to: - compute the loss - and/or predict the labels of spans</p>"},{"location":"reference/edsnlp/pipes/trainable/span_classifier/span_classifier/#edsnlp.pipes.trainable.span_classifier.span_classifier.TrainableSpanClassifier.forward--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch</code> <p>The input batch</p> <p> TYPE: <code>SpanClassifierBatchInput</code> </p> RETURNS DESCRIPTION <code>BatchOutput</code>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/","title":"<code>edsnlp.pipes.trainable.span_linker</code>","text":""},{"location":"reference/edsnlp/pipes/trainable/span_linker/factory/","title":"<code>edsnlp.pipes.trainable.span_linker.factory</code>","text":"<ol><li><p><p>Wajsb\u00fcrt P., Sarfati A. and Tannier X., 2021. Medical concept normalization in French using multilingual terminologies and contextual embeddings. Journal of Biomedical Informatics. 114, pp.103684. https://doi.org/10.1016/j.jbi.2021.103684</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/factory/#edsnlp.pipes.trainable.span_linker.factory.create_component","title":"<code>create_component = registry.factory.register('eds.span_linker', assigns=[], deprecated=[])(TrainableSpanLinker)</code>  <code>module-attribute</code>","text":"<p>The <code>eds.span_linker</code> component is a trainable span concept predictor, typically used to match spans in the text with concepts in a knowledge base. This task is known as \"Entity Linking\", \"Named Entity Disambiguation\" or \"Normalization\" (the latter is mostly used in the biomedical machine learning community).</p> <p>Entity Linking vs Named Entity Recognition</p> <p>Entity Linking is the task of linking existing entities to their concept in a knowledge base, while Named Entity Recognition is the task of detecting spans in the text that correspond to entities. The <code>eds.span_linker</code> component should therefore be used after the Named Entity Recognition step (e.g. using the <code>eds.ner_crf</code> component).</p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/factory/#edsnlp.pipes.trainable.span_linker.factory.create_component--how-it-works","title":"How it works","text":"<p>To perform this task, this components compare the embedding of a given query span (e.g. \"aspirin\") with the embeddings in the knowledge base, where each embedding represents a concept (e.g. \"B01AC06\"), and selects the most similar embedding and returns its concept id. This comparison is done using either:</p> <ul> <li>the cosine similarity between the input and output embeddings (recommended)</li> <li>a simple dot product</li> </ul> <p>We filter out the concepts that are not relevant for a given query by using groups. For each span to link, we use its label to select a group of concepts to compare with. For example, if the span is labeled as \"drug\", we only compare it with concepts that are drugs. These concepts groups are inferred from the training data when running the <code>post_init</code> method, or can be provided manually using the <code>pipe.update_concepts(concepts, mapping, [embeddings])</code> method. If a label is not found in the mapping, the span is compared with all concepts.</p> <p>We support comparing entity queries against two kind of references : either the embeddings of the concepts themselves (<code>reference_mode = \"concept\"</code>), or the embeddings of the synonyms of the concepts (<code>reference_mode = \"synonym\"</code>).</p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/factory/#edsnlp.pipes.trainable.span_linker.factory.create_component--synonym-similarity","title":"Synonym similarity","text":"<p>When performing span linking in <code>synonym</code> mode, the span linker embedding matrix contains one embedding vector per concept per synonym, and each embedding maps to the concept of its synonym. This mode is slower and more memory intensive, since you have to store multiple embeddings per concept, but it can yield good results in zero-shot scenarios (see example below).</p> <p> </p> Entity linking based on synonym similarity"},{"location":"reference/edsnlp/pipes/trainable/span_linker/factory/#edsnlp.pipes.trainable.span_linker.factory.create_component--concept-similarity","title":"Concept similarity","text":"<p>In <code>concept</code> mode, the span linker embedding matrix contains one embedding vector per concept : imagine a single vector that approximately averages all the synonyms of a concept (e.g. B01AC06 = average of \"aspirin\", \"acetyl-salicylic acid\", etc.). This mode is faster and more memory efficient, but usually requires that the concept weights are fine-tuned.</p> <p> </p> Entity linking based on concept similarity"},{"location":"reference/edsnlp/pipes/trainable/span_linker/factory/#edsnlp.pipes.trainable.span_linker.factory.create_component--examples","title":"Examples","text":"<p>Here is how you can use the <code>eds.span_linker</code> component to link spans without training, in <code>synonym</code> mode. You will still need to pre-compute the embeddings of the target synonyms.</p> <p>First, initialize the component:</p> <pre><code>import pandas as pd\nimport edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.span_linker(\n        rescale=20.0,\n        threshold=0.8,\n        metric=\"cosine\",\n        reference_mode=\"synonym\",\n        probability_mode=\"sigmoid\",\n        span_getter=[\"ents\"],\n        embedding=eds.span_pooler(\n            hidden_size=128,\n            embedding=eds.transformer(\n                model=\"prajjwal1/bert-tiny\",\n                window=128,\n                stride=96,\n            ),\n        ),\n    ),\n    name=\"linker\",\n)\n</code></pre> <p>We will assume you have a list of synonyms with their concept and label with the columns:</p> <ul> <li><code>STR</code>: synonym text</li> <li><code>CUI</code>: concept id</li> <li><code>GRP</code>: label.</li> </ul> <p>All we need to do is to initialize the component with the synonyms and that's it ! Since we have set <code>init_weights</code> to True, and we are in <code>synonym</code> mode, the embeddings of the synonyms will be stored in the component and used to compute the similarity scores</p> <pre><code>synonyms_df = pd.read_csv(\"synonyms.csv\")\n\ndef make_doc(row):\n    doc = nlp.make_doc(row[\"STR\"])\n    span = doc[:]\n    span.label_ = row[\"GRP\"]\n    doc.ents = [span]\n    span._.cui = row[\"CUI\"]\n    return doc\n\nnlp.post_init(\n    edsnlp.data.from_pandas(\n        synonyms_df,\n        converter=make_doc,\n    )\n)\n</code></pre> <p>Now, you can now use it in a text: <pre><code>doc = nlp.make_doc(\"Aspirin is a drug\")\nspan = doc[0:1]  # \"Aspirin\"\nspan.label_ = \"Drug\"\ndoc.ents = [span]\n\ndoc = nlp(doc)\nprint(doc.ents[0]._.cui)\n# \"B01AC06\"\n</code></pre></p> <p>To use the <code>eds.span_linker</code> component in <code>class</code> mode, we refer to the following repository: deep_multilingual_normalization based on the work of Wajsb\u00fcrt et al., 2021.</p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/factory/#edsnlp.pipes.trainable.span_linker.factory.create_component--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Spacy vocabulary</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'span_linker'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>SpanEmbeddingComponent</code> </p> <code>metric</code> <p>Whether to compute the cosine similarity between the input and output embeddings or the dot product.</p> <p> TYPE: <code>Literal[\"cosine\", \"dot\"] = \"cosine\"</code> DEFAULT: <code>cosine</code> </p> <code>rescale</code> <p>Rescale the output cosine similarities by a constant factor.</p> <p> TYPE: <code>float</code> DEFAULT: <code>20</code> </p> <code>threshold</code> <p>Threshold probability to consider a concept as valid</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>attribute</code> <p>The attribute to store the concept id</p> <p> TYPE: <code>str</code> DEFAULT: <code>cui</code> </p> <code>reference_mode</code> <p>Whether to compare the embeddings with the concepts embeddings (one per concept) or the synonyms embeddings (one per concept per synonym). See above for more details.</p> <p> TYPE: <code>Literal['concept', 'synonym']</code> DEFAULT: <code>concept</code> </p> <code>span_getter</code> <p>How to extract the candidate spans to predict or train on.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the entity only, so no context)</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>probability_mode</code> <p>Whether to compute the probabilities using a softmax or a sigmoid function. This will also determine the loss function to use, either cross-entropy or binary cross-entropy.</p> <p>Subsetting the concepts</p> <p>The probabilities returned in <code>softmax</code> mode depend on the number of concepts (as an extreme cas, if you have only one concept, its softmax probability will always be 1). This is why we recommend using the <code>sigmoid</code> mode in which the probabilities are computed independently for each concept.</p> <p> TYPE: <code>Literal['softmax', 'sigmoid']</code> DEFAULT: <code>sigmoid</code> </p> <code>init_weights</code> <p>Whether to initialize the weights of the component with the embeddings of the entities of the docs provided to the <code>post_init</code> method. How this is done depends on the <code>reference_mode</code> parameter:</p> <ul> <li><code>concept</code>: the embeddings are averaged</li> <li><code>synonym</code>: the embeddings are stored as is</li> </ul> <p>By default, this is set to <code>True</code> if <code>reference_mode</code> is <code>synonym</code>, and <code>False</code> otherwise.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/factory/#edsnlp.pipes.trainable.span_linker.factory.create_component--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.span_linker</code> component was developed by AP-HP's Data Science team.</p> <p>The deep learning concept-based architecture was adapted from Wajsb\u00fcrt et al., 2021.</p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/","title":"<code>edsnlp.pipes.trainable.span_linker.span_linker</code>","text":"<ol><li><p><p>Wajsb\u00fcrt P., Sarfati A. and Tannier X., 2021. Medical concept normalization in French using multilingual terminologies and contextual embeddings. Journal of Biomedical Informatics. 114, pp.103684. https://doi.org/10.1016/j.jbi.2021.103684</p></p></li></ol>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/#edsnlp.pipes.trainable.span_linker.span_linker.SpanLinkerBatchInput","title":"<code>SpanLinkerBatchInput = TypedDict('SpanLinkerBatchInput', {'embedding': BatchInput, 'span_labels': torch.Tensor, 'concepts': NotRequired[torch.Tensor]})</code>  <code>module-attribute</code>","text":"<p>embeds: torch.FloatTensor     Token embeddings to predict the tags from mask: torch.BoolTensor     Mask of the sequences spans: torch.LongTensor     2d tensor of n_spans * (doc_idx, ner_label_idx, begin, end) targets: NotRequired[List[torch.LongTensor]]     list of 2d tensor of n_spans * n_combinations (1 hot)</p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/#edsnlp.pipes.trainable.span_linker.span_linker.TrainableSpanLinker","title":"<code>TrainableSpanLinker</code>","text":"<p>           Bases: <code>TorchComponent[BatchOutput, SpanLinkerBatchInput]</code>, <code>BaseSpanAttributeClassifierComponent</code></p> <p>The <code>eds.span_linker</code> component is a trainable span concept predictor, typically used to match spans in the text with concepts in a knowledge base. This task is known as \"Entity Linking\", \"Named Entity Disambiguation\" or \"Normalization\" (the latter is mostly used in the biomedical machine learning community).</p> <p>Entity Linking vs Named Entity Recognition</p> <p>Entity Linking is the task of linking existing entities to their concept in a knowledge base, while Named Entity Recognition is the task of detecting spans in the text that correspond to entities. The <code>eds.span_linker</code> component should therefore be used after the Named Entity Recognition step (e.g. using the <code>eds.ner_crf</code> component).</p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/#edsnlp.pipes.trainable.span_linker.span_linker.TrainableSpanLinker--how-it-works","title":"How it works","text":"<p>To perform this task, this components compare the embedding of a given query span (e.g. \"aspirin\") with the embeddings in the knowledge base, where each embedding represents a concept (e.g. \"B01AC06\"), and selects the most similar embedding and returns its concept id. This comparison is done using either:</p> <ul> <li>the cosine similarity between the input and output embeddings (recommended)</li> <li>a simple dot product</li> </ul> <p>We filter out the concepts that are not relevant for a given query by using groups. For each span to link, we use its label to select a group of concepts to compare with. For example, if the span is labeled as \"drug\", we only compare it with concepts that are drugs. These concepts groups are inferred from the training data when running the <code>post_init</code> method, or can be provided manually using the <code>pipe.update_concepts(concepts, mapping, [embeddings])</code> method. If a label is not found in the mapping, the span is compared with all concepts.</p> <p>We support comparing entity queries against two kind of references : either the embeddings of the concepts themselves (<code>reference_mode = \"concept\"</code>), or the embeddings of the synonyms of the concepts (<code>reference_mode = \"synonym\"</code>).</p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/#edsnlp.pipes.trainable.span_linker.span_linker.TrainableSpanLinker--synonym-similarity","title":"Synonym similarity","text":"<p>When performing span linking in <code>synonym</code> mode, the span linker embedding matrix contains one embedding vector per concept per synonym, and each embedding maps to the concept of its synonym. This mode is slower and more memory intensive, since you have to store multiple embeddings per concept, but it can yield good results in zero-shot scenarios (see example below).</p> <p> </p> Entity linking based on synonym similarity"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/#edsnlp.pipes.trainable.span_linker.span_linker.TrainableSpanLinker--concept-similarity","title":"Concept similarity","text":"<p>In <code>concept</code> mode, the span linker embedding matrix contains one embedding vector per concept : imagine a single vector that approximately averages all the synonyms of a concept (e.g. B01AC06 = average of \"aspirin\", \"acetyl-salicylic acid\", etc.). This mode is faster and more memory efficient, but usually requires that the concept weights are fine-tuned.</p> <p> </p> Entity linking based on concept similarity"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/#edsnlp.pipes.trainable.span_linker.span_linker.TrainableSpanLinker--examples","title":"Examples","text":"<p>Here is how you can use the <code>eds.span_linker</code> component to link spans without training, in <code>synonym</code> mode. You will still need to pre-compute the embeddings of the target synonyms.</p> <p>First, initialize the component:</p> <pre><code>import pandas as pd\nimport edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\n    eds.span_linker(\n        rescale=20.0,\n        threshold=0.8,\n        metric=\"cosine\",\n        reference_mode=\"synonym\",\n        probability_mode=\"sigmoid\",\n        span_getter=[\"ents\"],\n        embedding=eds.span_pooler(\n            hidden_size=128,\n            embedding=eds.transformer(\n                model=\"prajjwal1/bert-tiny\",\n                window=128,\n                stride=96,\n            ),\n        ),\n    ),\n    name=\"linker\",\n)\n</code></pre> <p>We will assume you have a list of synonyms with their concept and label with the columns:</p> <ul> <li><code>STR</code>: synonym text</li> <li><code>CUI</code>: concept id</li> <li><code>GRP</code>: label.</li> </ul> <p>All we need to do is to initialize the component with the synonyms and that's it ! Since we have set <code>init_weights</code> to True, and we are in <code>synonym</code> mode, the embeddings of the synonyms will be stored in the component and used to compute the similarity scores</p> <pre><code>synonyms_df = pd.read_csv(\"synonyms.csv\")\n\ndef make_doc(row):\n    doc = nlp.make_doc(row[\"STR\"])\n    span = doc[:]\n    span.label_ = row[\"GRP\"]\n    doc.ents = [span]\n    span._.cui = row[\"CUI\"]\n    return doc\n\nnlp.post_init(\n    edsnlp.data.from_pandas(\n        synonyms_df,\n        converter=make_doc,\n    )\n)\n</code></pre> <p>Now, you can now use it in a text: <pre><code>doc = nlp.make_doc(\"Aspirin is a drug\")\nspan = doc[0:1]  # \"Aspirin\"\nspan.label_ = \"Drug\"\ndoc.ents = [span]\n\ndoc = nlp(doc)\nprint(doc.ents[0]._.cui)\n# \"B01AC06\"\n</code></pre></p> <p>To use the <code>eds.span_linker</code> component in <code>class</code> mode, we refer to the following repository: deep_multilingual_normalization based on the work of Wajsb\u00fcrt et al., 2021.</p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/#edsnlp.pipes.trainable.span_linker.span_linker.TrainableSpanLinker--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>Spacy vocabulary</p> <p> TYPE: <code>Optional[PipelineProtocol]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'span_linker'</code> </p> <code>embedding</code> <p>The word embedding component</p> <p> TYPE: <code>SpanEmbeddingComponent</code> </p> <code>metric</code> <p>Whether to compute the cosine similarity between the input and output embeddings or the dot product.</p> <p> TYPE: <code>Literal[\"cosine\", \"dot\"] = \"cosine\"</code> DEFAULT: <code>cosine</code> </p> <code>rescale</code> <p>Rescale the output cosine similarities by a constant factor.</p> <p> TYPE: <code>float</code> DEFAULT: <code>20</code> </p> <code>threshold</code> <p>Threshold probability to consider a concept as valid</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>attribute</code> <p>The attribute to store the concept id</p> <p> TYPE: <code>str</code> DEFAULT: <code>cui</code> </p> <code>reference_mode</code> <p>Whether to compare the embeddings with the concepts embeddings (one per concept) or the synonyms embeddings (one per concept per synonym). See above for more details.</p> <p> TYPE: <code>Literal['concept', 'synonym']</code> DEFAULT: <code>concept</code> </p> <code>span_getter</code> <p>How to extract the candidate spans to predict or train on.</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>None</code> </p> <code>context_getter</code> <p>What context to use when computing the span embeddings (defaults to the entity only, so no context)</p> <p> TYPE: <code>Optional[SpanGetterArg]</code> DEFAULT: <code>None</code> </p> <code>probability_mode</code> <p>Whether to compute the probabilities using a softmax or a sigmoid function. This will also determine the loss function to use, either cross-entropy or binary cross-entropy.</p> <p>Subsetting the concepts</p> <p>The probabilities returned in <code>softmax</code> mode depend on the number of concepts (as an extreme cas, if you have only one concept, its softmax probability will always be 1). This is why we recommend using the <code>sigmoid</code> mode in which the probabilities are computed independently for each concept.</p> <p> TYPE: <code>Literal['softmax', 'sigmoid']</code> DEFAULT: <code>sigmoid</code> </p> <code>init_weights</code> <p>Whether to initialize the weights of the component with the embeddings of the entities of the docs provided to the <code>post_init</code> method. How this is done depends on the <code>reference_mode</code> parameter:</p> <ul> <li><code>concept</code>: the embeddings are averaged</li> <li><code>synonym</code>: the embeddings are stored as is</li> </ul> <p>By default, this is set to <code>True</code> if <code>reference_mode</code> is <code>synonym</code>, and <code>False</code> otherwise.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/pipes/trainable/span_linker/span_linker/#edsnlp.pipes.trainable.span_linker.span_linker.TrainableSpanLinker--authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.span_linker</code> component was developed by AP-HP's Data Science team.</p> <p>The deep learning concept-based architecture was adapted from Wajsb\u00fcrt et al., 2021.</p>"},{"location":"reference/edsnlp/processing/","title":"<code>edsnlp.processing</code>","text":""},{"location":"reference/edsnlp/processing/deprecated_pipe/","title":"<code>edsnlp.processing.deprecated_pipe</code>","text":""},{"location":"reference/edsnlp/processing/deprecated_pipe/#edsnlp.processing.deprecated_pipe.slugify","title":"<code>slugify</code>","text":"<p>Slugify a chained attribute name</p>"},{"location":"reference/edsnlp/processing/deprecated_pipe/#edsnlp.processing.deprecated_pipe.slugify--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>chained_attr</code> <p>The string to slugify (replace dots by _)</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The slugified string</p>"},{"location":"reference/edsnlp/processing/deprecated_pipe/#edsnlp.processing.deprecated_pipe.pipe","title":"<code>pipe</code>","text":"<p>Helper to process a pandas, koalas or spark dataframe. This function is deprecated. Prefer using the following instead:</p> <pre><code>import edsnlp\n\ndocs = edsnlp.data.from_***(\n    df,\n    converter='omop',\n    doc_attributes=context,\n)\ndocs = docs.map_pipeline(nlp)\nres = edsnlp.data.to_***(\n    docs,\n    converter='ents',  # or custom extractor\n    span_getter=\"ents\",\n    span_attributes=span_attributes,\n    **kwargs\n)\n</code></pre> <p>You can also call this function to get a migration suggestion.</p>"},{"location":"reference/edsnlp/processing/deprecated_pipe/#edsnlp.processing.deprecated_pipe.pipe--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>df</code> <p>The dataframe to process, can be a pandas, spark or koalas dataframe</p> <p> TYPE: <code>Union[DataFrame, DataFrame, DataFrame]</code> </p> <code>nlp</code> <p>The pipeline to use</p> <p> TYPE: <code>PipelineProtocol</code> </p> <code>n_jobs</code> <p>Number of CPU workers to use</p> <p> TYPE: <code>int</code> DEFAULT: <code>-2</code> </p> <code>context</code> <p>List of context attributes to keep</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>[]</code> </p> <code>results_extractor</code> <p>Function to extract results from the pipeline. Defaults to one row per entities.</p> <p> TYPE: <code>Optional[Callable[[Doc], List[Dict[str, Any]]]]</code> DEFAULT: <code>None</code> </p> <code>additional_spans</code> <p>Additional spans groups to keep, defaults to <code>ents</code> (doc.ents)</p> <p> TYPE: <code>SpanGetterArg</code> DEFAULT: <code>[]</code> </p> <code>extensions</code> <p>Span extensions to export as a column. Can be a list of extension names, a dict of extension names to types, or a string</p> <p> TYPE: <code>ExtensionSchema</code> DEFAULT: <code>[]</code> </p> <code>dtypes</code> <p>Spark schema to use for the output dataframe. This is only used if the input dataframe is a spark dataframe.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>edsnlp.data.to_*</code> function</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Union[DataFrame, DataFrame, DataFrame]</code> <p>The processed dataframe</p>"},{"location":"reference/edsnlp/processing/multiprocessing/","title":"<code>edsnlp.processing.multiprocessing</code>","text":""},{"location":"reference/edsnlp/processing/multiprocessing/#edsnlp.processing.multiprocessing.ForkingPickler","title":"<code>ForkingPickler</code>","text":"<p>           Bases: <code>Pickler</code></p> <p>ForkingPickler that uses dill instead of pickle to transfer objects between processes.</p>"},{"location":"reference/edsnlp/processing/multiprocessing/#edsnlp.processing.multiprocessing.ForkingPickler.register","title":"<code>register</code>  <code>classmethod</code>","text":"<p>Register a reduce function for a type.</p>"},{"location":"reference/edsnlp/processing/multiprocessing/#edsnlp.processing.multiprocessing.MultiprocessingStreamExecutor","title":"<code>MultiprocessingStreamExecutor</code>","text":""},{"location":"reference/edsnlp/processing/multiprocessing/#edsnlp.processing.multiprocessing.MultiprocessingStreamExecutor.feed_queue","title":"<code>feed_queue</code>","text":"<p>Enqueue items in a queue. Note that a queue may be shared between multiple workers, so have to send items destined multiple workers in the same queue. For that, we first determine which worker should receive the item based on the item index and some other env variables. Then we lookup the worker queue, and if it matches the current queue, we send the item, even if all workers share the same queue, in which case there is only on queue feeder thread that sends all the items (non-deterministic mode).</p>"},{"location":"reference/edsnlp/processing/multiprocessing/#edsnlp.processing.multiprocessing.MultiprocessingStreamExecutor.feed_queue--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>queue</code> <p>The queue to feed</p> <p> </p> <code>items</code> <p>The items to send. Note that this iterator is a tee of the main iterator, such that each worker can process items at its own pace.</p> <p> </p>"},{"location":"reference/edsnlp/processing/multiprocessing/#edsnlp.processing.multiprocessing.replace_pickler","title":"<code>replace_pickler</code>","text":"<p>Replace the default pickler used by multiprocessing with dill. \"multiprocess\" didn't work for obscure reasons (maybe the reducers / dispatchers are not propagated between multiprocessing and multiprocess =&gt; torch specific reducers might be missing ?), so this patches multiprocessing directly. directly.</p> <p>For some reason I do not explain, this has a massive impact on the performance of the multiprocessing backend. With the original pickler, the performance can be up to 2x slower than with our custom one.</p>"},{"location":"reference/edsnlp/processing/multiprocessing/#edsnlp.processing.multiprocessing.cpu_count","title":"<code>cpu_count</code>","text":"<p>Heavily inspired (partially copied) from joblib's loky (https://github.com/joblib/loky/blob/2c21e/loky/backend/context.py#L83) by Thomas Moreau and Olivier Grisel.</p> <p>Return the number of CPUs we can use to process data in parallel.</p> <p>The returned number of CPUs returns the minimum of:  * <code>os.cpu_count()</code>  * the CPU affinity settings  * cgroup CPU bandwidth limit (share of total CPU time allowed in a given job)    typically used in containerized environments like Docker</p> <p>Note that on Windows, the returned number of CPUs cannot exceed 61 (or 60 for Python &lt; 3.10), see: https://bugs.python.org/issue26903.</p> <p>It is also always larger or equal to 1.</p>"},{"location":"reference/edsnlp/processing/multiprocessing/#edsnlp.processing.multiprocessing.get_dispatch_schedule","title":"<code>get_dispatch_schedule</code>","text":"<p>To which consumer should a given worker/producer dispatch its data to. This function returns a list of consumers over a period to be determined by the function.</p> <p>This is actually a fun problem, because we want: - to distribute the data evenly between consumers - minimize the number of distinct unique producers sending data to the consumer   (because we move the tensors to the GPU inside the producer, which    creates takes a bit of VRAM for each producer)</p>"},{"location":"reference/edsnlp/processing/multiprocessing/#edsnlp.processing.multiprocessing.get_dispatch_schedule--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>producer_idx</code> <p>Index of the CPU worker</p> <p> TYPE: <code>U</code> </p> <code>producers</code> <p>Producers, ie workers</p> <p> TYPE: <code>Sequence[U]</code> </p> <code>consumers</code> <p>Consumers, ie devices</p> <p> TYPE: <code>Sequence[T]</code> </p> RETURNS DESCRIPTION <code>List[T]</code>"},{"location":"reference/edsnlp/processing/multiprocessing/#edsnlp.processing.multiprocessing.execute_multiprocessing_backend","title":"<code>execute_multiprocessing_backend</code>","text":"<p>If you have multiple CPU cores, and optionally multiple GPUs, we provide the <code>multiprocessing</code> backend that allows to run the inference on multiple processes.</p> <p>This accelerator dispatches the batches between multiple workers (data-parallelism), and distribute the computation of a given batch on one or two workers (model-parallelism):</p> <ul> <li>a <code>CPUWorker</code> which handles the non deep-learning components and the   preprocessing, collating and postprocessing of deep-learning components</li> <li>a <code>GPUWorker</code> which handles the forward call of the deep-learning components</li> </ul> <p>If no GPU is available, no <code>GPUWorker</code> is started, and the <code>CPUWorkers</code> handle the forward call of the deep-learning components as well.</p> <p>The advantage of dedicating a worker to the deep-learning components is that it allows to prepare multiple batches in parallel in multiple <code>CPUWorker</code>, and ensure that the <code>GPUWorker</code> never wait for a batch to be ready.</p> <p>The overall architecture described in the following figure, for 3 CPU workers and 2 GPU workers.</p> <p>Here is how a small pipeline with rule-based components and deep-learning components is distributed between the workers:</p>"},{"location":"reference/edsnlp/processing/simple/","title":"<code>edsnlp.processing.simple</code>","text":""},{"location":"reference/edsnlp/processing/simple/#edsnlp.processing.simple.execute_simple_backend","title":"<code>execute_simple_backend</code>","text":"<p>This is the default execution mode which batches the documents and processes each batch on the current process in a sequential manner.</p>"},{"location":"reference/edsnlp/processing/spark/","title":"<code>edsnlp.processing.spark</code>","text":""},{"location":"reference/edsnlp/processing/spark/#edsnlp.processing.spark.execute_spark_backend","title":"<code>execute_spark_backend</code>","text":"<p>This execution mode uses Spark to parallelize the processing of the documents. The documents are first stored in a Spark DataFrame (if it was not already the case) and then processed in parallel using Spark.</p> <p>Beware, if the original reader was not a SparkReader (<code>edsnlp.data.from_spark</code>), the local docs \u2192 spark dataframe conversion might take some time, and the whole process might be slower than using the <code>multiprocessing</code> backend.</p>"},{"location":"reference/edsnlp/reducers/","title":"<code>edsnlp.reducers</code>","text":""},{"location":"reference/edsnlp/train/","title":"<code>edsnlp.train</code>","text":""},{"location":"reference/edsnlp/training/","title":"<code>edsnlp.training</code>","text":""},{"location":"reference/edsnlp/training/loggers/","title":"<code>edsnlp.training.loggers</code>","text":""},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.CSVLogger","title":"<code>CSVLogger</code>","text":"<p>           Bases: <code>GeneralTracker</code></p> <p>A simple CSV-based logger that writes logs to a CSV file. By default, with <code>edsnlp.train</code> the CSV file is located under a local directory <code>${CWD}/artifact/metrics.csv</code>.</p> <p>Consistent Keys</p> <p>This logger expects that the <code>values</code> dictionary passed to <code>log</code> has consistent keys across all calls. If a new key is encountered in a subsequent call, it will be ignored and a warning will be issued.</p>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.CSVLogger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>logging_dir</code> <p>Directory in which to store the CSV.</p> <p> TYPE: <code>str or PathLike</code> </p> <code>file_name</code> <p>Name of the CSV file. Defaults to \"metrics.csv\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'metrics.csv'</code> </p>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.CSVLogger.log","title":"<code>log</code>","text":"<p>Logs <code>values</code> to the CSV file, at an optional <code>step</code>.</p> <ul> <li>If it's the first call, the columns are inferred from the keys in <code>values</code>   plus a \"step\" column if the user provides <code>step</code>.</li> <li>All subsequent calls must use the same columns. Any missing columns get   written as empty, any new columns generate a warning.</li> </ul>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.JSONLogger","title":"<code>JSONLogger</code>","text":"<p>           Bases: <code>GeneralTracker</code></p> <p>A simple JSON-based logger that writes logs to a JSON file as a list of dictionaries. By default, with <code>edsnlp.train</code> the JSON file is located under a local directory <code>${CWD}/artifact/metrics.json</code>.</p> <p>This method is not recommended for large and frequent logging, as it re-writes the entire JSON file on every call. Prefer <code>CSVLogger</code> for frequent and heavy logging.</p>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.JSONLogger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>logging_dir</code> <p>Directory in which to store the JSON file.</p> <p> TYPE: <code>str or PathLike</code> </p> <code>file_name</code> <p>Name of the JSON file. Defaults to \"metrics.json\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'metrics.json'</code> </p>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.JSONLogger.log","title":"<code>log</code>","text":"<p>Logs <code>values</code> along with a <code>step</code> (if provided).</p> <p>On every call, we:   1. Append a new record to our in-memory list.   2. Write out the entire JSON file containing all records.</p>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.RichLogger","title":"<code>RichLogger</code>","text":"<p>           Bases: <code>GeneralTracker</code></p> <p>A logger that displays logs in a Rich-based table using rich-logger. This logger is also available via the loggers registry as <code>rich</code>.</p> <p>No Disk Logging</p> <p>This logger doesn't save logs to disk. It's meant for displaying logs in a pretty table during training. If you need to save logs to disk, consider combining this logger with any other logger.</p>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.RichLogger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>fields</code> <p>Field descriptors containing goal (\"lower_is_better\" or \"higher_is_better\"),  format and display name The key is a regex that will be used to match the fields to log Each entry of the dictionary should match the following scheme:</p> <ul> <li>key: a regex to match columns</li> <li>value: either a Dict or False to hide the column, the dict format is<ul> <li>name: the name of the column</li> <li>goal: \"lower_is_better\" or \"higher_is_better\"</li> </ul> </li> </ul> <p>This defaults to a set of metrics and stats that are commonly logged during EDS-NLP training.</p> <p> TYPE: <code>Dict[str, Union[Dict, bool]]</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>Key to group the logs</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>hijack_tqdm</code> <p>Whether to replace the tqdm progress bar with a rich progress bar. Indeed, rich progress bars integrate better with the rich table.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.RichLogger.log","title":"<code>log</code>","text":"<p>Logs values in the Rich table. If <code>step</code> is provided, we include it in the logged data.</p>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.RichLogger.finish","title":"<code>finish</code>","text":"<p>Finalize the table (e.g., stop rendering).</p>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.TensorBoardLogger","title":"<code>TensorBoardLogger</code>","text":"<p>           Bases: <code>TensorBoardTracker</code></p> <p>Logger for TensorBoard. This logger is also available via the loggers registry as <code>tensorboard</code>.</p>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.TensorBoardLogger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>project_name</code> <p>Name of the project.</p> <p> TYPE: <code>str</code> </p> <code>logging_dir</code> <p>Directory in which to store the TensorBoard logs. Logs of different runs will be stored in <code>logging_dir/project_name</code>. The environment variable <code>TENSORBOARD_LOGGING_DIR</code> takes precedence over this argument.</p> <p> TYPE: <code>Optional[Union[str, PathLike]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to <code>tensorboard.SummaryWriter</code>.</p> <p> </p>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.AimLogger","title":"<code>AimLogger</code>","text":"<p>           Bases: <code>AimTracker</code></p> <p>Logger for Aim.</p>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.AimLogger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>project_name</code> <p>Name of the project.</p> <p> TYPE: <code>str</code> </p> <code>logging_dir</code> <p>Directory in which to store the Aim logs. The environment variable <code>AIM_LOGGING_DIR</code> takes precedence over this argument.</p> <p> TYPE: <code>Optional[Union[str, PathLike]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the Aim init function.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.WandBLogger","title":"<code>WandBLogger</code>","text":"<p>Logger for Weights &amp; Biases. This logger is also available via the loggers registry as <code>wandb</code>.</p>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.WandBLogger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>project_name</code> <p>Name of the project. This will become the <code>project</code> parameter in <code>wandb.init</code>.</p> <p> TYPE: <code>str</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the WandB init function.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>WandBTracker</code>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.MLflowLogger","title":"<code>MLflowLogger</code>","text":"<p>Logger for MLflow. This logger is also available via the loggers registry as <code>mlflow</code>.</p>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.MLflowLogger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>project_name</code> <p>Name of the project. This will become the mlflow experiment name.</p> <p> TYPE: <code>str</code> </p> <code>logging_dir</code> <p>Directory in which to store the MLflow logs.</p> <p> TYPE: <code>Optional[Union[str, PathLike]]</code> DEFAULT: <code>None</code> </p> <code>run_id</code> <p>If specified, get the run with the specified UUID and log parameters and metrics under that run. The run\u2019s end time is unset and its status is set to running, but the run\u2019s other attributes (source_version, source_type, etc.) are not changed. Environment variable MLFLOW_RUN_ID has priority over this argument.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>An optional <code>dict</code> of <code>str</code> keys and values, or a <code>str</code> dump from a <code>dict</code>, to set as tags on the run. If a run is being resumed, these tags are set on the resumed run. If a new run is being created, these tags are set on the new run. Environment variable MLFLOW_TAGS has priority over this argument.</p> <p> TYPE: <code>Optional[Union[Dict[str, Any], str]]</code> DEFAULT: <code>None</code> </p> <code>nested_run</code> <p>Controls whether run is nested in parent run. True creates a nested run. Environment variable MLFLOW_NESTED_RUN has priority over this argument.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> <code>run_name</code> <p>Name of new run (stored as a mlflow.runName tag). Used only when <code>run_id</code> is unspecified.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>description</code> <p>An optional string that populates the description box of the run. If a run is being resumed, the description is set on the resumed run. If a new run is being created, the description is set on the new run.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>MLflowTracker</code>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.CometMLLogger","title":"<code>CometMLLogger</code>","text":"<p>Logger for CometML. This logger is also available via the loggers registry as <code>cometml</code>.</p>"},{"location":"reference/edsnlp/training/loggers/#edsnlp.training.loggers.CometMLLogger--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>project_name</code> <p>Name of the project.</p> <p> TYPE: <code>str</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the CometML Experiment object.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>CometMLTracker</code>"},{"location":"reference/edsnlp/training/optimizer/","title":"<code>edsnlp.training.optimizer</code>","text":""},{"location":"reference/edsnlp/training/optimizer/#edsnlp.training.optimizer.LinearSchedule","title":"<code>LinearSchedule</code>","text":"<p>           Bases: <code>Schedule</code></p> <p>Linear schedule for a parameter group. The schedule will linearly increase the value from <code>start_value</code> to <code>max_value</code> in the first <code>warmup_rate</code> of the <code>total_steps</code> and then linearly decrease it to <code>end_value</code>.</p>"},{"location":"reference/edsnlp/training/optimizer/#edsnlp.training.optimizer.LinearSchedule--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>total_steps</code> <p>The total number of steps, usually used to calculate ratios.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>max_value</code> <p>The maximum value to reach.</p> <p> TYPE: <code>Optional[Any]</code> DEFAULT: <code>None</code> </p> <code>start_value</code> <p>The initial value.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>path</code> <p>The path to the attribute to set.</p> <p> TYPE: <code>Optional[Union[str, int, List[Union[str, int]]]]</code> DEFAULT: <code>None</code> </p> <code>warmup_rate</code> <p>The rate of the warmup.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>end_value</code> <p>The final value to reach after the decay phase. Defaults to 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p>"},{"location":"reference/edsnlp/training/optimizer/#edsnlp.training.optimizer.ScheduledOptimizer","title":"<code>ScheduledOptimizer</code>","text":"<p>           Bases: <code>Optimizer</code></p> <p>Wrapper optimizer that supports schedules for the parameters and easy parameter selection using the key of the <code>groups</code> dictionary as regex patterns to match the parameter names.</p> <p>Schedules are defined directly in the groups, in place of the scheduled value.</p>"},{"location":"reference/edsnlp/training/optimizer/#edsnlp.training.optimizer.ScheduledOptimizer--examples","title":"Examples","text":"<pre><code>optim = ScheduledOptimizer(\n    cls=\"adamw\",\n    module=model,\n    groups=[\n        # Exclude all parameters matching 'bias' from optimization.\n        {\n            \"selector\": \"bias\",\n            \"exclude\": True,\n        },\n        # Parameters of the NER module's embedding receive this learning rate\n        # schedule. If a parameter matches both 'transformer' and 'ner',\n        # the first group settings take precedence due to the order.\n        {\n            \"selector\": \"^ner[.]embedding\"\n            \"lr\": {\n                \"@schedules\": \"linear\",\n                \"start_value\": 0.0,\n                \"max_value\": 5e-4,\n                \"warmup_rate\": 0.2,\n            },\n        },\n        # Parameters starting with 'ner' receive this learning rate schedule,\n        # unless a 'lr' value has already been set by an earlier selector.\n        {\n            \"selector\": \"^ner\"\n            \"lr\": {\n                \"@schedules\": \"linear\",\n                \"start_value\": 0.0,\n                \"max_value\": 1e-4,\n                \"warmup_rate\": 0.2,\n            },\n        },\n        # Apply a weight_decay of 0.01 to all parameters not excluded.\n        # This setting doesn't conflict with others and applies to all.\n        {\n            \"selector\": \"\",\n            \"weight_decay\": 0.01,\n        },\n    ],\n    total_steps=1000,\n)\n</code></pre>"},{"location":"reference/edsnlp/training/optimizer/#edsnlp.training.optimizer.ScheduledOptimizer--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>optim</code> <p>The optimizer to use. If a string (like \"adamw\") or a type to instantiate, the <code>module</code> and <code>groups</code> must be provided.</p> <p> TYPE: <code>Union[str, Type[Optimizer], Optimizer]</code> </p> <code>module</code> <p>The module to optimize. Usually the <code>nlp</code> pipeline object.</p> <p> TYPE: <code>Optional[Union[PipelineProtocol, Module]]</code> DEFAULT: <code>None</code> </p> <code>total_steps</code> <p>The total number of steps, used for schedules.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>groups</code> <p>The groups to optimize. Each group is a dictionary containing:</p> <ul> <li>a regex <code>selector</code> key to match the parameter of that group by their names   (as listed by <code>nlp.named_parameters()</code>)</li> <li>and several other keys that define the optimizer parameters for that   group, such as <code>lr</code>, <code>weight_decay</code> etc. The value for these keys can   be a <code>Schedule</code> instance or a simple value</li> <li>an <code>exclude</code> key that can be set to True to exclude parameters</li> </ul> <p>The matching is performed by running <code>regex.search(selector, name)</code> so you do not have to match the full name. Note that the order of the groups matters. If a parameter name matches multiple selectors, the configurations of these selectors are combined in reverse order (from the last matched selector to the first), allowing later selectors to complete options from earlier ones. If a selector contains <code>exclude=True</code>, any parameter matching it is excluded from optimization.</p> <p> TYPE: <code>Optional[List[Group]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/training/trainer/","title":"<code>edsnlp.training.trainer</code>","text":""},{"location":"reference/edsnlp/training/trainer/#edsnlp.training.trainer.GenericScorer","title":"<code>GenericScorer</code>","text":"<p>A scorer to evaluate the model performance on various tasks.</p>"},{"location":"reference/edsnlp/training/trainer/#edsnlp.training.trainer.GenericScorer--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>batch_size</code> <p>The batch size to use for scoring. Can be an int (number of documents) or a string (batching expression like \"2000 words\").</p> <p> TYPE: <code>Union[int, str]</code> DEFAULT: <code>1</code> </p> <code>speed</code> <p>Whether to compute the model speed (words/documents per second)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>autocast</code> <p>Whether to use autocasting for mixed precision during the evaluation, defaults to True.</p> <p> TYPE: <code>Union[bool, Any]</code> DEFAULT: <code>None</code> </p> <code>metrics</code> <p>A keyword arguments mapping of metric names to metrics objects. See the metrics documentation for more info.</p> <p> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edsnlp/training/trainer/#edsnlp.training.trainer.TrainingData","title":"<code>TrainingData</code>","text":"<p>A training data object.</p>"},{"location":"reference/edsnlp/training/trainer/#edsnlp.training.trainer.TrainingData--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>data</code> <p>The stream of documents to train on. The documents will be preprocessed and collated according to the pipeline's components.</p> <p> TYPE: <code>Stream</code> </p> <code>batch_size</code> <p>The batch size. Can be a batching expression like \"2000 words\", an int (number of documents), or a tuple (batch_size, batch_by). The batch_by argument should be a statistic produced by the pipes that will be trained. For instance, the <code>eds.span_pooler</code> component produces a \"spans\" statistic, that can be used to produce batches of no more than 16 spans by setting batch_size to \"16 spans\".</p> <p> TYPE: <code>BatchSizeArg</code> </p> <code>shuffle</code> <p>The shuffle strategy. Can be \"dataset\" to shuffle the entire dataset (this can be memory-intensive for large file based datasets), \"fragment\" to shuffle the fragment-based datasets like parquet files, or a batching expression like \"2000 words\" to shuffle the dataset in chunks of 2000 words.</p> <p> TYPE: <code>Union[str, Literal[False]]</code> </p> <code>sub_batch_size</code> <p>How to split each batch into sub-batches that will be fed to the model independently to accumulate gradients over. To split a batch of 8000 tokens into smaller batches of 1000 tokens each, just set this to \"1000 tokens\".</p> <p>You can also request a number of splits, like \"4 splits\", to split the batch into N parts each close to (but less than) batch_size / N.</p> <p> TYPE: <code>Optional[BatchSizeArg]</code> DEFAULT: <code>None</code> </p> <code>pipe_names</code> <p>The names of the pipes that should be trained on this data. If None, defaults to all trainable pipes.</p> <p> TYPE: <code>Optional[AsList[str]]</code> DEFAULT: <code>None</code> </p> <code>post_init</code> <p>Whether to call the pipeline's post_init method with the data before training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"reference/edsnlp/training/trainer/#edsnlp.training.trainer.train","title":"<code>train</code>","text":"<p>Train a pipeline.</p>"},{"location":"reference/edsnlp/training/trainer/#edsnlp.training.trainer.train--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The pipeline that will be trained in place.</p> <p> TYPE: <code>Pipeline</code> </p> <code>train_data</code> <p>The training data. Can be a single TrainingData object, a dict that will be cast or a list of these objects.</p> <code>TrainingData</code> object/dictionary PARAMETER DESCRIPTION <code>data</code> <p>The stream of documents to train on. The documents will be preprocessed and collated according to the pipeline's components.</p> <p> TYPE: <code>Stream</code> </p> <code>batch_size</code> <p>The batch size. Can be a batching expression like \"2000 words\", an int (number of documents), or a tuple (batch_size, batch_by). The batch_by argument should be a statistic produced by the pipes that will be trained. For instance, the <code>eds.span_pooler</code> component produces a \"spans\" statistic, that can be used to produce batches of no more than 16 spans by setting batch_size to \"16 spans\".</p> <p> TYPE: <code>BatchSizeArg</code> </p> <code>shuffle</code> <p>The shuffle strategy. Can be \"dataset\" to shuffle the entire dataset (this can be memory-intensive for large file based datasets), \"fragment\" to shuffle the fragment-based datasets like parquet files, or a batching expression like \"2000 words\" to shuffle the dataset in chunks of 2000 words.</p> <p> TYPE: <code>Union[str, Literal[False]]</code> </p> <code>sub_batch_size</code> <p>How to split each batch into sub-batches that will be fed to the model independently to accumulate gradients over. To split a batch of 8000 tokens into smaller batches of 1000 tokens each, just set this to \"1000 tokens\".</p> <p>You can also request a number of splits, like \"4 splits\", to split the batch into N parts each close to (but less than) batch_size / N.</p> <p> TYPE: <code>Optional[BatchSizeArg]</code> DEFAULT: <code>None</code> </p> <code>pipe_names</code> <p>The names of the pipes that should be trained on this data. If None, defaults to all trainable pipes.</p> <p> TYPE: <code>Optional[AsList[str]]</code> DEFAULT: <code>None</code> </p> <code>post_init</code> <p>Whether to call the pipeline's post_init method with the data before training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p> TYPE: <code>AsList[TrainingData]</code> </p> <code>val_data</code> <p>The validation data. Can be a single Stream object or a list of Stream.</p> <p> TYPE: <code>AsList[Stream]</code> DEFAULT: <code>[]</code> </p> <code>seed</code> <p>The random seed</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>max_steps</code> <p>The maximum number of training steps</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>optimizer</code> <p>The optimizer. If None, a default optimizer will be used.</p> <code>ScheduledOptimizer</code> object/dictionary PARAMETER DESCRIPTION <code>optim</code> <p>The optimizer to use. If a string (like \"adamw\") or a type to instantiate, the <code>module</code> and <code>groups</code> must be provided.</p> <p> TYPE: <code>Union[str, Type[Optimizer], Optimizer]</code> </p> <code>module</code> <p>The module to optimize. Usually the <code>nlp</code> pipeline object.</p> <p> TYPE: <code>Optional[Union[PipelineProtocol, Module]]</code> DEFAULT: <code>None</code> </p> <code>total_steps</code> <p>The total number of steps, used for schedules.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>groups</code> <p>The groups to optimize. Each group is a dictionary containing:</p> <ul> <li>a regex <code>selector</code> key to match the parameter of that group by their names   (as listed by <code>nlp.named_parameters()</code>)</li> <li>and several other keys that define the optimizer parameters for that   group, such as <code>lr</code>, <code>weight_decay</code> etc. The value for these keys can   be a <code>Schedule</code> instance or a simple value</li> <li>an <code>exclude</code> key that can be set to True to exclude parameters</li> </ul> <p>The matching is performed by running <code>regex.search(selector, name)</code> so you do not have to match the full name. Note that the order of the groups matters. If a parameter name matches multiple selectors, the configurations of these selectors are combined in reverse order (from the last matched selector to the first), allowing later selectors to complete options from earlier ones. If a selector contains <code>exclude=True</code>, any parameter matching it is excluded from optimization.</p> <p> TYPE: <code>Optional[List[Group]]</code> DEFAULT: <code>None</code> </p> <p> TYPE: <code>Union[Draft[ScheduledOptimizer], ScheduledOptimizer, Optimizer]</code> DEFAULT: <code>None</code> </p> <code>validation_interval</code> <p>The number of steps between each evaluation. Defaults to 1/10 of max_steps</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>checkpoint_interval</code> <p>The number of steps between each model save. Defaults to validation_interval</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>grad_max_norm</code> <p>The maximum gradient norm</p> <p> TYPE: <code>float</code> DEFAULT: <code>5.0</code> </p> <code>grad_dev_policy</code> <p>The policy to apply when a gradient spike is detected, ie. when the gradient norm is higher than the mean + std * grad_max_dev. Can be:</p> <ul> <li>\"clip_mean\": clip the gradients to the mean gradient norm</li> <li>\"clip_threshold\": clip the gradients to the mean + std * grad_max_dev</li> <li>\"skip\": skip the step</li> </ul> <p>These do not apply to <code>grad_max_norm</code> that is always enforced when it is not None, since <code>grad_max_norm</code> is not adaptive and would most likely prohibit the model from learning during the early stages of training when gradients are expected to be high.</p> <p> TYPE: <code>Optional[Literal['clip_mean', 'clip_threshold', 'skip']]</code> DEFAULT: <code>None</code> </p> <code>grad_ewm_window</code> <p>Approximately how many steps should we look back to compute the average gradient norm and variance to detect gradient deviation spikes.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>grad_max_dev</code> <p>The threshold to apply to detect gradient spikes. A spike is detected when the value is higher than the mean + variance * threshold.</p> <p> TYPE: <code>float</code> DEFAULT: <code>7.0</code> </p> <code>loss_scales</code> <p>The loss scales for each component (useful for multi-task learning)</p> <p> TYPE: <code>Dict[str, float]</code> DEFAULT: <code>{}</code> </p> <code>scorer</code> <p>How to score the model. Expects a <code>GenericScorer</code> object or a dict containing a mapping of metric names to metric objects.</p> <code>GenericScorer</code> object/dictionary PARAMETER DESCRIPTION <code>batch_size</code> <p>The batch size to use for scoring. Can be an int (number of documents) or a string (batching expression like \"2000 words\").</p> <p> TYPE: <code>Union[int, str]</code> DEFAULT: <code>1</code> </p> <code>speed</code> <p>Whether to compute the model speed (words/documents per second)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>autocast</code> <p>Whether to use autocasting for mixed precision during the evaluation, defaults to True.</p> <p> TYPE: <code>Union[bool, Any]</code> DEFAULT: <code>None</code> </p> <code>metrics</code> <p>A keyword arguments mapping of metric names to metrics objects. See the metrics documentation for more info.</p> <p> DEFAULT: <code>{}</code> </p> <p> TYPE: <code>GenericScorer</code> DEFAULT: <code>GenericScorer()</code> </p> <code>num_workers</code> <p>The number of workers to use for preprocessing the data in parallel. Setting it to 0 means no parallelization : data is processed on the main thread which may induce latency slow down the training. To avoid this, a good practice consist in doing the preprocessing either before training or in parallel in a separate process. Because of how EDS-NLP handles stream multiprocessing, changing this value will affect the order of the documents in the produces batches. A stream [1, 2, 3, 4, 5, 6] split in batches of size 3 will produce:</p> <ul> <li>[1, 2, 3] and [4, 5, 6] with 1 worker</li> <li>[1, 3, 5] and [2, 4, 6] with 2 workers</li> </ul> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>cpu</code> <p>Whether to use force training on CPU. On MacOS, this might be necessary to get around some <code>mps</code> backend issues.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>mixed_precision</code> <p>The mixed precision mode. Can be \"no\", \"fp16\", \"bf16\" or \"fp8\".</p> <p> TYPE: <code>Literal['no', 'fp16', 'bf16', 'fp8']</code> DEFAULT: <code>'no'</code> </p> <code>output_dir</code> <p>The output directory, which will contain a <code>model-last</code> directory with the last model, and a <code>train_metrics.json</code> file with the training metrics and stats.</p> <p> TYPE: <code>Union[Path, str]</code> DEFAULT: <code>Path('artifacts')</code> </p> <code>output_model_dir</code> <p>The directory where to save the model. If None, defaults to <code>output_dir / \"model-last\"</code>.</p> <p> TYPE: <code>Optional[Union[Path, str]]</code> DEFAULT: <code>None</code> </p> <code>save_model</code> <p>Whether to save the model or not. This can be useful if you are only interested in the metrics, but no the model, and want to avoid spending time dumping the model weights to the disk.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>logger</code> <p>The logger to use. Can be a boolean to use the default loggers (rich and json), a list of logger names, or a list of logger objects.</p> <p>You can use huggingface accelerate integrated loggers (<code>tensorboard</code>, <code>wandb</code>, <code>comet_ml</code>, <code>aim</code>, <code>mlflow</code>, <code>clearml</code>, <code>dvclive</code>), or EDS-NLP simple loggers, or a combination of both:</p> <ul> <li><code>csv</code>: logs to a CSV file in <code>output_dir</code> (<code>artifacts/metrics.csv</code>)</li> <li><code>json</code>: logs to a JSON file in <code>output_dir</code> (<code>artifacts/metrics.json</code>)</li> <li><code>rich</code>: logs to a rich table in the terminal</li> </ul> <p> TYPE: <code>Union[bool, AsList[Union[str, GeneralTracker, Draft[GeneralTracker]]]]</code> DEFAULT: <code>True</code> </p> <code>log_weight_grads</code> <p>Whether to log the weight gradients during training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>on_validation_callback</code> <p>A callback function invoked during validation steps to handle custom logic.</p> <p> TYPE: <code>Optional[Callable[[Dict], None]]</code> DEFAULT: <code>None</code> </p> <code>project_name</code> <p>The project name, used to group experiments in some loggers. If None, defaults to the path of the config file, relative to the home directory, with slashes replaced by double underscores.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments.</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Pipeline</code> <p>The trained pipeline</p>"},{"location":"reference/edsnlp/tune/","title":"<code>edsnlp.tune</code>","text":""},{"location":"reference/edsnlp/tune/#edsnlp.tune.HyperparameterConfig","title":"<code>HyperparameterConfig</code>","text":"<p>           Bases: <code>BaseModel</code></p> <p>A configuration model for hyperparameters used in optimization or tuning processes.</p>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.HyperparameterConfig.to_dict","title":"<code>to_dict</code>","text":"<p>Convert the hyperparameter configuration to a dictionary. Excludes unset and default values to provide a minimal representation.</p> <p>Returns:     dict: A dictionary representation of the hyperparameter configuration.</p>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.is_plotly_install","title":"<code>is_plotly_install</code>","text":"<p>Check if Plotly is installed. If not warn the user. Plotly is needed by optuna.visualization to produce tuning visual results.</p> <p>Returns:     bool: True if Plotly is installed, False otherwise.</p>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.compute_time_per_trial","title":"<code>compute_time_per_trial</code>","text":"<p>Compute the time for the first trial or the EMA (Exponential Moving Average) across all trials in the study.</p>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.compute_time_per_trial--parameters","title":"Parameters:","text":"<p>study : optuna.study.Study     An Optuna study object containing past trials. ema : bool     If True, computes the EMA of trial times; otherwise, computes the     time of the first trial. alpha : float, optional     Smoothing factor for EMA. Only used if ema is True.</p>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.compute_time_per_trial--returns","title":"Returns:","text":"<pre><code>float: Time for the first trial or the EMA across all trials, in seconds.\n</code></pre>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.compute_n_trials","title":"<code>compute_n_trials</code>","text":"<p>Estimate the maximum number of trials that can be executed within a given GPU time budget.</p>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.compute_n_trials--parameters","title":"Parameters:","text":"<p>gpu_hours : float     The total amount of GPU time available for tuning, in hours. time_per_trial : float     Time per trial, in seconds.</p>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.compute_n_trials--returns","title":"Returns:","text":"<pre><code>int: The number of trials that can be run within the given GPU time budget.\n</code></pre>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.update_config","title":"<code>update_config</code>","text":"<p>Update a configuration dictionary with tuned hyperparameter values.</p> <p>This function modifies a given configuration dictionary by updating the specified hyperparameters with values from either a dictionary or an Optuna trial object. The updated configuration and training keyword arguments are returned.</p>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.update_config--parameters","title":"Parameters:","text":"<p>config : dict     The configuration dictionary to be updated. tuned_parameters : dict     A dictionary specifying the hyperparameters to tune. values : dict, optional     A dictionary of parameter names and their corresponding values to update     the configuration. Used when <code>trial</code> is not provided. trial : optuna.trial.Trial, optional     An Optuna trial object to sample parameter values.     Used when <code>values</code> is not provided.</p>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.update_config--returns","title":"Returns:","text":"<p>tuple     - kwargs : dict       The resolved training keyword arguments from the updated configuration.     - updated_config : dict       The modified configuration dictionary.</p>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.tune_two_phase","title":"<code>tune_two_phase</code>","text":"<p>Perform two-phase hyperparameter tuning using Optuna.</p> <p>This method executes a two-phase tuning strategy. In the first phase, all specified hyperparameters are tuned. Based on their computed importance, only the most important hyperparameters (top 50% by importance) are selected for fine-tuning in the second phase, while the less important hyperparameters are frozen to their best values from phase one.</p>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.tune_two_phase--parameters","title":"Parameters:","text":"<p>config : dict     The configuration dictionary for the model and training process. hyperparameters : dict     A dictionary specifying the hyperparameters to tune. output_dir : str     Directory where tuning results, visualizations, and best parameters will     be saved. checkpoint_dir : str,     Path to save the checkpoint file. n_trials : int     The total number of trials to execute across both tuning phases.     This number will be split between the two phases, with approximately half     of the trials assigned to each phase. viz : bool     Whether or not to include visual features (False if Plotly is unavailable). metric : Tuple[str]     Metric used to evaluate trials. study : optuna.study.Study, optional     Optuna study containing the first trial that was used to compute <code>n_trials</code>     in case the user specifies a GPU hour budget. is_fixed_trial : bool, optional     Whether or not the user specified fixed <code>n_trials</code> in config.     If not, recompute n_trials between the two phases. In case there was multiples     trials pruned in phase 1, we raise n_trials to compensate. Default is False. gpu_hours : float, optional     Total GPU time available for tuning, in hours. Default is 1 hour. skip_phase_1 : bool, optional     Whether or not to skip phase 1 (in case of resuming from checkpoint).     Default is False.</p>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.compute_remaining_n_trials_possible","title":"<code>compute_remaining_n_trials_possible</code>","text":"<p>Compute the remaining number of trials possible within the GPU time budget that was not used by the study (in cases where multiple trials were pruned).</p>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.compute_remaining_n_trials_possible--parameters","title":"Parameters:","text":"<p>study : optuna.study.Study     An Optuna study object containing past trials. gpu_hours : float     The total amount of GPU time available for tuning, in hours.</p>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.compute_remaining_n_trials_possible--returns","title":"Returns:","text":"<p>int: The remaining number of trials possible.</p>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.tune","title":"<code>tune</code>","text":"<p>Perform hyperparameter tuning for a model using Optuna.</p>"},{"location":"reference/edsnlp/tune/#edsnlp.tune.tune--parameters","title":"Parameters:","text":"<p>config_meta : dict     Metadata for the configuration file, containing at least the key \"config_path\"     which specifies the path to the configuration file. hyperparameters : dict     A dictionary specifying the hyperparameters to tune. The keys are the parameter     names, and the values are dictionaries containing the following fields:     - \"path\": List[str] representing the path to the parameter in <code>config</code>.     - \"type\": The type of parameter (\"float\", \"int\", \"categorical\").     - \"low\": (optional) Lower bound for numerical parameters.     - \"high\": (optional) Upper bound for numerical parameters.     - \"step\": (optional) Step size for numerical parameters.     - \"log\": (optional) Whether to sample numerical parameters on a log scale.     - \"choices\": (optional) List of values for categorical parameters. output_dir : str     Directory where tuning results, visualizations, and best parameters will     be saved. checkpoint_dir : str,     Path to save the checkpoint file. gpu_hours : float, optional     Total GPU time available for tuning, in hours. Default is 1 hour. n_trials : int, optional     Number of trials for tuning. If not provided, it will be computed based on the     <code>gpu_hours</code> and the estimated time per trial. two_phase_tuning : bool, optional     If True, performs two-phase tuning. In the first phase, all hyperparameters     are tuned, and in the second phase, the top half (based on importance) are     fine-tuned while freezing others.     Default is False. seed : int, optional     Random seed for reproducibility. Default is 42. metric : str, optional     Metric used to evaluate trials. Default is \"ner.micro.f\". keep_checkpoint : bool, optional     If True, keeps the checkpoint file after tuning. Default is False.</p>"},{"location":"reference/edsnlp/utils/","title":"<code>edsnlp.utils</code>","text":""},{"location":"reference/edsnlp/utils/batching/","title":"<code>edsnlp.utils.batching</code>","text":""},{"location":"reference/edsnlp/utils/batching/#edsnlp.utils.batching.BatchSizeArg","title":"<code>BatchSizeArg</code>","text":"<p>           Bases: <code>Validated</code></p> <p>Batch size argument validator / caster for confit/pydantic</p>"},{"location":"reference/edsnlp/utils/batching/#edsnlp.utils.batching.BatchSizeArg--examples","title":"Examples","text":"<pre><code>def fn(batch_size: BatchSizeArg):\n    return batch_size\n\n\nprint(fn(\"10 samples\"))\n# Out: (10, \"samples\")\n\nprint(fn(\"10 words\"))\n# Out: (10, \"words\")\n\nprint(fn(10))\n# Out: (10, \"samples\")\n</code></pre>"},{"location":"reference/edsnlp/utils/batching/#edsnlp.utils.batching.batchify","title":"<code>batchify</code>","text":"<p>Yields batch that contain at most <code>batch_size</code> elements. If an item contains more than <code>batch_size</code> elements, it will be yielded as a single batch.</p>"},{"location":"reference/edsnlp/utils/batching/#edsnlp.utils.batching.batchify--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>iterable</code> <p>The iterable to batchify</p> <p> TYPE: <code>Iterable[T]</code> </p> <code>batch_size</code> <p>The maximum number of elements in a batch</p> <p> TYPE: <code>int</code> </p> <code>drop_last</code> <p>Whether to drop the last batch if it is smaller than <code>batch_size</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sentinel_mode</code> <p>How to handle the sentinel values in the iterable:</p> <ul> <li>\"drop\": drop sentinel values</li> <li>\"keep\": keep sentinel values inside the produced batches</li> <li>\"split\": split batches at sentinel values and yield sentinel values     separately</li> </ul> <p> TYPE: <code>Literal['drop', 'keep', 'split']</code> DEFAULT: <code>'drop'</code> </p>"},{"location":"reference/edsnlp/utils/batching/#edsnlp.utils.batching.batchify_by_length_sum","title":"<code>batchify_by_length_sum</code>","text":"<p>Yields batch that contain at most <code>batch_size</code> words. If an item contains more than <code>batch_size</code> words, it will be yielded as a single batch.</p>"},{"location":"reference/edsnlp/utils/batching/#edsnlp.utils.batching.batchify_by_length_sum--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>iterable</code> <p>The iterable to batchify</p> <p> TYPE: <code>Iterable[T]</code> </p> <code>batch_size</code> <p>The maximum number of words in a batch</p> <p> TYPE: <code>int</code> </p> <code>drop_last</code> <p>Whether to drop the last batch if it is smaller than <code>batch_size</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sentinel_mode</code> <p>How to handle the sentinel values in the iterable:</p> <ul> <li>\"drop\": drop sentinel values</li> <li>\"keep\": keep sentinel values inside the produced batches</li> <li>\"split\": split batches at sentinel values and yield sentinel values     separately</li> </ul> <p> TYPE: <code>Literal['drop', 'keep', 'split']</code> DEFAULT: <code>'drop'</code> </p> RETURNS DESCRIPTION <code>Iterable[List[T]]</code>"},{"location":"reference/edsnlp/utils/batching/#edsnlp.utils.batching.batchify_by_padded","title":"<code>batchify_by_padded</code>","text":"<p>Yields batch that contain at most <code>batch_size</code> padded words, ie the number of total words if all items were padded to the length of the longest item.</p>"},{"location":"reference/edsnlp/utils/batching/#edsnlp.utils.batching.batchify_by_padded--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>iterable</code> <p>The iterable to batchify</p> <p> TYPE: <code>Iterable[T]</code> </p> <code>batch_size</code> <p>The maximum number of padded words in a batch</p> <p> TYPE: <code>int</code> </p> <code>drop_last</code> <p>Whether to drop the last batch if it is smaller than <code>batch_size</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sentinel_mode</code> <p>How to handle the sentinel values in the iterable:</p> <ul> <li>\"drop\": drop sentinel values</li> <li>\"keep\": keep sentinel values inside the produced batches</li> <li>\"split\": split batches at sentinel values and yield sentinel values     separately</li> </ul> <p> TYPE: <code>Literal['drop', 'keep', 'split']</code> DEFAULT: <code>'drop'</code> </p> RETURNS DESCRIPTION <code>Iterable[List[T]]</code>"},{"location":"reference/edsnlp/utils/batching/#edsnlp.utils.batching.batchify_by_dataset","title":"<code>batchify_by_dataset</code>","text":"<p>Yields batch that contain at most <code>batch_size</code> datasets. If an item contains more than <code>batch_size</code> datasets, it will be yielded as a single batch.</p>"},{"location":"reference/edsnlp/utils/batching/#edsnlp.utils.batching.batchify_by_dataset--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>iterable</code> <p>The iterable to batchify</p> <p> TYPE: <code>Iterable[T]</code> </p> <code>batch_size</code> <p>Unused, always 1 full dataset per batch</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>drop_last</code> <p>Whether to drop the last batch if it is smaller than <code>batch_size</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sentinel_mode</code> <p>How to handle the sentinel values in the iterable:</p> <ul> <li>\"drop\": drop sentinel values</li> <li>\"keep\": keep sentinel values inside the produced batches</li> <li>\"split\": split batches at sentinel values and yield sentinel values     separately</li> </ul> <p> TYPE: <code>Literal['drop', 'keep', 'split']</code> DEFAULT: <code>'drop'</code> </p> RETURNS DESCRIPTION <code>Iterable[List[T]]</code>"},{"location":"reference/edsnlp/utils/batching/#edsnlp.utils.batching.batchify_by_fragment","title":"<code>batchify_by_fragment</code>","text":"<p>Yields batch that contain at most <code>batch_size</code> fragments. If an item contains more than <code>batch_size</code> fragments, it will be yielded as a single batch.</p>"},{"location":"reference/edsnlp/utils/batching/#edsnlp.utils.batching.batchify_by_fragment--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>iterable</code> <p>The iterable to batchify</p> <p> TYPE: <code>Iterable[T]</code> </p> <code>batch_size</code> <p>Unused, always 1 full fragment per batch</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>drop_last</code> <p>Whether to drop the last batch if it is smaller than <code>batch_size</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sentinel_mode</code> <p>How to handle the sentinel values in the iterable:</p> <ul> <li>\"drop\": drop sentinel values</li> <li>\"keep\": keep sentinel values inside the produced batches</li> <li>\"split\": split batches at sentinel values and yield sentinel values     separately</li> </ul> <p> TYPE: <code>Literal['drop', 'keep', 'split']</code> DEFAULT: <code>'drop'</code> </p> RETURNS DESCRIPTION <code>Iterable[List[T]]</code>"},{"location":"reference/edsnlp/utils/batching/#edsnlp.utils.batching.stat_batchify","title":"<code>stat_batchify</code>","text":"<p>Create a batching function that uses the value of a specific key in the items to determine the batch size. This function is primarily meant to be used on the flattened outputs of the <code>preprocess</code> method of a Pipeline object.</p> <p>It expects the items to be a dictionary in which some keys contain the string \"/stats/\" and the <code>key</code> pattern. For instance:</p> <pre><code>from edsnlp.utils.batching import stat_batchify\n\nitems = [\n    {\"text\": \"first sample\", \"obj/stats/words\": 2, \"obj/stats/chars\": 12},\n    {\"text\": \"dos\", \"obj/stats/words\": 1, \"obj/stats/chars\": 3},\n    {\"text\": \"third one !\", \"obj/stats/words\": 3, \"obj/stats/chars\": 11},\n]\nbatcher = stat_batchify(\"words\")\nassert list(batcher(items, 4)) == [\n    [\n        {\"text\": \"first sample\", \"obj/stats/words\": 2, \"obj/stats/chars\": 12},\n        {\"text\": \"dos\", \"obj/stats/words\": 1, \"obj/stats/chars\": 3},\n    ],\n    [\n        {\"text\": \"third one !\", \"obj/stats/words\": 3, \"obj/stats/chars\": 11},\n    ],\n]\n</code></pre>"},{"location":"reference/edsnlp/utils/batching/#edsnlp.utils.batching.stat_batchify--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>key</code> <p>The key pattern to use to determine the actual key to look up in the items.</p> <p> </p> RETURNS DESCRIPTION <code>Callable[[Iterable, int, bool, Literal[\"drop\", \"split\"]], Iterable</code>"},{"location":"reference/edsnlp/utils/bindings/","title":"<code>edsnlp.utils.bindings</code>","text":""},{"location":"reference/edsnlp/utils/bindings/#edsnlp.utils.bindings.AttributesArg","title":"<code>AttributesArg</code>","text":"<p>           Bases: <code>Validated</code></p> <p>Valid values for the <code>attributes</code> argument of a component can be :</p> <ul> <li>a (span) -&gt; attribute callable</li> <li>a attribute name (\"_.negated\")</li> <li>a list of attribute names ([\".negated\", \".event\"])</li> <li>a dict of attribute name to True or list of labels, to filter the attributes</li> </ul>"},{"location":"reference/edsnlp/utils/bindings/#edsnlp.utils.bindings.AttributesArg--examples","title":"Examples","text":"<ul> <li><code>attributes=\"_.negated\"</code> will use the <code>negated</code> extention of the span</li> <li><code>attributes=[\"_.negated\", \"_.past\"]</code> will use the <code>negated</code> and <code>past</code>    extensions of the span</li> <li><code>attributes={\"_.negated\": True, \"_.past\": \"DATE\"}</code> will use the <code>negated</code>    extension of any span, and the <code>past</code> extension of spans with the <code>DATE</code> label</li> </ul>"},{"location":"reference/edsnlp/utils/bindings/#edsnlp.utils.bindings.make_binding_getter","title":"<code>make_binding_getter</code>","text":"<p>Make a attribute getter</p>"},{"location":"reference/edsnlp/utils/bindings/#edsnlp.utils.bindings.make_binding_getter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>attribute</code> <p>Either one of the following: - a path to a nested attributes of the span, such as \"attribute_\" or \"_.negated\" - a tuple of (key, value) equality, such as <code>(\"_.date.mode\", \"PASSED\")</code></p> <p> TYPE: <code>Union[str, Binding]</code> </p> RETURNS DESCRIPTION <code>Callable[[Span], bool]</code> <p>The attribute getter</p>"},{"location":"reference/edsnlp/utils/bindings/#edsnlp.utils.bindings.make_binding_setter","title":"<code>make_binding_setter</code>","text":"<p>Make a attribute setter</p>"},{"location":"reference/edsnlp/utils/bindings/#edsnlp.utils.bindings.make_binding_setter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>binding</code> <p>A pair of - a path to a nested attributes of the span, such as <code>attribute_</code> or <code>_.negated</code> - a value assignment</p> <p> TYPE: <code>Binding</code> </p> RETURNS DESCRIPTION <code>Callable[[Span]]</code> <p>The attribute setter</p>"},{"location":"reference/edsnlp/utils/collections/","title":"<code>edsnlp.utils.collections</code>","text":""},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.batch_compress_dict","title":"<code>batch_compress_dict</code>","text":"<p>Compress a sequence of dictionaries in which values that occur multiple times are deduplicated. The corresponding keys will be merged into a single string using the \"|\" character as a separator. This is useful to preserve referential identities when decompressing the dictionary after it has been serialized and deserialized.</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.batch_compress_dict--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>seq</code> <p>Sequence of dictionaries to compress</p> <p> TYPE: <code>Optional[Iterable[Dict[str, Any]]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.multi_tee","title":"<code>multi_tee</code>","text":"<p>Makes copies of an iterable such that every iteration over it starts from 0. If the iterable is a sequence (list, tuple), just returns it since every iter() over the object restart from the beginning</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.FrozenDict","title":"<code>FrozenDict</code>","text":"<p>           Bases: <code>dict</code></p> <p>Copied from <code>spacy.util.SimpleFrozenDict</code> to ensure compatibility.</p> <p>Initialize the frozen dict. Can be initialized with pre-defined values.</p> <p>error (str): The error message when user tries to assign to dict.</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.FrozenList","title":"<code>FrozenList</code>","text":"<p>           Bases: <code>list</code></p> <p>Copied from <code>spacy.util.SimpleFrozenDict</code> to ensure compatibility</p> <p>Initialize the frozen list.</p> <p>error (str): The error message when user tries to mutate the list.</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.ld_to_dl","title":"<code>ld_to_dl</code>","text":"<p>Convert a list of dictionaries to a dictionary of lists</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.ld_to_dl--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>ld</code> <p>The list of dictionaries</p> <p> TYPE: <code>Iterable[Mapping[str, T]]</code> </p> RETURNS DESCRIPTION <code>Dict[str, List[T]]</code> <p>The dictionary of lists</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.dl_to_ld","title":"<code>dl_to_ld</code>","text":"<p>Convert a dictionary of lists to a list of dictionaries</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.dl_to_ld--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>dl</code> <p>The dictionary of lists</p> <p> TYPE: <code>Mapping[str, Sequence[Any]]</code> </p> RETURNS DESCRIPTION <code>List[Dict[str, Any]]</code> <p>The list of dictionaries</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.decompress_dict","title":"<code>decompress_dict</code>","text":"<p>Decompress a dictionary of lists into a sequence of dictionaries. This function assumes that the dictionary structure was obtained using the <code>batch_compress_dict</code> class. Keys that were merged into a single string using the \"|\" character as a separator will be split into a nested dictionary structure.</p>"},{"location":"reference/edsnlp/utils/collections/#edsnlp.utils.collections.decompress_dict--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>seq</code> <p>The dictionary to decompress or a sequence of dictionaries to decompress</p> <p> TYPE: <code>Union[Iterable[Dict[str, Any]], Dict[str, Any]]</code> </p>"},{"location":"reference/edsnlp/utils/deprecation/","title":"<code>edsnlp.utils.deprecation</code>","text":""},{"location":"reference/edsnlp/utils/doc_to_text/","title":"<code>edsnlp.utils.doc_to_text</code>","text":""},{"location":"reference/edsnlp/utils/doc_to_text/#edsnlp.utils.doc_to_text.aggregate_tokens","title":"<code>aggregate_tokens</code>  <code>cached</code>","text":"<p>Aggregate tokens strings, computed from their <code>attr</code> attribute, into a single string, possibly ignoring excluded tokens (like pollution tokens) and/or space tokens. This also returns the start and end offsets of each token in the aggregated string, as well as a bytes array indicating which tokens were kept. The reason for the bytes array is that it is faster to index, and allows reverse indexing as well.</p>"},{"location":"reference/edsnlp/utils/doc_to_text/#edsnlp.utils.doc_to_text.aggregate_tokens--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> <code>attr</code> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Tuple[str, List[int], List[int], bytes]</code> <p>The aggregated text, the start offsets, the end offsets, and the bytes array indicating which tokens were kept.</p>"},{"location":"reference/edsnlp/utils/doc_to_text/#edsnlp.utils.doc_to_text.get_text","title":"<code>get_text</code>","text":"<p>Get text using a custom attribute, possibly ignoring excluded tokens.</p>"},{"location":"reference/edsnlp/utils/doc_to_text/#edsnlp.utils.doc_to_text.get_text--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p>Doc or Span to get text from.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>attr</code> <p>Attribute to use.</p> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens, by default False</p> <p> TYPE: <code>bool</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens, by default False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Extracted text.</p>"},{"location":"reference/edsnlp/utils/doc_to_text/#edsnlp.utils.doc_to_text.get_char_offsets","title":"<code>get_char_offsets</code>","text":"<p>Get char offsets of the doc tokens in the \"cleaned\" text.</p>"},{"location":"reference/edsnlp/utils/doc_to_text/#edsnlp.utils.doc_to_text.get_char_offsets--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p>Doc or Span to get text from.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>attr</code> <p>Attribute to use.</p> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens, by default False</p> <p> TYPE: <code>bool</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens, by default False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Tuple[List[int], List[int]]</code> <p>An alignment tuple: clean start/end offsets lists.</p>"},{"location":"reference/edsnlp/utils/examples/","title":"<code>edsnlp.utils.examples</code>","text":""},{"location":"reference/edsnlp/utils/examples/#edsnlp.utils.examples.find_matches","title":"<code>find_matches</code>","text":"<p>Finds entities within the example.</p>"},{"location":"reference/edsnlp/utils/examples/#edsnlp.utils.examples.find_matches--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>example</code> <p>Example to process.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>List[Match]</code> <p>List of matches for entities.</p>"},{"location":"reference/edsnlp/utils/examples/#edsnlp.utils.examples.parse_match","title":"<code>parse_match</code>","text":"<p>Parse a regex match representing an entity.</p>"},{"location":"reference/edsnlp/utils/examples/#edsnlp.utils.examples.parse_match--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>match</code> <p>Match for an entity.</p> <p> TYPE: <code>Match</code> </p> RETURNS DESCRIPTION <code>Match</code> <p>Usable representation for the entity match.</p>"},{"location":"reference/edsnlp/utils/examples/#edsnlp.utils.examples.parse_example","title":"<code>parse_example</code>","text":"<p>Parses an example : finds examples and removes the tags.</p>"},{"location":"reference/edsnlp/utils/examples/#edsnlp.utils.examples.parse_example--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>example</code> <p>Example to process.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Tuple[str, List[Entity]]</code> <p>Cleaned text and extracted entities.</p>"},{"location":"reference/edsnlp/utils/extensions/","title":"<code>edsnlp.utils.extensions</code>","text":""},{"location":"reference/edsnlp/utils/extensions/#edsnlp.utils.extensions.rgetattr","title":"<code>rgetattr</code>","text":"<p>Get attribute recursively</p>"},{"location":"reference/edsnlp/utils/extensions/#edsnlp.utils.extensions.rgetattr--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>obj</code> <p>An object</p> <p> TYPE: <code>Any</code> </p> <code>attr</code> <p>The name of the attribute to get. Can contain dots.</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/edsnlp/utils/file_system/","title":"<code>edsnlp.utils.file_system</code>","text":""},{"location":"reference/edsnlp/utils/filter/","title":"<code>edsnlp.utils.filter</code>","text":""},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.default_sort_key","title":"<code>default_sort_key</code>","text":"<p>Returns the sort key for filtering spans.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.default_sort_key--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span</code> <p>Span to sort.</p> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>key</code> <p>Sort key.</p> <p> TYPE: <code>Tuple(int, int)</code> </p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.start_sort_key","title":"<code>start_sort_key</code>","text":"<p>Returns the sort key for filtering spans by start order.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.start_sort_key--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span</code> <p>Span to sort.</p> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>key</code> <p>Sort key.</p> <p> TYPE: <code>Tuple(int, int)</code> </p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.filter_spans","title":"<code>filter_spans</code>","text":"<p>Re-definition of spacy's filtering function, that returns discarded spans as well as filtered ones.</p> <p>Can also accept a <code>label_to_remove</code> argument, useful for filtering out pseudo cues. If set, <code>results</code> can contain overlapping spans: only spans overlapping with excluded labels are removed. The main expected use case is for pseudo-cues.</p> <p>It can handle an iterable of tuples instead of an iterable of <code>Span</code>s. The primary use-case is the use with the <code>RegexMatcher</code>'s capacity to return the span's <code>groupdict</code>.</p> <p>The spaCy documentation states:</p> <p>Filter a sequence of spans and remove duplicates or overlaps. Useful for creating named entities (where one token can only be part of one entity) or when merging spans with <code>Retokenizer.merge</code>. When spans overlap, the (first) longest span is preferred over shorter spans.</p> <p>Filtering out spans</p> <p>If the <code>label_to_remove</code> argument is supplied, it might be tempting to filter overlapping spans that are not part of a label to remove.</p> <p>The reason we keep all other possibly overlapping labels is that in qualifier pipelines, the same cue can precede and follow a marked entity. Hence we need to keep every example.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.filter_spans--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>spans</code> <p>Spans to filter.</p> <p> TYPE: <code>Iterable[Union[Span, Tuple[Span, Any]]]</code> </p> <code>return_discarded</code> <p>Whether to return discarded spans.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>label_to_remove</code> <p>Label to remove. If set, results can contain overlapping spans.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>sort_key</code> <p>Key to sorting spans before applying overlap conflict resolution. A span with a higher key will have precedence over another span. By default, the largest, leftmost spans are selected first.</p> <p> TYPE: <code>Callable[Span, Any]</code> DEFAULT: <code>default_sort_key</code> </p> RETURNS DESCRIPTION <code>results</code> <p>Filtered spans</p> <p> TYPE: <code>List[Union[Span, Tuple[Span, Any]]]</code> </p> <code>discarded</code> <p>Discarded spans</p> <p> TYPE: <code>(List[Union[Span, Tuple[Span, Any]]], optional)</code> </p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.consume_spans","title":"<code>consume_spans</code>","text":"<p>Consume a list of span, according to a filter.</p> <p>Warning</p> <p>This method makes the hard hypothesis that:</p> <ol> <li>Spans are sorted.</li> <li>Spans are consumed in sequence and only once.</li> </ol> <p>The second item is problematic for the way we treat long entities, hence the <code>second_chance</code> parameter, which lets entities be seen more than once.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.consume_spans--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>spans</code> <p>List of spans to filter</p> <p> TYPE: <code>List of spans</code> </p> <code>filter</code> <p>Filtering function. Should return True when the item is to be included.</p> <p> TYPE: <code>Callable</code> </p> <code>second_chance</code> <p>Optional list of spans to include again (useful for long entities), by default None</p> <p> TYPE: <code>List of spans</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>matches</code> <p>List of spans consumed by the filter.</p> <p> TYPE: <code>List of spans</code> </p> <code>remainder</code> <p>List of remaining spans in the original <code>spans</code> parameter.</p> <p> TYPE: <code>List of spans</code> </p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.get_spans","title":"<code>get_spans</code>","text":"<p>Extracts spans with a given label. Prefer using hash label for performance reasons.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.get_spans--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>spans</code> <p>List of spans to filter.</p> <p> TYPE: <code>List[Span]</code> </p> <code>label</code> <p>Label to filter on.</p> <p> TYPE: <code>Union[int, str]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>Filtered spans.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.span_f1","title":"<code>span_f1</code>","text":"<p>Computes the F1 overlap between two spans.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.span_f1--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>a</code> <p>First span</p> <p> TYPE: <code>Span</code> </p> <code>b</code> <p>Second span</p> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>float</code> <p>F1 overlap</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.align_spans","title":"<code>align_spans</code>","text":"<p>Aligns two lists of spans, by matching source spans that overlap target spans. This function is optimized to avoid quadratic complexity.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.align_spans--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>source_spans</code> <p>List of spans to align.</p> <p> TYPE: <code>List[Span]</code> </p> <code>target_spans</code> <p>List of spans to align.</p> <p> TYPE: <code>List[Span]</code> </p> <code>sort_by_overlap</code> <p>Whether to sort the aligned spans by maximum dice/f1 overlap with the target span.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>List[List[Span]]</code> <p>Subset of <code>source</code> spans for each target span</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.get_span_group","title":"<code>get_span_group</code>","text":"<p>Get the spans of a span group that are contained inside a doclike object.</p>"},{"location":"reference/edsnlp/utils/filter/#edsnlp.utils.filter.get_span_group--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>doclike</code> <p>Doclike object to act as a mask.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>group</code> <p>Group name from which to get the spans.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>List of spans.</p>"},{"location":"reference/edsnlp/utils/inclusion/","title":"<code>edsnlp.utils.inclusion</code>","text":""},{"location":"reference/edsnlp/utils/inclusion/#edsnlp.utils.inclusion.check_inclusion","title":"<code>check_inclusion</code>","text":"<p>Checks whether the span overlaps the boundaries.</p>"},{"location":"reference/edsnlp/utils/inclusion/#edsnlp.utils.inclusion.check_inclusion--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span</code> <p>Span to check.</p> <p> TYPE: <code>Span</code> </p> <code>start</code> <p>Start of the boundary</p> <p> TYPE: <code>int</code> </p> <code>end</code> <p>End of the boundary</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the span overlaps the boundaries.</p>"},{"location":"reference/edsnlp/utils/inclusion/#edsnlp.utils.inclusion.check_sent_inclusion","title":"<code>check_sent_inclusion</code>","text":"<p>Checks whether the span overlaps the boundaries.</p>"},{"location":"reference/edsnlp/utils/inclusion/#edsnlp.utils.inclusion.check_sent_inclusion--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>span</code> <p>Span to check.</p> <p> TYPE: <code>Span</code> </p> <code>start</code> <p>Start of the boundary</p> <p> TYPE: <code>int</code> </p> <code>end</code> <p>End of the boundary</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the span overlaps the boundaries.</p>"},{"location":"reference/edsnlp/utils/lazy_module/","title":"<code>edsnlp.utils.lazy_module</code>","text":""},{"location":"reference/edsnlp/utils/numbers/","title":"<code>edsnlp.utils.numbers</code>","text":""},{"location":"reference/edsnlp/utils/regex/","title":"<code>edsnlp.utils.regex</code>","text":""},{"location":"reference/edsnlp/utils/regex/#edsnlp.utils.regex.make_pattern","title":"<code>make_pattern</code>","text":"<p>Create OR pattern from a list of patterns.</p>"},{"location":"reference/edsnlp/utils/regex/#edsnlp.utils.regex.make_pattern--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to merge.</p> <p> TYPE: <code>List[str]</code> </p> <code>with_breaks</code> <p>Whether to add breaks (<code>\\b</code>) on each side, by default False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>name</code> <p>Name of the group, using regex <code>?P&lt;&gt;</code> directive.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Merged pattern.</p>"},{"location":"reference/edsnlp/utils/regex/#edsnlp.utils.regex.compile_regex","title":"<code>compile_regex</code>","text":"<p>This function tries to compile <code>reg</code>  using the <code>re</code> module, and fallbacks to the <code>regex</code> module that is more permissive.</p>"},{"location":"reference/edsnlp/utils/regex/#edsnlp.utils.regex.compile_regex--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>reg</code> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Union[Pattern, Pattern]</code>"},{"location":"reference/edsnlp/utils/resources/","title":"<code>edsnlp.utils.resources</code>","text":""},{"location":"reference/edsnlp/utils/resources/#edsnlp.utils.resources.get_verbs","title":"<code>get_verbs</code>","text":"<p>Extract verbs from the resources, as a pandas dataframe.</p>"},{"location":"reference/edsnlp/utils/resources/#edsnlp.utils.resources.get_verbs--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>verbs</code> <p>List of verbs to keep. Returns all verbs by default.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>check_contains</code> <p>Whether to check that no verb is missing if a list of verbs was provided. By default True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>DataFrame containing conjugated verbs.</p>"},{"location":"reference/edsnlp/utils/resources/#edsnlp.utils.resources.get_adicap_dict","title":"<code>get_adicap_dict</code>  <code>cached</code>","text":"RETURNS DESCRIPTION <code>Dict</code>"},{"location":"reference/edsnlp/utils/span_getters/","title":"<code>edsnlp.utils.span_getters</code>","text":""},{"location":"reference/edsnlp/utils/span_getters/#edsnlp.utils.span_getters.SpanSetterArg","title":"<code>SpanSetterArg</code>","text":"<p>           Bases: <code>Validated</code></p> <p>Valid values for the <code>span_setter</code> argument of a component can be :</p> <ul> <li>a (doc, matches) -&gt; None callable</li> <li>a span group name</li> <li>a list of span group names</li> <li>a dict of group name to True or list of labels</li> </ul> <p>The group name <code>\"ents\"</code> is a special case, and will add the matches to <code>doc.ents</code></p>"},{"location":"reference/edsnlp/utils/span_getters/#edsnlp.utils.span_getters.SpanSetterArg--examples","title":"Examples","text":"<ul> <li><code>span_setter=[\"ents\", \"ckd\"]</code> will add the matches to both <code>doc.ents</code> and <code>doc.spans[\"ckd\"]</code>. It is equivalent to <code>{\"ents\": True, \"ckd\": True}</code>.</li> <li><code>span_setter={\"ents\": [\"foo\", \"bar\"]}</code> will add the matches with label \"foo\" and \"bar\" to <code>doc.ents</code>.</li> <li><code>span_setter=\"ents\"</code> will add all matches only to <code>doc.ents</code>.</li> <li><code>span_setter=\"ckd\"</code> will add all matches only to <code>doc.spans[\"ckd\"]</code>.</li> </ul>"},{"location":"reference/edsnlp/utils/span_getters/#edsnlp.utils.span_getters.SpanGetterArg","title":"<code>SpanGetterArg</code>","text":"<p>           Bases: <code>Validated</code></p> <p>Valid values for the <code>span_getter</code> argument of a component can be :</p> <ul> <li>a (doc) -&gt; spans callable</li> <li>a span group name</li> <li>a list of span group names</li> <li>a dict of group name to True or list of labels</li> </ul> <p>The group name <code>\"ents\"</code> is a special case, and will get the matches from <code>doc.ents</code></p>"},{"location":"reference/edsnlp/utils/span_getters/#edsnlp.utils.span_getters.SpanGetterArg--examples","title":"Examples","text":"<ul> <li><code>span_getter=[\"ents\", \"ckd\"]</code> will get the matches from both <code>doc.ents</code> and <code>doc.spans[\"ckd\"]</code>. It is equivalent to <code>{\"ents\": True, \"ckd\": True}</code>.</li> <li><code>span_getter={\"ents\": [\"foo\", \"bar\"]}</code> will get the matches with label \"foo\" and \"bar\" from <code>doc.ents</code>.</li> <li><code>span_getter=\"ents\"</code> will get all matches from <code>doc.ents</code>.</li> <li><code>span_getter=\"ckd\"</code> will get all matches from <code>doc.spans[\"ckd\"]</code>.</li> </ul>"},{"location":"reference/edsnlp/utils/span_getters/#edsnlp.utils.span_getters.make_span_context_getter","title":"<code>make_span_context_getter</code>","text":"<p>Create a span context getter.</p>"},{"location":"reference/edsnlp/utils/span_getters/#edsnlp.utils.span_getters.make_span_context_getter--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>context_words</code> <p>Minimum number of words to include on each side of the span. It could be asymmetric. For example (5,2) will include 5 words before the start of the span and 2 after the end of the span</p> <p> TYPE: <code>Union[NonNegativeInt, Tuple[NonNegativeInt, NonNegativeInt]]</code> </p> <code>context_sents</code> <pre><code>Union[NonNegativeInt, Tuple[NonNegativeInt, NonNegativeInt]]\n</code></pre> <p>] = 1 Minimum number of sentences to include on each side of the span:</p> <ul> <li>0: don't use sentences to build the context.</li> <li>1: include the sentence of the span.</li> <li>n: include n-1 sentences on each side of the span + the sentence of the span</li> </ul> <p> TYPE: <code>Optional[</code> </p> <pre><code>By default, 0 if the document has no sentence annotations, 1 otherwise.\n</code></pre>"},{"location":"reference/edsnlp/utils/span_getters/#edsnlp.utils.span_getters.ContextWindow","title":"<code>ContextWindow</code>","text":"<p>           Bases: <code>Validated</code>, <code>ABC</code></p> <p>A ContextWindow specifies how much additional context (such as sentences or words) should be included relative to an anchor span. For example, one might define a context window that extracts the sentence immediately preceding and following the anchor span, or one that extends the span by a given number of words before and after.</p> <p>ContextWindow objects can be combined using logical operations to create more complex context windows. For example, one can create a context window that includes either words from a -10 to +10 range or words from the sentence.</p>"},{"location":"reference/edsnlp/utils/span_getters/#edsnlp.utils.span_getters.ContextWindow--examples","title":"Examples","text":"<pre><code>from confit import validate_arguments\nfrom spacy.tokens import Span\n\nimport edsnlp\nfrom edsnlp.utils.span_getters import ContextWindow\n\n\n@validate_arguments\ndef apply_context(span: Span, ctx: ContextWindow):\n    # ctx will be parsed and cast as a ContextWindow\n    return ctx(span)\n\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(\"eds.sentences\")\n\ndoc = nlp(\"A first sentence. A second sentence, longer this time. A third.\")\nspan = doc[5:6]  # \"second\"\n\n# Will return a span with the 10 words before and after the span\n# and words of the current sentence and the next sentence.\napply_context(span, \"words[-3:3] | sents[0:1]\").text\n# Out: \"sentence. A second sentence, longer this time. A third.\"\n\n# Will return the span covering at most the -5 and +5 words\n# around the span and the current sentence of the span.\napply_context(span, \"words[-4:4] &amp; sent\").text\n# Out: \"A second sentence, longer this\"\n</code></pre> <p>Indexing</p> <p>Unlike standard Python sequence slicing, <code>sents[0:0]</code> returns the current sentence, not an empty span.</p>"},{"location":"reference/edsnlp/utils/spark_dtypes/","title":"<code>edsnlp.utils.spark_dtypes</code>","text":""},{"location":"reference/edsnlp/utils/spark_dtypes/#edsnlp.utils.spark_dtypes.infer_type","title":"<code>infer_type</code>","text":"<p>Infer the DataType from obj</p>"},{"location":"reference/edsnlp/utils/spark_dtypes/#edsnlp.utils.spark_dtypes.infer_schema","title":"<code>infer_schema</code>","text":"<p>Infer the schema from dict/namedtuple/object</p>"},{"location":"reference/edsnlp/utils/stream_sentinels/","title":"<code>edsnlp.utils.stream_sentinels</code>","text":""},{"location":"reference/edsnlp/utils/torch/","title":"<code>edsnlp.utils.torch</code>","text":""},{"location":"reference/edsnlp/utils/torch/#edsnlp.utils.torch.mask_to_triangle","title":"<code>mask_to_triangle</code>","text":"<p>Convert a boolean mask to a tensor containing distance to the nearest edge of the mask, in each direction. For example, if the mask is <pre><code>[\n    [1, 1, 1, 1, 1],\n    [1, 1, 1, 0, 0],\n]\n</code></pre></p> <p>The output will be <pre><code>[\n    [0, 1, 2, 1, 0],\n    [0, 1, 0, -1, -2]\n]\n</code></pre></p>"},{"location":"reference/edsnlp/utils/torch/#edsnlp.utils.torch.mask_to_triangle--parameters","title":"Parameters","text":"PARAMETER DESCRIPTION <code>mask</code> <p>A boolean mask</p> <p> </p> RETURNS DESCRIPTION <code>Tensor</code>"},{"location":"reference/edsnlp/utils/typing/","title":"<code>edsnlp.utils.typing</code>","text":""},{"location":"reference/edsnlp/viz/","title":"<code>edsnlp.viz</code>","text":""},{"location":"contributing/","title":"Contributing to EDS-NLP","text":"<p>We welcome contributions ! There are many ways to help. For example, you can:</p> <ol> <li>Help us track bugs by filing issues</li> <li>Suggest and help prioritise new functionalities</li> <li>Develop a new pipe ! Fork the project and propose a new functionality through a pull request</li> <li>Help us make the library as straightforward as possible, by simply asking questions on whatever does not seem clear to you.</li> </ol>"},{"location":"contributing/#development-installation","title":"Development installation","text":"<p>To be able to run the test suite, run the example notebooks and develop your own pipeline component, you should clone the repo and install it locally.</p> <pre><code># Clone the repository and change directory\n$ git clone https://github.com/aphp/edsnlp.git\n---&gt; 100%\n$ cd edsnlp\n\n# Optional: create a virtual environment\n$ python -m venv venv\n$ source venv/bin/activate\n\n# Install the package with common, dev, setup dependencies in editable mode\n$ pip install -e '.[dev,setup]'\n# And build resources\n$ python scripts/conjugate_verbs.py\n</code></pre> <p>To make sure the pipeline will not fail because of formatting errors, we added pre-commit hooks using the <code>pre-commit</code> Python library. To use it, simply install it:</p> <pre><code>$ pre-commit install\n</code></pre> <p>The pre-commit hooks defined in the configuration will automatically run when you commit your changes, letting you know if something went wrong.</p> <p>The hooks only run on staged changes. To force-run it on all files, run:</p> <pre><code>$ pre-commit run --all-files\n---&gt; 100%\ncolor:green All good !\n</code></pre>"},{"location":"contributing/#proposing-a-merge-request","title":"Proposing a merge request","text":"<p>At the very least, your changes should :</p> <ul> <li>Be well-documented ;</li> <li>Pass every tests, and preferably implement its own ;</li> <li>Follow the style guide.</li> </ul>"},{"location":"contributing/#testing-your-code","title":"Testing your code","text":"<p>We use the Pytest test suite.</p> <p>The following command will run the test suite. Writing your own tests is encouraged !</p> <pre><code>python -m pytest\n</code></pre> <p>Testing Cython code</p> <p>Make sure the package is installed in editable mode. Otherwise <code>Pytest</code> won't be able to find the Cython modules.</p> <p>Should your contribution propose a bug fix, we require the bug be thoroughly tested.</p>"},{"location":"contributing/#architecture-of-a-pipeline-component","title":"Architecture of a pipeline component","text":"<p>Pipes should follow the same pattern :</p> <pre><code>edsnlp/pipes/&lt;pipe&gt;\n   |-- &lt;pipe&gt;.py                # Defines the component logic\n   |-- patterns.py                  # Defines matched patterns\n   |-- factory.py                   # Declares the component to spaCy\n</code></pre>"},{"location":"contributing/#style-guide","title":"Style Guide","text":"<p>We use Black to reformat the code. While other formatter only enforce PEP8 compliance, Black also makes the code uniform. In short :</p> <p>Black reformats entire files in place. It is not configurable.</p> <p>Moreover, the CI/CD pipeline enforces a number of checks on the \"quality\" of the code. To wit, non black-formatted code will make the test pipeline fail. We use <code>pre-commit</code> to keep our codebase clean.</p> <p>Refer to the development install tutorial for tips on how to format your files automatically. Most modern editors propose extensions that will format files on save.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Make sure to document your improvements, both within the code with comprehensive docstrings, as well as in the documentation itself if need be.</p> <p>We use <code>MkDocs</code> for EDS-NLP's documentation. You can checkout the changes you make with:</p> <pre><code># Install the requirements\n$ pip install -e '.[docs]'\n---&gt; 100%\ncolor:green Installation successful\n\n# Run the documentation\n$ mkdocs serve\n</code></pre> <p>Go to <code>localhost:8000</code> to see your changes. MkDocs watches for changes in the documentation folder and automatically reloads the page.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Added support for multiple loggers (<code>tensorboard</code>, <code>wandb</code>, <code>comet_ml</code>, <code>aim</code>, <code>mlflow</code>, <code>clearml</code>, <code>dvclive</code>, <code>csv</code>, <code>json</code>, <code>rich</code>) in <code>edsnlp.train</code> via the <code>logger</code> parameter. Default is [<code>json</code> and <code>rich</code>] for backward compatibility.</li> <li>Sub batch sizes for gradient accumulation can now be defined as simple \"splits\" of the original batch, e.g. <code>batch_size = 10000 tokens</code> and <code>sub_batch_size = 5 splits</code> to accumulate batches of 2000 tokens.</li> <li>Parquet writer now has a <code>pyarrow_write_kwargs</code> to pass to pyarrow.dataset.write_dataset</li> <li>LinearSchedule (mostly used for LR scheduling) now allows a <code>end_value</code> parameter to configure if the learning rate should decay to zero or another value.</li> <li>New <code>eds.explode</code> pipe that splits one document into multiple documents, one per span yielded by its <code>span_getter</code> parameter, each new document containing exactly that single span.</li> <li>New <code>Training a span classifier</code> tutorial, and reorganized deep-learning docs</li> <li><code>ScheduledOptimizer</code> now warns when a parameter selector does not match any parameter.</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li><code>use_section</code> in <code>eds.history</code> should now correctly handle cases when there are other sections following history sections.</li> <li>Added clickable snippets in the documentation for more registered functions</li> <li>Pyarrow dataset writing with multiprocessing should be faster, as we removed a useless data transfer</li> <li>We should now correctly support loading transformers in offline mode if they were already in huggingface's cache</li> <li>We now support <code>words[-10:10]</code> syntax in trainable span classifier <code>context_getter</code> parameter</li> <li> Until now, <code>post_init</code> was applied after the instantiation of the optimizer : if the model discovered new labels, and therefore changed its parameter tensors to reflect that, these new tensors were not taken into account by the optimizer, which could likely lead to subpar performance. Now, <code>post_init</code> is applied before the optimizer is instantiated, so that the optimizer can correctly handle the new tensors.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Sections cues in <code>eds.history</code> are now section titles, and not the full section.</li> <li> Validation metrics are now found under the root field <code>validation</code> in the training logs (e.g. <code>metrics['validation']['ner']['micro']['f']</code>)</li> <li>It is now recommended to define optimizer groups of <code>ScheduledOptimizer</code> as a list of dicts of optim hyper-parameters, each containing a <code>selector</code> regex key, rather than as a single dict with a <code>selector</code> as keys and a dict of optim hyper-parameters as values. This allows for more flexibility in defining the optimizer groups, and is more consistent with the rest of the EDS-NLP API. This makes it easier to reference groups values from other places in config files, since their path doesn't contain a complex regex string anymore. See the updated training tutorials for more details.</li> </ul>"},{"location":"changelog/#v0172-2025-06-25","title":"v0.17.2 (2025-06-25)","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Handling intra-word linebreak as pollution : adds a pollution pattern that detects intra-word linebreak, which can then be removed in the <code>get_text</code> method</li> <li>Qualifiers can process <code>Span</code> or <code>Doc</code> : this feature especially makes it easier to nest qualifiers components in other components</li> <li>New label_weights parameter in eds.span_classifier`, which allows the user to set per label-value loss weights during training</li> <li>New <code>edsnlp.data.converters.MarkupToDocConverter</code> to convert Markdown or XML-like markup to documents, which is particularly useful to create annotated documents from scratch (e.g., for testing purposes).</li> <li>New Metrics documentation page to document the available metrics and how to use them.</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Various disorders/behaviors patches</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Deduplicate spans between doc.ents and doc.spans during train: previously, a <code>span_getter</code> requesting entities from both <code>ents</code> and <code>spans</code> could yield duplicates.</li> </ul>"},{"location":"changelog/#v0171-2025-05-26","title":"v0.17.1 (2025-05-26)","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Added grad spike detection to the <code>edsnlp.train</code> script, and per weight layer gradient logging.</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Fixed mini-batch accumulation for multi-task training</li> <li>Fixed a pickling error when applying a pipeline in multiprocessing mode. This occurred in some cases when one of the pipes was declared in a \"difficultly importable\" module (e.g., causing a \"PicklingWarning: Cannot locate reference to &lt;class...\").</li> <li>Fixed typo in <code>eds.consultation_dates</code> towns: <code>berck.sur.mer</code>.</li> <li>Fixed a bug where relative date expressions with bounds (e.g. 'depuis hier') raised an error when converted to durations.</li> <li>Fixed pipe ADICAP to deal with cases where not code is found after 'codification'/'adicap'</li> <li>Support \"00\"-like hours and minutes in the <code>eds.dates</code> component</li> <li>Fix arc minutes, arc seconds and degree unit scales in <code>eds.quantities</code>, used when converting between different time (or angle) units</li> </ul>"},{"location":"changelog/#v0170-2025-04-15","title":"v0.17.0 (2025-04-15)","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Support for numpy&gt;2.0, and formal support for Python 3.11 and Python 3.12</li> <li>Expose the defaults patterns of <code>eds.negation</code>, <code>eds.hypothesis</code>, <code>eds.family</code>, <code>eds.history</code> and <code>eds.reported_speech</code> under a <code>eds.negation.default_patterns</code> attribute</li> <li>Added a <code>context_getter</code> SpanGetter argument to the <code>eds.matcher</code> class to only retrieve entities inside the spans returned by the getter</li> <li>Added a <code>filter_expr</code> parameter to scorers to filter the documents to score</li> <li>Added a new <code>required</code> field to <code>eds.contextual_matcher</code> assign patterns to only match if the required field has been found, and an <code>include</code> parameter (similar to <code>exclude</code>) to search for required patterns without assigning them to the entity</li> <li>Added context strings (e.g., \"words[0:5] | sent[0:1]\") to the <code>eds.contextual_matcher</code> component to allow for more complex patterns in the selection of the window around the trigger spans.</li> <li>Include and exclude patterns in the contextual matcher now dismiss matches that occur inside the anchor pattern (e.g. \"anti\" exclude pattern for anchor pattern \"antibiotics\" will not match the \"anti\" part of \"antibiotics\")</li> <li>Pull Requests will now build a public accessible preview of the docs</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Improve the contextual matcher documentation.</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li><code>edsnlp.package</code> now correctly detect if a project uses an old-style poetry pyproject or a PEP621 pyproject.toml.</li> <li>PEP621 projects containing nested directories (e.g., \"my_project/pipes/foo.py\") are now supported.</li> <li>Try several paths to find current pip executable</li> <li>The parameter \"value_extract\" of <code>eds.score</code> now correctly handles lists of patterns.</li> <li>\"Zero variance error\" when computing param tuning importance are now catched and converted as a warning</li> </ul>"},{"location":"changelog/#v0160-2025-03-26","title":"v0.16.0 (2025-03-26)","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Hyperparameter Tuning for EDS-NLP: introduced a new script <code>edsnlp.tune</code> for hyperparameter tuning using Optuna. This feature allows users to efficiently optimize model parameters with options for single-phase or two-phase tuning strategies. Includes support for parameter importance analysis, visualization, pruning, and automatic handling of GPU time budgets.</li> <li>Provided a detailed tutorial on hyperparameter tuning, covering usage scenarios and configuration options.</li> <li><code>ScheduledOptimizer</code> (e.g., <code>@core: \"optimizer\"</code>) now supports importing optimizers using their qualified name (e.g., <code>optim: \"torch.optim.Adam\"</code>).</li> <li><code>eds.ner_crf</code> now computes confidence score on spans.</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>The loss of <code>eds.ner_crf</code> is now computed as the mean over the words instead of the sum. This change is compatible with multi-gpu training.</li> <li>Having multiple stats keys matching a batching pattern now warns instead of raising an error.</li> </ul>"},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Support packaging with poetry 2.0</li> <li>Solve pickling issues with multiprocessing when pytorch is installed</li> <li>Allow deep attributes like <code>a.b.c</code> for <code>span_attributes</code> in Standoff and OMOP doc2dict converters</li> <li> <p>Fixed various aspects of stream shuffling:</p> </li> <li> <p>Ensure the Parquet reader shuffles the data when <code>shuffle=True</code></p> </li> <li>Ensure we don't overwrite the RNG of the data reader when calling <code>stream.shuffle()</code> with no seed</li> <li>Raise an error if the batch size in <code>stream.shuffle(batch_size=...)</code> is not compatible with the stream</li> <li><code>eds.split</code> now keeps doc and span attributes in the sub-documents.</li> </ul>"},{"location":"changelog/#v0150-2024-12-13","title":"v0.15.0 (2024-12-13)","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li><code>edsnlp.data.read_parquet</code> now accept a <code>work_unit=\"fragment\"</code> option to split tasks between workers by parquet fragment instead of row. When this is enabled, workers do not read every fragment while skipping 1 in n rows, but read all rows of 1/n fragments, which should be faster.</li> <li>Accept no validation data in <code>edsnlp.train</code> script</li> <li>Log the training config at the beginning of the trainings</li> <li>Support a specific model output dir path for trainings (<code>output_model_dir</code>), and whether to save the model or not (<code>save_model</code>)</li> <li>Specify whether to log the validation results or not (<code>logger=False</code>)</li> <li>Added support for the CoNLL format with <code>edsnlp.data.read_conll</code> and with a specific <code>eds.conll_dict2doc</code> converter</li> <li>Added a Trainable Biaffine Dependency Parser (<code>eds.biaffine_dep_parser</code>) component and metrics</li> <li>New <code>eds.extractive_qa</code> component to perform extractive question answering using questions as prompts to tag entities instead of a list of predefined labels as in <code>eds.ner_crf</code>.</li> </ul>"},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Fix <code>join_thread</code> missing attribute in <code>SimpleQueue</code> when cleaning a multiprocessing executor</li> <li>Support huggingface transformers that do not set <code>cls_token_id</code> and <code>sep_token_id</code> (we now also look for these tokens in the <code>special_tokens_map</code> and <code>vocab</code> mappings)</li> <li>Fix changing scorers dict size issue when evaluating during training</li> <li>Seed random states (instead of using <code>random.RandomState()</code>) when shuffling in data readers : this is important for</li> <li>reproducibility</li> <li>in multiprocessing mode, ensure that the same data is shuffled in the same way in all workers</li> <li>Bubble BaseComponent instantiation errors correctly</li> <li>Improved support for multi-gpu gradient accumulation (only sync the gradients at the end of the accumulation), now controled by the optiona <code>sub_batch_size</code> argument of <code>TrainingData</code>.</li> <li>Support again edsnlp without pytorch installed</li> <li>We now test that edsnlp works without pytorch installed</li> <li>Fix units and scales, ie 1l = 1dm3, 1ml = 1cm3</li> </ul>"},{"location":"changelog/#v0140-2024-11-14","title":"v0.14.0 (2024-11-14)","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>Support for setuptools based projects in <code>edsnlp.package</code> command</li> <li>Pipelines can now be instantiated directly from a config file (instead of having to cast a dict containing their arguments) by putting the @core = \"pipeline\" or \"load\" field in the pipeline section)</li> <li><code>edsnlp.load</code> now correctly takes disable, enable and exclude parameters into account</li> <li>Pipeline now has a basic repr showing is base langage (mostly useful to know its tokenizer) and its pipes</li> <li>New <code>python -m edsnlp.evaluate</code> script to evaluate a model on a dataset</li> <li>Sentence detection can now be configured to change the minimum number of newlines to consider a newline-triggered sentence, and disable capitalization checking.</li> <li>New <code>eds.split</code> pipe to split a document into multiple documents based on a splitting pattern (useful for training)</li> <li>Allow <code>converter</code> argument of <code>edsnlp.data.read/from_...</code> to be a list of converters instead of a single converter</li> <li>New revamped and documented <code>edsnlp.train</code> script and API</li> <li>Support YAML config files (supported only CFG/INI files before)</li> <li>Most of EDS-NLP functions are now clickable in the documentation</li> <li> <p>ScheduledOptimizer now accepts schedules directly in place of parameters, and easy parameter selection:</p> <pre><code>ScheduledOptimizer(\n    optim=\"adamw\",\n    module=nlp,\n    total_steps=2000,\n    groups={\n        \"^transformer\": {\n            # lr will go from 0 to 5e-5 then to 0 for params matching \"transformer\"\n            \"lr\": {\"@schedules\": \"linear\", \"warmup_rate\": 0.1, \"start_value\": 0 \"max_value\": 5e-5,},\n        },\n        \"\": {\n            # lr will go from 3e-4 during 200 steps then to 0 for other params\n            \"lr\": {\"@schedules\": \"linear\", \"warmup_rate\": 0.1, \"start_value\": 3e-4 \"max_value\": 3e-4,},\n        },\n    },\n)\n</code></pre> </li> </ul>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li><code>eds.span_context_getter</code>'s parameter <code>context_sents</code> is no longer optional and must be explicitly set to 0 to disable sentence context</li> <li>In multi-GPU setups, streams that contain torch components are now stripped of their parameter tensors when sent to CPU Workers since these workers only perform preprocessing and postprocessing and should therefore not need the model parameters.</li> <li>The <code>batch_size</code> argument of <code>Pipeline</code> is deprecated and is not used anymore. Use the <code>batch_size</code> argument of <code>stream.map_pipeline</code> instead.</li> </ul>"},{"location":"changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Sort files before iterating over a standoff or json folder to ensure reproducibility</li> <li>Sentence detection now correctly match capitalized letters + apostrophe</li> <li>We now ensure that the workers pool is properly closed whatever happens (exception, garbage collection, data ending) in the <code>multiprocessing</code> backend. This prevents some executions from hanging indefinitely at the end of the processing.</li> <li>Propagate torch sharing strategy to other workers in the <code>multiprocessing</code> backend. This is useful when the system is running out of file descriptors and <code>ulimit -n</code> is not an option. Torch sharing strategy can also be set via an environment variable <code>TORCH_SHARING_STRATEGY</code> (default is <code>file_descriptor</code>, consider using <code>file_system</code> if you encounter issues).</li> </ul>"},{"location":"changelog/#data-api-changes","title":"Data API changes","text":"<ul> <li><code>LazyCollection</code> objects are now called <code>Stream</code> objects</li> <li>By default, <code>multiprocessing</code> backend now preserves the order of the input data. To disable this and improve performance, use <code>deterministic=False</code> in the <code>set_processing</code> method</li> <li> <p> Parallelized GPU inference throughput improvements !</p> <ul> <li>For simple {pre-process \u2192 model \u2192 post-process} pipelines, GPU inference can be up to 30% faster in non-deterministic mode (results can be out of order) and up to 20% faster in deterministic mode (results are in order)</li> <li>For multitask pipelines, GPU inference can be up to twice as fast (measured in a two-tasks BERT+NER+Qualif pipeline on T4 and A100 GPUs)</li> </ul> </li> <li> <p>The <code>.map_batches</code>, <code>.map_pipeline</code> and <code>.map_gpu</code> methods now support a specific <code>batch_size</code> and batching function, instead of having a single batch size for all pipes</p> </li> <li>Readers now have a <code>loop</code> parameter to cycle over the data indefinitely (useful for training)</li> <li>Readers now have a <code>shuffle</code> parameter to shuffle the data before iterating over it</li> <li>In <code>multiprocessing</code> mode, file based readers now read the data in the workers (was an option before)</li> <li> <p>We now support two new special batch sizes</p> <ul> <li>\"fragment\" in the case of parquet datasets: rows of a full parquet file fragment per batch</li> <li>\"dataset\" which is mostly useful during training, for instance to shuffle the dataset at each epoch.   These are also compatible in batched writer such as parquet, where each input fragment can be processed and mapped to a single matching output fragment.</li> </ul> </li> <li> <p> Breaking change: a <code>map</code> function returning a list or a generator won't be automatically flattened anymore. Use <code>flatten()</code> to flatten the output if needed. This shouldn't change the behavior for most users since most writers (to_pandas, to_polars, to_parquet, ...) still flatten the output</p> </li> <li> Breaking change: the <code>chunk_size</code> and <code>sort_chunks</code> are now deprecated : to sort data before applying a transformation, use <code>.map_batches(custom_sort_fn, batch_size=...)</code></li> </ul>"},{"location":"changelog/#training-api-changes","title":"Training API changes","text":"<ul> <li>We now provide a training script <code>python -m edsnlp.train --config config.cfg</code> that should fit many use cases. Check out the docs !</li> <li>In particular, we do not require pytorch's Dataloader for training and can rely solely on EDS-NLP stream/data API, which is better suited for large streamable datasets and dynamic preprocessing (ie different result each time we apply a noised preprocessing op on a sample).</li> <li> <p>Each trainable component can now provide a <code>stats</code> field in its <code>preprocess</code> output to log info about the sample (number of words, tokens, spans, ...):</p> <ul> <li>these stats are both used for batching (e.g., make batches of no more than \"25000 tokens\")</li> <li>for logging</li> <li>for computing correct loss means when accumulating gradients over multiple mini-mini-batches</li> <li>for computing correct loss means in multi-GPU setups, since these stats are synchronized and accumulated across GPUs</li> </ul> </li> <li> <p>Support multi GPU training via hugginface <code>accelerate</code> and EDS-NLP <code>Stream</code> API consideration of env['WOLRD_SIZE'] and env['LOCAL_RANK'] environment variables</p> </li> </ul>"},{"location":"changelog/#v0131","title":"v0.13.1","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li><code>eds.tables</code> accepts a minimum_table_size (default 2) argument to reduce pollution</li> <li><code>RuleBasedQualifier</code> now expose a <code>process</code> method that only returns qualified entities and token without actually tagging them, deferring this task to the <code>__call__</code> method.</li> <li>Added new patterns for metastasis detection. Developed on CT-Scan reports.</li> <li>Added citation of articles</li> </ul>"},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>Renamed <code>edsnlp.scorers</code> to <code>edsnlp.metrics</code> and removed the <code>_scorer</code> suffix from their   registry name (e.g, <code>@scorers = ner_overlap_scorer</code> \u2192 <code>@metrics = ner_overlap</code>)</li> <li>Rename <code>eds.measurements</code> to <code>eds.quantities</code></li> <li>scikit-learn (used in <code>eds.endlines</code>) is no longer installed by default when installing <code>edsnlp[ml]</code></li> </ul>"},{"location":"changelog/#fixed_7","title":"Fixed","text":"<ul> <li>Disorder and Behavior pipes don't use a \"PRESENT\" or \"ABSENT\" <code>status</code> anymore. Instead, <code>status=None</code> by default,   and <code>ent._.negation</code> is set to True instead of setting <code>status</code> to \"ABSENT\". To this end, the tobacco and alcohol   now use the <code>NegationQualifier</code> internally.</li> <li>Numbers are now only detected without trying to remove the pollution in between digits, ie <code>55 @ 77777</code> could be detected as a full number before, but not anymore.</li> <li>Resolve encoding-related data reading issues by forcing utf-8</li> </ul>"},{"location":"changelog/#v0130","title":"v0.13.0","text":""},{"location":"changelog/#added_8","title":"Added","text":"<ul> <li><code>data.set_processing(...)</code> now expose an <code>autocast</code> parameter to disable or tweak the automatic casting of the tensor   during the processing. Autocasting should result in a slight speedup, but may lead to numerical instability.</li> <li>Use <code>torch.inference_mode</code> to disable view tracking and version counter bumps during inference.</li> <li>Added a new NER pipeline for suicide attempt detection</li> <li>Added date cues (regular expression matches that contributed to a date being detected) under the extension <code>ent._.date_cues</code></li> <li>Added tables processing in eds.measurement</li> <li>Added 'all' as possible input in eds.measurement measurements config</li> <li>Added new units in eds.measurement</li> </ul>"},{"location":"changelog/#changed_6","title":"Changed","text":"<ul> <li>Default to mixed precision inference</li> </ul>"},{"location":"changelog/#fixed_8","title":"Fixed","text":"<ul> <li><code>edsnlp.load(\"your/huggingface-model\", install_dependencies=True)</code> now correctly resolves the python pip   (especially on Colab) to auto-install the model dependencies</li> <li>We now better handle empty documents in the <code>eds.transformer</code>, <code>eds.text_cnn</code> and <code>eds.ner_crf</code> components</li> <li>Support mixed precision in <code>eds.text_cnn</code> and <code>eds.ner_crf</code> components</li> <li>Support pre-quantization (&lt;4.30) transformers versions</li> <li>Verify that all batches are non empty</li> <li>Fix <code>span_context_getter</code> for <code>context_words</code> = 0, <code>context_sents</code> &gt; 2 and support assymetric contexts</li> <li>Don't split sentences on rare unicode symbols</li> <li>Better detect abbreviations, like <code>E.coli</code>, now split as [<code>E.</code>, <code>coli</code>] and not [<code>E</code>, <code>.</code>, <code>coli</code>]</li> </ul>"},{"location":"changelog/#v0123","title":"v0.12.3","text":""},{"location":"changelog/#changed_7","title":"Changed","text":"<p>Packages:</p> <ul> <li>Pip-installable models are now built with <code>hatch</code> instead of poetry, which allows us to expose <code>artifacts</code> (weights)   at the root of the sdist package (uploadable to HF) and move them inside the package upon installation to avoid conflicts.</li> <li>Dependencies are no longer inferred with dill-magic (this didn't work well before anyway)</li> <li>Option to perform substitutions in the model's README.md file (e.g., for the model's name, metrics, ...)</li> <li>Huggingface models are now installed with pip editable installations, which is faster since it doesn't copy around the weights</li> </ul>"},{"location":"changelog/#v0121","title":"v0.12.1","text":""},{"location":"changelog/#added_9","title":"Added","text":"<ul> <li>Added binary distribution for linux aarch64 (Streamlit's environment)</li> <li>Added new separator option in eds.table and new input check</li> </ul>"},{"location":"changelog/#fixed_9","title":"Fixed","text":"<ul> <li>Make catalogue &amp; entrypoints compatible with py37-py312</li> <li>Check that a data has a doc before trying to use the document's <code>note_datetime</code></li> </ul>"},{"location":"changelog/#v0120","title":"v0.12.0","text":""},{"location":"changelog/#added_10","title":"Added","text":"<ul> <li>The <code>eds.transformer</code> component now accepts <code>prompts</code> (passed to its <code>preprocess</code> method, see breaking change below) to add before each window of text to embed.</li> <li><code>LazyCollection.map</code> / <code>map_batches</code> now support generator functions as arguments.</li> <li>Window stride can now be disabled (i.e., stride = window) during training in the <code>eds.transformer</code> component by <code>training_stride = False</code></li> <li>Added a new <code>eds.ner_overlap_scorer</code> to evaluate matches between two lists of entities, counting true when the dice overlap is above a given threshold</li> <li><code>edsnlp.load</code> now accepts EDS-NLP models from the huggingface hub \ud83e\udd17 !</li> <li>New <code>python -m edsnlp.package</code> command to package a model for the huggingface hub or pypi-like registries</li> <li>Improve table detection in <code>eds.tables</code> and support new options in <code>table._.to_pd_table(...)</code>:</li> <li><code>header=True</code> to use first row as header</li> <li><code>index=True</code> to use first column as index</li> <li><code>as_spans=True</code> to fill cells as document spans instead of strings</li> </ul>"},{"location":"changelog/#changed_8","title":"Changed","text":"<ul> <li> Major breaking change in trainable components, moving towards a more \"task-centric\" design:</li> <li>the <code>eds.transformer</code> component is no longer responsible for deciding which spans of text (\"contexts\") should be embedded. These contexts are now passed via the <code>preprocess</code> method, which now accepts more arguments than just the docs to process.</li> <li>similarly the <code>eds.span_pooler</code> is now longer responsible for deciding which spans to pool, and instead pools all spans passed to it in the <code>preprocess</code> method.</li> </ul> <p>Consequently, the <code>eds.transformer</code> and <code>eds.span_pooler</code> no longer accept their <code>span_getter</code> argument, and the <code>eds.ner_crf</code>, <code>eds.span_classifier</code>, <code>eds.span_linker</code> and <code>eds.span_qualifier</code> components now accept a <code>context_getter</code> argument instead, as well as a <code>span_getter</code> argument for the latter two. This refactoring can be summarized as follows:</p> <pre><code>```diff\n- eds.transformer.span_getter\n+ eds.ner_crf.context_getter\n+ eds.span_classifier.context_getter\n+ eds.span_linker.context_getter\n\n- eds.span_pooler.span_getter\n+ eds.span_qualifier.span_getter\n+ eds.span_linker.span_getter\n```\n\nand as an example for the `eds.span_linker` component:\n\n```diff\nnlp.add_pipe(\n    eds.span_linker(\n        metric=\"cosine\",\n        probability_mode=\"sigmoid\",\n+       span_getter=\"ents\",\n+       # context_getter=\"ents\",  -&gt; by default, same as span_getter\n        embedding=eds.span_pooler(\n            hidden_size=128,\n-           span_getter=\"ents\",\n            embedding=eds.transformer(\n-               span_getter=\"ents\",\n                model=\"prajjwal1/bert-tiny\",\n                window=128,\n                stride=96,\n            ),\n        ),\n    ),\n    name=\"linker\",\n)\n```\n</code></pre> <ul> <li>Trainable embedding components now all use <code>foldedtensor</code> to return embeddings, instead of returning a tensor of floats and a mask tensor.</li> <li> TorchComponent <code>__call__</code> no longer applies the end to end method, and instead calls the <code>forward</code> method directly, like all torch modules.</li> <li>The trainable <code>eds.span_qualifier</code> component has been renamed to <code>eds.span_classifier</code> to reflect its general purpose (it doesn't only predict qualifiers, but any attribute of a span using its context or not).</li> <li><code>omop</code> converter now takes the <code>note_datetime</code> field into account by default when building a document</li> <li><code>span._.date.to_datetime()</code> and <code>span._.date.to_duration()</code> now automatically take the <code>note_datetime</code> into account</li> <li><code>nlp.vocab</code> is no longer serialized when saving a model, as it may contain sensitive information and can be recomputed during inference anyway</li> </ul>"},{"location":"changelog/#fixed_10","title":"Fixed","text":"<ul> <li><code>edsnlp.data.read_json</code> now correctly read the files from the directory passed as an argument, and not from the parent directory.</li> <li>Overwrite spacy's Doc, Span and Token pickling utils to allow recursively storing Doc, Span and Token objects in the extension values (in particular, span._.date.doc)</li> <li>Removed pendulum dependency, solving various pickling, multiprocessing and missing attributes errors</li> </ul>"},{"location":"changelog/#v0112","title":"v0.11.2","text":""},{"location":"changelog/#fixed_11","title":"Fixed","text":"<ul> <li>Fix <code>edsnlp.utils.file_system.normalize_fs_path</code> file system detection not working correctly</li> <li>Improved performance of <code>edsnlp.data</code> methods over a filesystem (<code>fs</code> parameter)</li> </ul>"},{"location":"changelog/#v0111-2024-04-02","title":"v0.11.1 (2024-04-02)","text":""},{"location":"changelog/#added_11","title":"Added","text":"<ul> <li>Automatic estimation of cpu count when using multiprocessing</li> <li><code>optim.initialize()</code> method to create optim state before the first backward pass</li> </ul>"},{"location":"changelog/#changed_9","title":"Changed","text":"<ul> <li><code>nlp.post_init</code> will not tee lazy collections anymore (use <code>edsnlp.utils.collections.multi_tee</code> yourself if needed)</li> </ul>"},{"location":"changelog/#fixed_12","title":"Fixed","text":"<ul> <li>Corrected inconsistencies in <code>eds.span_linker</code></li> </ul>"},{"location":"changelog/#v0110-2024-03-29","title":"v0.11.0 (2024-03-29)","text":""},{"location":"changelog/#added_12","title":"Added","text":"<ul> <li>Support for a <code>filesystem</code> parameter in every <code>edsnlp.data.read_*</code> and <code>edsnlp.data.write_*</code> functions</li> <li>Pipes of a pipeline are now easily accessible with <code>nlp.pipes.xxx</code> instead of <code>nlp.get_pipe(\"xxx\")</code></li> <li>Support builtin Span attributes in converters <code>span_attributes</code> parameter, e.g.   <pre><code>import edsnlp\n\nnlp = ...\nnlp.add_pipe(\"eds.sentences\")\n\ndata = edsnlp.data.from_xxx(...)\ndata = data.map_pipeline(nlp)\ndata.to_pandas(converters={\"ents\": {\"span_attributes\": [\"sent.text\", \"start\", \"end\"]}})\n</code></pre></li> <li>Support assigning Brat AnnotatorNotes as span attributes: <code>edsnlp.data.read_standoff(...,  notes_as_span_attribute=\"cui\")</code></li> <li>Support for mapping full batches in <code>edsnlp.processing</code> pipelines with <code>map_batches</code> lazy collection method:   <pre><code>import edsnlp\n\ndata = edsnlp.data.from_xxx(...)\ndata = data.map_batches(lambda batch: do_something(batch))\ndata.to_pandas()\n</code></pre></li> <li>New <code>data.map_gpu</code> method to map a deep learning operation on some data and take advantage of edsnlp multi-gpu inference capabilities</li> <li>Added average precision computation in edsnlp span_classification scorer</li> <li>You can now add pipes to your pipeline by instantiating them directly, which comes with many advantages, such as auto-completion, introspection and type checking !</li> </ul> <pre><code>import edsnlp, edsnlp.pipes as eds\n\nnlp = edsnlp.blank(\"eds\")\nnlp.add_pipe(eds.sentences())\n# instead of nlp.add_pipe(\"eds.sentences\")\n</code></pre> <p>The previous way of adding pipes is still supported. - New <code>eds.span_linker</code> deep-learning component to match entities with their concepts in a knowledge base, in synonym-similarity or concept-similarity mode.</p>"},{"location":"changelog/#changed_10","title":"Changed","text":"<ul> <li><code>nlp.preprocess_many</code> now uses lazy collections to enable parallel processing</li> <li> Breaking change. Improved and simplified <code>eds.span_qualifier</code>: we didn't support combination groups before, so this feature was scrapped for now. We now also support splitting values of a single qualifier between different span labels.</li> <li>Optimized edsnlp.data batching, especially for large batch sizes (removed a quadratic loop)</li> <li> Breaking change. By default, the name of components added to a pipeline is now the default name defined in their class <code>__init__</code> signature. For most components of EDS-NLP, this will change the name from \"eds.xxx\" to \"xxx\".</li> </ul>"},{"location":"changelog/#fixed_13","title":"Fixed","text":"<ul> <li>Flatten list outputs (such as \"ents\" converter) when iterating: <code>nlp.map(data).to_iterable(\"ents\")</code> is now a list of entities, and not a list of lists of entities</li> <li>Allow span pooler to choose between multiple base embedding spans (as likely produced by <code>eds.transformer</code>) by sorting them by Dice overlap score.</li> <li>EDS-NLP does not raise an error anymore when saving a model to an already existing, but empty directory</li> </ul>"},{"location":"changelog/#v0107-2024-03-12","title":"v0.10.7 (2024-03-12)","text":""},{"location":"changelog/#added_13","title":"Added","text":"<ul> <li>Support empty writer converter by default in <code>edsnlp.data</code> readers / writers (do not convert by default)</li> <li>Add support for polars data import / export</li> <li>Allow kwargs in <code>eds.transformer</code> to pass to the transformer model</li> </ul>"},{"location":"changelog/#changed_11","title":"Changed","text":"<ul> <li>Saving pipelines now longer saves the <code>disabled</code> status of the pipes (i.e., all pipes are considered \"enabled\" when saved). This feature was not used and causing issues when saving a model wrapped in a <code>nlp.select_pipes</code> context.</li> </ul>"},{"location":"changelog/#fixed_14","title":"Fixed","text":"<ul> <li>Allow missing <code>meta.json</code>, <code>tokenizer</code> and <code>vocab</code> paths when loading saved models</li> <li>Save torch buffers when dumping machine learning models to disk (previous versions only saved the model parameters)</li> <li>Fix automatic <code>batch_size</code> estimation in <code>eds.transformer</code> when <code>max_tokens_per_device</code> is set to <code>auto</code> and multiple GPUs are used</li> <li>Fix JSONL file parsing</li> </ul>"},{"location":"changelog/#v0106-2024-02-24","title":"v0.10.6 (2024-02-24)","text":""},{"location":"changelog/#added_14","title":"Added","text":"<ul> <li>Added <code>batch_by</code>, <code>split_into_batches_after</code>, <code>sort_chunks</code>, <code>chunk_size</code>, <code>disable_implicit_parallelism</code> parameters to processing (<code>simple</code> and <code>multiprocessing</code>) backends to improve performance   and memory usage. Sorting chunks can improve yield up to twice the speed in some cases.</li> <li>The deep learning cache mechanism now supports multitask models with weight sharing in multiprocessing mode.</li> <li>Added <code>max_tokens_per_device=\"auto\"</code> parameter to <code>eds.transformer</code> to estimate memory usage and automatically split the input into chunks that fit into the GPU.</li> </ul>"},{"location":"changelog/#changed_12","title":"Changed","text":"<ul> <li>Improved speed and memory usage of the <code>eds.text_cnn</code> pipe by running the CNN on a non-padded version of its input: expect a speedup up to 1.3x in real-world use cases.</li> <li>Deprecate the converters' (especially for BRAT/Standoff data) <code>bool_attributes</code>   parameter in favor of general <code>default_attributes</code>. This new mapping describes how to   set attributes on spans for which no attribute value was found in the input format.   This is especially useful for negation, or frequent attributes values (e.g. \"negated\"   is often False, \"temporal\" is often \"present\"), that annotators may not want to   annotate every time.</li> <li>Default <code>eds.ner_crf</code> window is now set to 40 and stride set to 20, as it doesn't   affect throughput (compared to before, window set to 20) and improves accuracy.</li> <li>New default <code>overlap_policy='merge'</code> option and parameter renaming in   <code>eds.span_context_getter</code> (which replaces <code>eds.span_sentence_getter</code>)</li> </ul>"},{"location":"changelog/#fixed_15","title":"Fixed","text":"<ul> <li>Improved error handling in <code>multiprocessing</code> backend (e.g., no more deadlock)</li> <li>Various improvements to the data processing related documentation pages</li> <li>Begin of sentence / end of sentence transitions of the <code>eds.ner_crf</code> component are now   disabled when windows are used (e.g., neither <code>window=1</code> equivalent to softmax and   <code>window=0</code>equivalent to default full sequence Viterbi decoding)</li> <li><code>eds</code> tokenizer nows inherits from <code>spacy.Tokenizer</code> to avoid typing errors</li> <li>Only match 'ne' negation pattern when not part of another word to avoid false positives cases like <code>u[ne] cure de 10 jours</code></li> <li>Disabled pipes are now correctly ignored in the <code>Pipeline.preprocess</code> method</li> <li>Add \"eventuel*\" patterns to <code>eds.hyphothesis</code></li> </ul>"},{"location":"changelog/#v0105-2024-01-29","title":"v0.10.5 (2024-01-29)","text":""},{"location":"changelog/#fixed_16","title":"Fixed","text":"<ul> <li>Allow non-url paths when parquet filesystem is given</li> </ul>"},{"location":"changelog/#v0104-2024-01-19","title":"v0.10.4 (2024-01-19)","text":""},{"location":"changelog/#changed_13","title":"Changed","text":"<ul> <li>Assigning <code>doc._.note_datetime</code> will now automatically cast the value to a <code>pendulum.DateTime</code> object</li> </ul>"},{"location":"changelog/#added_15","title":"Added","text":"<ul> <li>Support loading model from package name (e.g., <code>edsnlp.load(\"eds_pseudo_aphp\")</code>)</li> <li>Support filesystem parameter in <code>edsnlp.data.read_parquet</code> and <code>edsnlp.data.write_parquet</code></li> </ul>"},{"location":"changelog/#fixed_17","title":"Fixed","text":"<ul> <li>Support doc -&gt; list converters with parquet files writer</li> <li>Fixed some OOM errors when writing many outputs to parquet files</li> <li>Both edsnlp &amp; spacy factories are now listed when a factory lookup fails</li> <li>Fixed some GPU OOM errors with the <code>eds.transformer</code> pipe when processing really long documents</li> </ul>"},{"location":"changelog/#v0103-2024-01-11","title":"v0.10.3 (2024-01-11)","text":""},{"location":"changelog/#added_16","title":"Added","text":"<ul> <li>By default, <code>edsnlp.data.write_json</code> will infer if the data should be written as a single JSONL   file or as a directory of JSON files, based on the <code>path</code> argument being a file or not.</li> </ul>"},{"location":"changelog/#fixed_18","title":"Fixed","text":"<ul> <li>Measurements now correctly match \"0.X\", \"0.XX\", ... numbers</li> <li>Typo in \"celsius\" measurement unit</li> <li>Spaces and digits are now supported in BRAT entity labels</li> <li>Fixed missing 'permet pas + verb' false positive negation patterns</li> </ul>"},{"location":"changelog/#v0102-2023-12-20","title":"v0.10.2 (2023-12-20)","text":""},{"location":"changelog/#changed_14","title":"Changed","text":"<ul> <li><code>eds.span_qualifier</code> qualifiers argument now automatically adds the underscore prefix if not present</li> </ul>"},{"location":"changelog/#fixed_19","title":"Fixed","text":"<ul> <li>Fix imports of components declared in <code>spacy_factories</code> entry points</li> <li>Support <code>pendulum</code> v3</li> <li><code>AsList</code> errors are now correctly reported</li> <li><code>eds.span_qualifier</code> saved configuration during <code>to_disk</code> is now longer null</li> </ul>"},{"location":"changelog/#v0101-2023-12-15","title":"v0.10.1 (2023-12-15)","text":""},{"location":"changelog/#changed_15","title":"Changed","text":"<ul> <li>Small regex matching performance improvement, up to 1.25x faster (e.g. <code>eds.measurements</code>)</li> </ul>"},{"location":"changelog/#fixed_20","title":"Fixed","text":"<ul> <li>Microgram scale is now correctly 1/1000g and inverse meter now 1/100 inverse cm.</li> <li>We now isolate some of edsnlp components (trainable pipes that require ml dependencies)   in a new <code>edsnlp_factories</code> entry points to prevent spacy from auto-importing them.</li> <li>TNM scores followed by a space are now correctly detected</li> <li>Removed various short TNM false positives (e.g., \"PT\" or \"a T\") and false negatives</li> <li>The Span value extension is not more forcibly overwritten, and user assigned values are returned by <code>Span._.value</code> in priority, before the aggregated <code>span._.get(span.label_)</code> getter result (#220)</li> <li>Enable mmap during multiprocessing model transfers</li> <li><code>RegexMatcher</code> now supports all alignment modes (<code>strict</code>, <code>expand</code>, <code>contract</code>) and better handles partial doc matching (#201).</li> <li><code>on_ent_only=False/True</code> is now supported again in qualifier pipes (e.g., \"eds.negation\", \"eds.hypothesis\", ...)</li> </ul>"},{"location":"changelog/#v0100-2023-12-04","title":"v0.10.0 (2023-12-04)","text":""},{"location":"changelog/#added_17","title":"Added","text":"<ul> <li>New add unified <code>edsnlp.data</code> api (json, brat, spark, pandas) and LazyCollection object   to efficiently read / write data from / to different formats &amp; sources.</li> <li>New unified processing API to select the execution execution backends via <code>data.set_processing(...)</code></li> <li>The training scripts can now use data from multiple concatenated adapters</li> <li>Support quantized transformers (compatible with multiprocessing as well !)</li> </ul>"},{"location":"changelog/#changed_16","title":"Changed","text":"<ul> <li><code>edsnlp.pipelines</code> has been renamed to <code>edsnlp.pipes</code>, but the old name is still available for backward compatibility</li> <li>Pipes (in <code>edsnlp/pipes</code>) are now lazily loaded, which should improve the loading time of the library.</li> <li><code>to_disk</code> methods can now return a config to override the initial config of the pipeline (e.g., to load a transformer directly from the path storing its fine-tuned weights)</li> <li>The <code>eds.tokenizer</code> tokenizer has been added to entry points, making it accessible from the outside</li> <li>Deprecate old connectors (e.g. BratDataConnector) in favor of the new <code>edsnlp.data</code> API</li> <li>Deprecate old <code>pipe</code> wrapper in favor of the new processing API</li> </ul>"},{"location":"changelog/#fixed_21","title":"Fixed","text":"<ul> <li>Support for pydantic v2</li> <li>Support for python 3.11 (not ci-tested yet)</li> </ul>"},{"location":"changelog/#v0100beta1-2023-12-04","title":"v0.10.0beta1 (2023-12-04)","text":"<p>Large refacto of EDS-NLP to allow training models and performing inference using PyTorch as the deep-learning backend. Rather than a mere wrapper of Pytorch using spaCy, this is a new framework to build hybrid multi-task models.</p> <p>To achieve this, instead of patching spaCy's pipeline, a new pipeline was implemented in a similar fashion to aphp/edspdf#12. The new pipeline tries to preserve the existing API, especially for non-machine learning uses such as rule-based components. This means that users can continue to use the library in the same way as before, while also having the option to train models using PyTorch. We still use spaCy data structures such as Doc and Span to represent the texts and their annotations.</p> <p>Otherwise, changes should be transparent for users that still want to use spacy pipelines with <code>nlp = spacy.blank('eds')</code>. To benefit from the new features, users should use <code>nlp = edsnlp.blank('eds')</code> instead.</p>"},{"location":"changelog/#added_18","title":"Added","text":"<ul> <li>New pipeline system available via <code>edsnlp.blank('eds')</code> (instead of <code>spacy.blank('eds')</code>)</li> <li>Use the confit package to instantiate components</li> <li>Training script with Pytorch only (<code>tests/training/</code>) and tutorial</li> <li>New trainable embeddings: <code>eds.transformer</code>, <code>eds.text_cnn</code>, <code>eds.span_pooler</code>   embedding contextualizer pipes</li> <li>Re-implemented the trainable NER component and trainable Span qualifier with the new   system under <code>eds.ner_crf</code> and <code>eds.span_classifier</code></li> <li>New efficient implementation for eds.transformer (to be used in place of   spacy-transformer)</li> </ul>"},{"location":"changelog/#changed_17","title":"Changed","text":"<ul> <li>Pipe registering: <code>Language.factory</code> -&gt; <code>edsnlp.registry.factory.register</code> via confit</li> <li>Lazy loading components from their entry point (had to patch spacy.Language.init)   to avoid having to wrap every import torch statement for pure rule-based use cases.   Hence, torch is not a required dependency</li> </ul>"},{"location":"changelog/#v092-2023-12-04","title":"v0.9.2 (2023-12-04)","text":""},{"location":"changelog/#changed_18","title":"Changed","text":"<ul> <li>Fix matchers to skip pipes with assigned extensions that are not required by the matcher during the initialization</li> </ul>"},{"location":"changelog/#v091-2023-09-22","title":"v0.9.1 (2023-09-22)","text":""},{"location":"changelog/#changed_19","title":"Changed","text":"<ul> <li>Improve negation patterns</li> <li>Abstent disorders now set the negation to True when matched as <code>ABSENT</code></li> <li>Default qualifier is now <code>None</code> instead of <code>False</code> (empty string)</li> </ul>"},{"location":"changelog/#fixed_22","title":"Fixed","text":"<ul> <li><code>span_getter</code> is not incompatible with on_ents_only anymore</li> <li><code>ContextualMatcher</code> now supports empty matches (e.g. lookahead/lookbehind) in <code>assign</code> patterns</li> </ul>"},{"location":"changelog/#v090-2023-09-15","title":"v0.9.0 (2023-09-15)","text":""},{"location":"changelog/#added_19","title":"Added","text":"<ul> <li>New <code>to_duration</code> method to convert an absolute date into a date relative to the note_datetime (or None)</li> </ul>"},{"location":"changelog/#changes","title":"Changes","text":"<ul> <li>Input and output of components are now specified by <code>span_getter</code> and <code>span_setter</code> arguments.</li> <li> Score / disorders / behaviors entities now have a fixed label (passed as an argument), instead of being dynamically set from the component name. The following scores may have a different name   than the current one in your pipelines:<ul> <li><code>eds.emergency.gemsa</code> \u2192 <code>emergency_gemsa</code></li> <li><code>eds.emergency.ccmu</code> \u2192 <code>emergency_ccmu</code></li> <li><code>eds.emergency.priority</code> \u2192 <code>emergency_priority</code></li> <li><code>eds.charlson</code> \u2192 <code>charlson</code></li> <li><code>eds.elston_ellis</code> \u2192 <code>elston_ellis</code></li> <li><code>eds.SOFA</code> \u2192 <code>sofa</code></li> <li><code>eds.adicap</code> \u2192 <code>adicap</code></li> <li><code>eds.measuremets</code> \u2192 <code>size</code>, <code>weight</code>, ... instead of <code>eds.size</code>, <code>eds.weight</code>, ...</li> </ul> </li> <li><code>eds.dates</code> now separate dates from durations. Each entity has its own label:<ul> <li><code>spans[\"dates\"]</code> \u2192 entities labelled as <code>date</code> with a <code>span._.date</code> parsed object</li> <li><code>spans[\"durations\"]</code> \u2192 entities labelled as <code>duration</code> with a <code>span._.duration</code> parsed object</li> </ul> </li> <li>the \"relative\" / \"absolute\" / \"duration\" mode of the time entity is now stored in   the <code>mode</code> attribute of the <code>span._.date/duration</code></li> <li>the \"from\" / \"until\" period bound, if any, is now stored in the <code>span._.date.bound</code> attribute</li> <li><code>to_datetime</code> now only return absolute dates, converts relative dates into absolute if <code>doc._.note_datetime</code> is given, and None otherwise</li> </ul>"},{"location":"changelog/#fixed_23","title":"Fixed","text":"<ul> <li><code>export_to_brat</code> issue with spans of entities on multiple lines.</li> </ul>"},{"location":"changelog/#v081-2023-05-31","title":"v0.8.1 (2023-05-31)","text":"<p>Fix release to allow installation from source</p>"},{"location":"changelog/#v080-2023-05-24","title":"v0.8.0 (2023-05-24)","text":""},{"location":"changelog/#added_20","title":"Added","text":"<ul> <li>New trainable component for multi-label, multi-class span qualification (any attribute/extension)</li> <li>Add range measurements (like <code>la tumeur fait entre 1 et 2 cm</code>) to <code>eds.measurements</code> matcher</li> <li>Add <code>eds.CKD</code> component</li> <li>Add <code>eds.COPD</code> component</li> <li>Add <code>eds.alcohol</code> component</li> <li>Add <code>eds.cerebrovascular_accident</code> component</li> <li>Add <code>eds.congestive_heart_failure</code> component</li> <li>Add <code>eds.connective_tissue_disease</code> component</li> <li>Add <code>eds.dementia</code> component</li> <li>Add <code>eds.diabetes</code> component</li> <li>Add <code>eds.hemiplegia</code> component</li> <li>Add <code>eds.leukemia</code> component</li> <li>Add <code>eds.liver_disease</code> component</li> <li>Add <code>eds.lymphoma</code> component</li> <li>Add <code>eds.myocardial_infarction</code> component</li> <li>Add <code>eds.peptic_ulcer_disease</code> component</li> <li>Add <code>eds.peripheral_vascular_disease</code> component</li> <li>Add <code>eds.solid_tumor</code> component</li> <li>Add <code>eds.tobacco</code> component</li> <li>Add <code>eds.spaces</code> (or <code>eds.normalizer</code> with <code>spaces=True</code>) to detect space tokens, and add <code>ignore_space_tokens</code> to <code>EDSPhraseMatcher</code> and <code>SimstringMatcher</code> to skip them</li> <li>Add <code>ignore_space_tokens</code> option in most components</li> <li><code>eds.tables</code>: new pipeline to identify formatted tables</li> <li>New <code>merge_mode</code> parameter in <code>eds.measurements</code> to normalize existing entities or detect   measures only inside existing entities</li> <li>Tokenization exceptions (<code>Mr.</code>, <code>Dr.</code>, <code>Mrs.</code>) and non end-of-sentence periods are now tokenized with the next letter in the <code>eds</code> tokenizer</li> </ul>"},{"location":"changelog/#changed_20","title":"Changed","text":"<ul> <li>Disable <code>EDSMatcher</code> preprocessing auto progress tracking by default</li> <li>Moved dependencies to a single pyproject.toml: support for <code>pip install -e '.[dev,docs,setup]'</code></li> <li>ADICAP matcher now allow dot separators (e.g. <code>B.H.HP.A7A0</code>)</li> </ul>"},{"location":"changelog/#fixed_24","title":"Fixed","text":"<ul> <li>Abbreviation and number tokenization issues in the <code>eds</code> tokenizer</li> <li><code>eds.adicap</code> : reparsed the dictionnary used to decode the ADICAP codes (some of them were wrongly decoded)</li> <li>Fix build for python 3.9 on Mac M1/M2 machines.</li> </ul>"},{"location":"changelog/#v074-2022-12-12","title":"v0.7.4 (2022-12-12)","text":""},{"location":"changelog/#added_21","title":"Added","text":"<ul> <li><code>eds.history</code> : Add the option to consider only the closest dates in the sentence (dates inside the boundaries and if there is not, it takes the closest date in the entire sentence).</li> <li><code>eds.negation</code> : It takes into account following past participates and preceding infinitives.</li> <li><code>eds.hypothesis</code>: It takes into account following past participates hypothesis verbs.</li> <li><code>eds.negation</code> &amp; <code>eds.hypothesis</code> : Introduce new patterns and remove unnecessary patterns.</li> <li><code>eds.dates</code> : Add a pattern for preceding relative dates (ex: l'embolie qui est survenue \u00e0 10 jours).</li> <li>Improve patterns in the <code>eds.pollution</code> component to account for multiline footers</li> <li>Add <code>QuickExample</code> object to quickly try a pipeline.</li> <li>Add UMLS terminology matcher <code>eds.umls</code></li> <li>New <code>RegexMatcher</code> method to create spans from groupdicts</li> <li>New <code>eds.dates</code> option to disable time detection</li> </ul>"},{"location":"changelog/#changed_21","title":"Changed","text":"<ul> <li>Improve date detection by removing false positives</li> </ul>"},{"location":"changelog/#fixed_25","title":"Fixed","text":"<ul> <li><code>eds.hypothesis</code> : Remove too generic patterns.</li> <li><code>EDSTokenizer</code> : It now tokenizes <code>\"rechereche d'\"</code> as <code>[\"recherche\", \"d'\"]</code>, instead of <code>[\"recherche\", \"d\", \"'\"]</code>.</li> <li>Fix small typos in the documentation and in the docstring.</li> <li>Harmonize processing utils (distributed custom_pipe) to have the same API for Pandas and Pyspark</li> <li>Fix BratConnector file loading issues with complex file hierarchies</li> </ul>"},{"location":"changelog/#v072-2022-10-26","title":"v0.7.2 (2022-10-26)","text":""},{"location":"changelog/#added_22","title":"Added","text":"<ul> <li>Improve the <code>eds.history</code> component by taking into account the date extracted from <code>eds.dates</code> component.</li> <li>New pop up when you click on the copy icon in the termynal widget (docs).</li> <li>Add NER <code>eds.elston-ellis</code> pipeline to identify Elston Ellis scores</li> <li>Add flags=re.MULTILINE to <code>eds.pollution</code> and change pattern of footer</li> </ul>"},{"location":"changelog/#fixed_26","title":"Fixed","text":"<ul> <li>Remove the warning in the <code>eds.sections</code> when <code>eds.normalizer</code> is in the pipe.</li> <li>Fix filter_spans for strictly nested entities</li> <li>Fill eds.remove-lowercase \"assign\" metadata to run the pipeline during EDSPhraseMatcher preprocessing</li> <li>Allow back spaCy components whose name contains a dot (forbidden since spaCy v3.4.2) for backward compatibility.</li> </ul>"},{"location":"changelog/#v071-2022-10-13","title":"v0.7.1 (2022-10-13)","text":""},{"location":"changelog/#added_23","title":"Added","text":"<ul> <li>Add new patterns (footer, web entities, biology tables, coding sections) to pipeline normalisation (pollution)</li> </ul>"},{"location":"changelog/#changed_22","title":"Changed","text":"<ul> <li>Improved TNM detection algorithm</li> <li>Account for more modifiers in ADICAP codes detection</li> </ul>"},{"location":"changelog/#fixed_27","title":"Fixed","text":"<ul> <li>Add nephew, niece and daughter to family qualifier patterns</li> <li>EDSTokenizer (<code>spacy.blank('eds')</code>) now recognizes non-breaking whitespaces as spaces and does not split float numbers</li> <li><code>eds.dates</code> pipeline now allows new lines as space separators in dates</li> </ul>"},{"location":"changelog/#v070-2022-09-06","title":"v0.7.0 (2022-09-06)","text":""},{"location":"changelog/#added_24","title":"Added","text":"<ul> <li>New nested NER trainable <code>nested_ner</code> pipeline component</li> <li>Support for nested entities and attributes in BratDataConnector</li> <li>Pytorch wrappers and experimental training utils</li> <li>Add attribute <code>section</code> to entities</li> <li>Add new cases for separator pattern when components of the TNM score are separated by a forward slash</li> <li>Add NER <code>eds.adicap</code> pipeline to identify ADICAP codes</li> <li>Add patterns to <code>pollution</code> pipeline and simplifies activating or deactivating specific patterns</li> </ul>"},{"location":"changelog/#changed_23","title":"Changed","text":"<ul> <li>Simplified the configuration scheme of the <code>pollution</code> pipeline</li> <li>Update of the <code>ContextualMatcher</code> (and all pipelines depending on it), rendering it more flexible to use</li> <li>Rename R component of score TNM as \"resection_completeness\"</li> </ul>"},{"location":"changelog/#fixed_28","title":"Fixed","text":"<ul> <li>Prevent section titles from capturing surrounding tokens, causing overlaps (#113)</li> <li>Enhance existing patterns for section detection and add patterns for previously ignored sections (introduction, evolution, modalites de sortie, vaccination) .</li> <li>Fix explain mode, which was always triggered, in <code>eds.history</code> factory.</li> <li>Fix test in <code>eds.sections</code>. Previously, no check was done</li> <li>Remove SOFA scores spurious span suffixes</li> </ul>"},{"location":"changelog/#v062-2022-08-02","title":"v0.6.2 (2022-08-02)","text":""},{"location":"changelog/#added_25","title":"Added","text":"<ul> <li>New <code>SimstringMatcher</code> matcher to perform fuzzy term matching, and <code>algorithm</code> parameter in terminology components and <code>eds.matcher</code> component</li> <li>Makefile to install,test the application and see the documentation</li> </ul>"},{"location":"changelog/#changed_24","title":"Changed","text":"<ul> <li>Add consultation date pattern \"CS\", and False Positive patterns for dates (namely phone numbers and pagination).</li> <li>Update the pipeline score <code>eds.TNM</code>. Now it is possible to return a dictionary where the results are either <code>str</code> or <code>int</code> values</li> </ul>"},{"location":"changelog/#fixed_29","title":"Fixed","text":"<ul> <li>Add new patterns to the negation qualifier</li> <li>Numpy header issues with binary distributed packages</li> <li>Simstring dependency on Windows</li> </ul>"},{"location":"changelog/#v061-2022-07-11","title":"v0.6.1 (2022-07-11)","text":""},{"location":"changelog/#added_26","title":"Added","text":"<ul> <li>Now possible to provide regex flags when using the RegexMatcher</li> <li>New <code>ContextualMatcher</code> pipe, aiming at replacing the <code>AdvancedRegex</code> pipe.</li> <li>New <code>as_ents</code> parameter for <code>eds.dates</code>, to save detected dates as entities</li> </ul>"},{"location":"changelog/#changed_25","title":"Changed","text":"<ul> <li>Faster <code>eds.sentences</code> pipeline component with Cython</li> <li>Bump version of Pydantic in <code>requirements.txt</code> to 1.8.2 to handle an incompatibility with the ContextualMatcher</li> <li>Optimise space requirements by using <code>.csv.gz</code> compression for verbs</li> </ul>"},{"location":"changelog/#fixed_30","title":"Fixed","text":"<ul> <li><code>eds.sentences</code> behaviour with dot-delimited dates (eg <code>02.07.2022</code>, which counted as three sentences)</li> </ul>"},{"location":"changelog/#v060-2022-06-17","title":"v0.6.0 (2022-06-17)","text":""},{"location":"changelog/#added_27","title":"Added","text":"<ul> <li>Complete revamp of the measurements detection pipeline, with better parsing and more exhaustive matching</li> <li>Add new functionality to the method <code>Span._.date.to_datetime()</code> to return a result infered from context for those cases with missing information.</li> <li>Force a batch size of 2000 when distributing a pipeline with Spark</li> <li>New patterns to pipeline <code>eds.dates</code> to identify cases where only the month is mentioned</li> <li>New <code>eds.terminology</code> component for generic terminology matching, using the <code>kb_id_</code> attribute to store fine-grained entity label</li> <li>New <code>eds.cim10</code> terminology matching pipeline</li> <li>New <code>eds.drugs</code> terminology pipeline that maps brand names and active ingredients to a unique ATC code</li> </ul>"},{"location":"changelog/#v053-2022-05-04","title":"v0.5.3 (2022-05-04)","text":""},{"location":"changelog/#added_28","title":"Added","text":"<ul> <li>Support for strings in the example utility</li> <li>TNM detection and normalisation with the <code>eds.TNM</code> pipeline</li> <li>Support for arbitrary callback for Pandas multiprocessing, with the <code>callback</code> argument</li> </ul>"},{"location":"changelog/#v052-2022-05-04","title":"v0.5.2 (2022-05-04)","text":""},{"location":"changelog/#added_29","title":"Added","text":"<ul> <li>Support for chained attributes in the <code>processing</code> pipelines</li> <li>Colour utility with the category20 colour palette</li> </ul>"},{"location":"changelog/#fixed_31","title":"Fixed","text":"<ul> <li>Correct a REGEX on the date detector (both <code>nov</code> and <code>nov.</code> are now detected, as all other months)</li> </ul>"},{"location":"changelog/#v051-2022-04-11","title":"v0.5.1 (2022-04-11)","text":""},{"location":"changelog/#fixed_32","title":"Fixed","text":"<ul> <li>Updated Numpy requirements to be compatible with the <code>EDSPhraseMatcher</code></li> </ul>"},{"location":"changelog/#v050-2022-04-08","title":"v0.5.0 (2022-04-08)","text":""},{"location":"changelog/#added_30","title":"Added","text":"<ul> <li>New <code>eds</code> language to better fit French clinical documents and improve speed</li> <li>Testing for markdown codeblocks to make sure the documentation is actually executable</li> </ul>"},{"location":"changelog/#changed_26","title":"Changed","text":"<ul> <li>Complete revamp of the date detection pipeline, with better parsing and more exhaustive matching</li> <li>Reimplementation of the EDSPhraseMatcher in Cython, leading to a x15 speed increase</li> </ul>"},{"location":"changelog/#v044-2022-03-31","title":"v0.4.4 (2022-03-31)","text":"<ul> <li>Add <code>measures</code> pipeline</li> <li>Cap Jinja2 version to fix mkdocs</li> <li>Adding the possibility to add context in the processing module</li> <li>Improve the speed of char replacement pipelines (accents and quotes)</li> <li>Improve the speed of the regex matcher</li> </ul>"},{"location":"changelog/#v043-2022-03-18","title":"v0.4.3 (2022-03-18)","text":"<ul> <li>Fix regex matching on spans.</li> <li>Add fast_parse in date pipeline.</li> <li>Add relative_date information parsing</li> </ul>"},{"location":"changelog/#v042-2022-03-16","title":"v0.4.2 (2022-03-16)","text":"<ul> <li>Fix issue with <code>dateparser</code> library (see scrapinghub/dateparser#1045)</li> <li>Fix <code>attr</code> issue in the <code>advanced-regex</code> pipelin</li> <li>Add documentation for <code>eds.covid</code></li> <li>Update the demo with an explanation for the regex</li> </ul>"},{"location":"changelog/#v041-2022-03-14","title":"v0.4.1 (2022-03-14)","text":"<ul> <li>Added support to Koalas DataFrames in the <code>edsnlp.processing</code> pipe.</li> <li>Added <code>eds.covid</code> NER pipeline for detecting COVID19 mentions.</li> </ul>"},{"location":"changelog/#v040-2022-02-22","title":"v0.4.0 (2022-02-22)","text":"<ul> <li>Profound re-write of the normalisation :<ul> <li>The custom attribute <code>CUSTOM_NORM</code> is completely abandoned in favour of a more spacyfic alternative</li> <li>The <code>normalizer</code> pipeline modifies the <code>NORM</code> attribute in place</li> <li>Other pipelines can modify the <code>Token._.excluded</code> custom attribute</li> </ul> </li> <li>EDS regex and term matchers can ignore excluded tokens during matching, effectively adding a second dimension to normalisation (choice of the attribute and possibility to skip pollution tokens   regardless of the attribute)</li> <li>Matching can be performed on custom attributes more easily</li> <li>Qualifiers are regrouped together within the <code>edsnlp.qualifiers</code> submodule, the inheritance from the <code>GenericMatcher</code> is dropped.</li> <li><code>edsnlp.utils.filter.filter_spans</code> now accepts a <code>label_to_remove</code> parameter. If set, only corresponding spans are removed, along with overlapping spans. Primary use-case: removing pseudo cues for   qualifiers.</li> <li>Generalise the naming convention for extensions, which keep the same name as the pipeline that created them (eg <code>Span._.negation</code> for the <code>eds.negation</code> pipeline). The previous convention is kept   for now, but calling it issues a warning.</li> <li>The <code>dates</code> pipeline underwent some light formatting to increase robustness and fix a few issues</li> <li>A new <code>consultation_dates</code> pipeline was added, which looks for dates preceded by expressions specific to consultation dates</li> <li>In rule-based processing, the <code>terms.py</code> submodule is replaced by <code>patterns.py</code> to reflect the possible presence of regular expressions</li> <li>Refactoring of the architecture :<ul> <li>pipelines are now regrouped by type (<code>core</code>, <code>ner</code>, <code>misc</code>, <code>qualifiers</code>)</li> <li><code>matchers</code> submodule contains <code>RegexMatcher</code> and <code>PhraseMatcher</code> classes, which interact with the normalisation</li> <li><code>multiprocessing</code> submodule contains <code>spark</code> and <code>local</code> multiprocessing tools</li> <li><code>connectors</code> contains <code>Brat</code>, <code>OMOP</code> and <code>LabelTool</code> connectors</li> <li><code>utils</code> contains various utilities</li> </ul> </li> <li>Add entry points to make pipeline usable directly, removing the need to import <code>edsnlp.components</code>.</li> <li>Add a <code>eds</code> namespace for components: for instance, <code>negation</code> becomes <code>eds.negation</code>. Using the former pipeline name still works, but issues a deprecation warning.</li> <li>Add 3 score pipelines related to emergency</li> <li>Add a helper function to use a spaCy pipeline as a Spark UDF.</li> <li>Fix alignment issues in RegexMatcher</li> <li>Change the alignment procedure, dropping clumsy <code>numpy</code> dependency in favour of <code>bisect</code></li> <li>Change the name of <code>eds.antecedents</code> to <code>eds.history</code>.   Calling <code>eds.antecedents</code> still works, but issues a deprecation warning and support will be removed in a future version.</li> <li>Add a <code>eds.covid</code> component, that identifies mentions of COVID</li> <li>Change the demo, to include NER components</li> </ul>"},{"location":"changelog/#v032-2021-11-24","title":"v0.3.2 (2021-11-24)","text":"<ul> <li>Major revamp of the normalisation.<ul> <li>The <code>normalizer</code> pipeline now adds atomic components (<code>lowercase</code>, <code>accents</code>, <code>quotes</code>, <code>pollution</code> &amp; <code>endlines</code>) to the processing pipeline, and compiles the results into a   new <code>Doc._.normalized</code> extension. The latter is itself a spaCy <code>Doc</code> object, wherein tokens are normalised and pollution tokens are removed altogether. Components that match on the <code>CUSTOM_NORM</code>   attribute process the <code>normalized</code> document, and matches are brought back to the original document using a token-wise mapping.</li> <li>Update the <code>RegexMatcher</code> to use the <code>CUSTOM_NORM</code> attribute</li> <li>Add an <code>EDSPhraseMatcher</code>, wrapping spaCy's <code>PhraseMatcher</code> to enable matching on <code>CUSTOM_NORM</code>.</li> <li>Update the <code>matcher</code> and <code>advanced</code> pipelines to enable matching on the <code>CUSTOM_NORM</code> attribute.</li> </ul> </li> <li>Add an OMOP connector, to help go back and forth between OMOP-formatted pandas dataframes and spaCy documents.</li> <li>Add a <code>reason</code> pipeline, that extracts the reason for visit.</li> <li>Add an <code>endlines</code> pipeline, that classifies newline characters between spaces and actual ends of line.</li> <li>Add possibility to annotate within entities for qualifiers (<code>negation</code>, <code>hypothesis</code>, etc), ie if the cue is within the entity. Disabled by default.</li> </ul>"},{"location":"changelog/#v031-2021-10-13","title":"v0.3.1 (2021-10-13)","text":"<ul> <li>Update <code>dates</code> to remove miscellaneous bugs.</li> <li>Add <code>isort</code> pre-commit hook.</li> <li>Improve performance for <code>negation</code>, <code>hypothesis</code>, <code>antecedents</code>, <code>family</code> and <code>rspeech</code> by using spaCy's <code>filter_spans</code> and our <code>consume_spans</code> methods.</li> <li>Add proposition segmentation to <code>hypothesis</code> and <code>family</code>, enhancing results.</li> </ul>"},{"location":"changelog/#v030-2021-09-29","title":"v0.3.0 (2021-09-29)","text":"<ul> <li>Renamed <code>generic</code> to <code>matcher</code>. This is a non-breaking change for the average user, adding the pipeline is still :</li> </ul> <pre><code>nlp.add_pipe(\"matcher\", config=dict(terms=dict(maladie=\"maladie\")))\n</code></pre> <ul> <li>Removed <code>quickumls</code> pipeline. It was untested, unmaintained. Will be added back in a future release.</li> <li>Add <code>score</code> pipeline, and <code>charlson</code>.</li> <li>Add <code>advanced-regex</code> pipeline</li> <li>Corrected bugs in the <code>negation</code> pipeline</li> </ul>"},{"location":"changelog/#v020-2021-09-13","title":"v0.2.0 (2021-09-13)","text":"<ul> <li>Add <code>negation</code> pipeline</li> <li>Add <code>family</code> pipeline</li> <li>Add <code>hypothesis</code> pipeline</li> <li>Add <code>antecedents</code> pipeline</li> <li>Add <code>rspeech</code> pipeline</li> <li>Refactor the library :<ul> <li>Remove the <code>rules</code> folder</li> <li>Add a <code>pipelines</code> folder, containing one subdirectory per component</li> <li>Every component subdirectory contains a module defining the component, and a module defining a factory, plus any other utilities (eg <code>terms.py</code>)</li> </ul> </li> </ul>"},{"location":"changelog/#v010-2021-09-29","title":"v0.1.0 (2021-09-29)","text":"<p>First working version. Available pipelines :</p> <ul> <li><code>section</code></li> <li><code>sentences</code></li> <li><code>normalization</code></li> <li><code>pollution</code></li> </ul>"}]}