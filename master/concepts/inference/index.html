<!DOCTYPE html>
<html class="no-js" lang="en"> <head><meta charset="utf-8"/><meta content="width=device-width,initial-scale=1" name="viewport"/><link href="../torch-component/" rel="prev"/><link href="../../metrics/" rel="next"/><link href="../../assets/logo/edsnlp.svg" rel="icon"/><meta content="mkdocs-1.6.1, mkdocs-material-9.2.8" name="generator"/><title>Inference - EDS-NLP</title><link href="../../assets/stylesheets/main.046329b4.min.css" rel="stylesheet"/><link href="../../assets/stylesheets/palette.85d0ee34.min.css" rel="stylesheet"/><link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/><link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link href="../../assets/_mkdocstrings.css" rel="stylesheet"/><link href="../../override.css" rel="stylesheet"/><link href="../../assets/stylesheets/extra.css" rel="stylesheet"/><link href="../../clickable-code.css" rel="stylesheet"/><link href="../../cards.css" rel="stylesheet"/><link href="../../pret.css" rel="stylesheet"/><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="default" dir="ltr"> <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script> <input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/> <input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/> <label class="md-overlay" for="__drawer"></label> <div data-md-component="skip"> <a class="md-skip" href="#inference"> Skip to content </a> </div> <div data-md-component="announce"> <aside class="md-banner"> <div class="md-banner__inner md-grid md-typeset"> <button aria-label="Don't show this again" class="md-banner__button md-icon"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg> </button> Check out the new <a href="../../tutorials/training-span-classifier">span classifier training tutorial</a> ! </div> <script>var content,el=document.querySelector("[data-md-component=announce]");el&&(content=el.querySelector(".md-typeset"),__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0))</script> </aside> </div> <div data-md-color-scheme="default" data-md-component="outdated" hidden=""> </div> <header class="md-header md-header--shadow" data-md-component="header"> <nav aria-label="Header" class="md-header__inner md-grid"> <a aria-label="EDS-NLP" class="md-header__button md-logo" data-md-component="logo" href="../.." title="EDS-NLP"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg> </a> <label class="md-header__button md-icon" for="__drawer"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg> </label> <div class="md-header__title" data-md-component="header-title"> <div class="md-header__ellipsis"> <div class="md-header__topic"> <span class="md-ellipsis"> EDS-NLP </span> </div> <div class="md-header__topic" data-md-component="header-topic"> <span class="md-ellipsis"> Inference </span> </div> </div> </div> <form class="md-header__option" data-md-component="palette"> <input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="" data-md-color-primary="indigo" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/> <label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to dark mode"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg> </label> <input aria-label="Switch to light mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="" data-md-color-primary="indigo" data-md-color-scheme="slate" id="__palette_2" name="__palette" type="radio"/> <label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg> </label> </form> <label class="md-header__button md-icon" for="__search"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg> </label> <div class="md-search" data-md-component="search" role="dialog"> <label class="md-search__overlay" for="__search"></label> <div class="md-search__inner" role="search"> <form class="md-search__form" name="search"> <input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/> <label class="md-search__icon md-icon" for="__search"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg> </label> <nav aria-label="Search" class="md-search__options"> <button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" title="Clear" type="reset"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg> </button> </nav> </form> <div class="md-search__output"> <div class="md-search__scrollwrap" data-md-scrollfix=""> <div class="md-search-result" data-md-component="search-result"> <div class="md-search-result__meta"> Initializing search </div> <ol class="md-search-result__list" role="presentation"></ol> </div> </div> </div> </div> </div> <div class="md-header__source"> <a class="md-source" data-md-component="source" href="https://github.com/aphp/edsnlp" title="Go to repository"> <div class="md-source__icon md-icon"> <svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!-- Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg> </div> <div class="md-source__repository"> aphp/edsnlp </div> </a> </div> </nav> </header> <div class="md-container" data-md-component="container"> <main class="md-main" data-md-component="main"> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation"> <div class="md-sidebar__scrollwrap"> <div class="md-sidebar__inner"> <nav aria-label="Navigation" class="md-nav md-nav--primary" data-md-level="0"> <label class="md-nav__title" for="__drawer"> <a aria-label="EDS-NLP" class="md-nav__button md-logo" data-md-component="logo" href="../.." title="EDS-NLP"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg> </a> EDS-NLP </label> <div class="md-nav__source"> <a class="md-source" data-md-component="source" href="https://github.com/aphp/edsnlp" title="Go to repository"> <div class="md-source__icon md-icon"> <svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!-- Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg> </div> <div class="md-source__repository"> aphp/edsnlp </div> </a> </div> <ul class="md-nav__list" data-md-scrollfix=""> <li class="md-nav__item"> <a class="md-nav__link" href="../.."> <span class="md-ellipsis"> Getting started </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="https://aphp.github.io/edsnlp/demo" target="_blank"> <span class="md-ellipsis"> Demo </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a class="md-nav__link" href="../../tutorials/"> <span class="md-ellipsis"> Tutorials </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a class="md-nav__link" href="../../pipes/"> <span class="md-ellipsis"> Pipes </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../../tokenizers/"> <span class="md-ellipsis"> Tokenizers </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a class="md-nav__link" href="../../data/"> <span class="md-ellipsis"> Data Connectors </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a class="md-nav__link" href="../../training/training-api/"> <span class="md-ellipsis"> Training </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input checked="" class="md-nav__toggle md-toggle" id="__nav_8" type="checkbox"/> <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0"> <span class="md-ellipsis"> Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded="true" aria-labelledby="__nav_8_label" class="md-nav" data-md-level="1"> <label class="md-nav__title" for="__nav_8"> <span class="md-nav__icon md-icon"></span> Concepts </label> <ul class="md-nav__list" data-md-scrollfix=""> <li class="md-nav__item"> <a class="md-nav__link" href="../pipeline/"> <span class="md-ellipsis"> Pipeline </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../torch-component/"> <span class="md-ellipsis"> Torch Component </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/> <label class="md-nav__link md-nav__link--active" for="__toc"> <span class="md-ellipsis"> Inference </span> <span class="md-nav__icon md-icon"></span> </label> <a class="md-nav__link md-nav__link--active" href="./"> <span class="md-ellipsis"> Inference </span> </a> <nav aria-label="Table of contents" class="md-nav md-nav--secondary"> <label class="md-nav__title" for="__toc"> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix=""> <li class="md-nav__item"> <a class="md-nav__link" href="#inference-on-a-single-document"> Inference on a single document </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.core.stream.Stream"> Streams </a> <nav aria-label="Streams" class="md-nav"> <ul class="md-nav__list"> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.core.stream.Stream.map"> map() </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.core.stream.Stream.map_batches"> map_batches() </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.core.stream.Stream.map_pipeline"> map_pipeline() </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.core.stream.Stream.map_gpu"> map_gpu() </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.core.stream.Stream.loop"> loop() </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.core.stream.Stream.shuffle"> shuffle() </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.core.stream.Stream.set_processing"> Configure the execution with set_processing() </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#backends"> Backends </a> <nav aria-label="Backends" class="md-nav"> <ul class="md-nav__list"> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.processing.simple.execute_simple_backend"> simple </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.processing.multiprocessing.execute_multiprocessing_backend"> multiprocessing </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.processing.spark.execute_spark_backend"> spark </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#batching"> Batching </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a class="md-nav__link" href="../../metrics/"> <span class="md-ellipsis"> Metrics </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a class="md-nav__link" href="../../utilities/"> <span class="md-ellipsis"> Utilities </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a class="md-nav__link" href="../../reference/edsnlp/"> <span class="md-ellipsis"> Code Reference </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../../contributing/"> <span class="md-ellipsis"> Contributing to EDS-NLP </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../../changelog/"> <span class="md-ellipsis"> Changelog </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc"> <div class="md-sidebar__scrollwrap"> <div class="md-sidebar__inner"> <nav aria-label="Table of contents" class="md-nav md-nav--secondary"> <label class="md-nav__title" for="__toc"> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix=""> <li class="md-nav__item"> <a class="md-nav__link" href="#inference-on-a-single-document"> Inference on a single document </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.core.stream.Stream"> Streams </a> <nav aria-label="Streams" class="md-nav"> <ul class="md-nav__list"> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.core.stream.Stream.map"> map() </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.core.stream.Stream.map_batches"> map_batches() </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.core.stream.Stream.map_pipeline"> map_pipeline() </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.core.stream.Stream.map_gpu"> map_gpu() </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.core.stream.Stream.loop"> loop() </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.core.stream.Stream.shuffle"> shuffle() </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.core.stream.Stream.set_processing"> Configure the execution with set_processing() </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#backends"> Backends </a> <nav aria-label="Backends" class="md-nav"> <ul class="md-nav__list"> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.processing.simple.execute_simple_backend"> simple </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.processing.multiprocessing.execute_multiprocessing_backend"> multiprocessing </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#edsnlp.processing.spark.execute_spark_backend"> spark </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#batching"> Batching </a> </li> </ul> </nav> </div> </div> </div> <div class="md-content" data-md-component="content"> <article class="md-content__inner md-typeset"> <h1 id="inference">Inference</h1> <p>Once you have obtained a pipeline, either by composing rule-based components, training a model or loading a model from the disk, you can use it to make predictions on documents. This is referred to as inference. This page answers the following questions :</p> <blockquote> <p>How do we leverage computational resources run a model on many documents?</p> <p>How do we connect to various data sources to retrieve documents?</p> </blockquote> <p>Be sure to check out the <a href="../../tutorials/multiple-texts">Processing multiple texts</a> tutorial for a practical example of how to use EDS-NLP to process large datasets.</p> <h2 id="inference-on-a-single-document">Inference on a single document</h2> <p>In EDS-NLP, computing the prediction on a single document is done by calling the pipeline on the document. The input can be either:</p> <ul> <li>a text string</li> <li>or a <a href="https://spacy.io/api/doc">Doc</a> object</li> </ul> <div class="no-check highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">"... my text ..."</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div> <p>If you're lucky enough to have a GPU, you can use it to speed up inference by moving the model to the GPU before calling the pipeline.</p> <div class="no-check highlight"><pre><span></span><code><span class="n">nlp</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>  <span class="c1"># same semantics as pytorch</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div> <p>To leverage multiple GPUs when processing multiple documents, refer to the <a class="autorefs autorefs-internal" href="#edsnlp.processing.multiprocessing.execute_multiprocessing_backend">multiprocessing backend</a> description below.</p> <h2 class="sourced-heading" id="edsnlp.core.stream.Stream">Streams<span class="sourced-heading-spacer"></span><a href="https///github.com/aphp/edsnlp/blob/65669dc92/edsnlp/core/stream.py#L265" target="_blank">[source]</a></h2> <p>When processing multiple documents, we can optimize the inference by parallelizing the computation on a single core, multiple cores and GPUs or even multiple machines.</p> <p>These optimizations are enabled by performing <em>lazy inference</em> : the operations (e.g., reading a document, converting it to a Doc, running the different pipes of a model or writing the result somewhere) are not executed immediately but are instead scheduled in a <a class="autorefs autorefs-internal" href="#edsnlp.core.stream.Stream" title="Streams">Stream</a> object. It can then be executed by calling the <code>execute</code> method, iterating over it or calling a writing method (e.g., <code>to_pandas</code>). In fact, data connectors like <code>edsnlp.data.read_json</code> return a stream, as well as the <code>nlp.pipe</code> method.</p> <p>A stream contains :</p> <ul> <li>a <code>reader</code>: the source of the data (e.g., a file, a database, a list of strings, etc.)</li> <li>the list of operations to perform (<code>stream.ops</code>) that contain the function / pipe, keyword arguments and context for each operation</li> <li>an optional <code>writer</code>: the destination of the data (e.g., a file, a database, a list of strings, etc.)</li> <li>the execution <code>config</code>, containing the backend to use and its configuration such as the number of workers, the batch size, etc.</li> </ul> <p>All methods (<code>map()</code>, <code>map_batches()</code>, <code>map_gpu()</code>, <code>map_pipeline()</code>, <code>set_processing()</code>) of the stream are chainable, meaning that they return a new stream object (no in-place modification).</p> <p>For instance, the following code will load a model, read a folder of JSON files, apply the model to each document and write the result in a Parquet folder, using 4 CPUs and 2 GPUs.</p> <div class="no-check highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">edsnlp</span>

<span class="c1"># Load or create a model</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">edsnlp</span><span class="o">.</span><span class="n"><html><head></head><body><a class="clickable-discrete-link" href="../../reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.load">load</a></body></html></span><span class="p">(</span><span class="s2">"path/to/model"</span><span class="p">)</span>

<span class="c1"># Read some data (this is lazy, no data will be read until the end of of this snippet)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">edsnlp</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n"><html><head></head><body><a class="clickable-discrete-link" href="../../data/json/#edsnlp.data.json.read_json">read_json</a></body></html></span><span class="p">(</span><span class="s2">"path/to/json_folder"</span><span class="p">,</span> <span class="n">converter</span><span class="o">=</span><span class="s2">"..."</span><span class="p">)</span>

<span class="c1"># Apply each pipe of the model to our documents and split the data</span>
<span class="c1"># into batches such that each contains at most 100 000 padded words</span>
<span class="c1"># (padded_words = max doc size in batch * batch size)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map_pipeline</span><span class="p">(</span>
    <span class="n">nlp</span><span class="p">,</span>
    <span class="c1"># optional arguments</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">100_000</span><span class="p">,</span>
    <span class="n">batch_by</span><span class="o">=</span><span class="s2">"padded_words"</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># or equivalently : data = nlp.pipe(data, batch_size=100_000, batch_by="padded_words")</span>

<span class="c1"># Sort the documents in chunks of 1024 documents</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">batch</span><span class="p">:</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">doc</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span>
    <span class="c1"># Apply a function to each batch of documents</span>
    <span class="k">lambda</span> <span class="n">batch</span><span class="p">:</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">my_custom_attribute</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Configure the execution</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">set_processing</span><span class="p">(</span>
    <span class="c1"># 4 CPUs to parallelize rule-based pipes, IO and preprocessing</span>
    <span class="n">num_cpu_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="c1"># 2 GPUs to accelerate deep-learning pipes</span>
    <span class="n">num_gpu_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="c1"># Show the progress bar</span>
    <span class="n">show_progress</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Write the result, this will execute the stream</span>
<span class="n">data</span><span class="o">.</span><span class="n">write_parquet</span><span class="p">(</span><span class="s2">"path/to/output_folder"</span><span class="p">,</span> <span class="n">converter</span><span class="o">=</span><span class="s2">"..."</span><span class="p">,</span> <span class="n">write_in_worker</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div> <p>Streams support a variety of operations, such as applying a function to each element of the stream, batching the elements, applying a model to the elements, etc. In each case, the operations will not be executed immediately but will be scheduled to be executed when iterating of the collection, or calling the <code>execute()</code>, <code>to_*()</code> or <code>write_*()</code> methods.</p> <h3 class="sourced-heading" id="edsnlp.core.stream.Stream.map"><code>map()</code><span class="sourced-heading-spacer"></span><a href="https///github.com/aphp/edsnlp/blob/65669dc92/edsnlp/core/stream.py#L476" target="_blank">[source]</a></h3> <div class="doc doc-object doc-function"> <div class="doc doc-contents first"> <p>Maps a callable to the documents. It takes a callable as input and an optional dictionary of keyword arguments. The function will be applied to each element of the collection. If the callable is a generator function, each element will be yielded to the stream as is.</p> <table> <thead> <tr> <th><b>PARAMETER</b></th> <th><b>DESCRIPTION</b></th> </tr> </thead> <tbody> <tr> <td><code>pipe</code></td> <td class="doc-param-details"> <p>The callable to map to the documents.</p> <p> </p> </td> </tr> <tr> <td><code>kwargs</code></td> <td class="doc-param-details"> <p>The keyword arguments to pass to the callable.</p> <p> <span class="doc-param-default"> <b>DEFAULT:</b> <code>{}</code> </span> </p> </td> </tr> </tbody> </table> </div> </div><h3 class="sourced-heading" id="edsnlp.core.stream.Stream.map_batches"><code>map_batches()</code><span class="sourced-heading-spacer"></span><a href="https///github.com/aphp/edsnlp/blob/65669dc92/edsnlp/core/stream.py#L516" target="_blank">[source]</a></h3> <p>To apply an operation to a stream in batches, you can use the <code>map_batches()</code> method. It takes a callable as input, an optional dictionary of keyword arguments and batching arguments.</p> <div class="doc doc-object doc-function"> <div class="doc doc-contents first"> <p>Maps a callable to a batch of documents. The callable should take a list of inputs. The output of the callable will be flattened if it is a list or a generator, or yielded to the stream as is if it is a single output (tuple or any other type).</p> <table> <thead> <tr> <th><b>PARAMETER</b></th> <th><b>DESCRIPTION</b></th> </tr> </thead> <tbody> <tr> <td><code>pipe</code></td> <td class="doc-param-details"> <p>The callable to map to the documents.</p> <p> </p> </td> </tr> <tr> <td><code>kwargs</code></td> <td class="doc-param-details"> <p>The keyword arguments to pass to the callable.</p> <p> <span class="doc-param-default"> <b>DEFAULT:</b> <code>{}</code> </span> </p> </td> </tr> <tr> <td><code>batch_size</code></td> <td class="doc-param-details"> <p>The batch size. Can also be a batching expression like "32 docs", "1024 words", "dataset", "fragment", etc.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="float">float</span>, <span title="str">str</span>]]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>batch_by</code></td> <td class="doc-param-details"> <p>Function to compute the batches. If set, it should take an iterable of documents and return an iterable of batches. You can also set it to "docs", "words" or "padded_words" to use predefined batching functions. Defaults to "docs".</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="edsnlp.utils.batching.BatchBy">BatchBy</span></code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> </tbody> </table> </div> </div><h3 class="sourced-heading" id="edsnlp.core.stream.Stream.map_pipeline"><code>map_pipeline()</code><span class="sourced-heading-spacer"></span><a href="https///github.com/aphp/edsnlp/blob/65669dc92/edsnlp/core/stream.py#L662" target="_blank">[source]</a></h3> <div class="doc doc-object doc-function"> <div class="doc doc-contents first"> <p>Maps a pipeline to the documents, i.e. adds each component of the pipeline to the stream operations. This function is called under the hood by <code>nlp.pipe()</code></p> <table> <thead> <tr> <th><b>PARAMETER</b></th> <th><b>DESCRIPTION</b></th> </tr> </thead> <tbody> <tr> <td><code>model</code></td> <td class="doc-param-details"> <p>The pipeline to map to the documents.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><a class="autorefs autorefs-internal" href="../../reference/edsnlp/core/pipeline/#edsnlp.core.pipeline.Pipeline" title="Pipeline (edsnlp.Pipeline)">Pipeline</a></code> </span> </p> </td> </tr> <tr> <td><code>batch_size</code></td> <td class="doc-param-details"> <p>The batch size. Can also be a batching expression like "32 docs", "1024 words", "dataset", "fragment", etc.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="float">float</span>, <span title="str">str</span>]]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>batch_by</code></td> <td class="doc-param-details"> <p>Function to compute the batches. If set, it should take an iterable of documents and return an iterable of batches. You can also set it to "docs", "words" or "padded_words" to use predefined batching functions. Defaults to "docs".</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="edsnlp.utils.batching.BatchBy">BatchBy</span></code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> </tbody> </table> </div> </div><h3 class="sourced-heading" id="edsnlp.core.stream.Stream.map_gpu"><code>map_gpu()</code><span class="sourced-heading-spacer"></span><a href="https///github.com/aphp/edsnlp/blob/65669dc92/edsnlp/core/stream.py#L604" target="_blank">[source]</a></h3> <div class="doc doc-object doc-function"> <div class="doc doc-contents first"> <p>Maps a deep learning operation to a batch of documents, on a GPU worker.</p> <table> <thead> <tr> <th><b>PARAMETER</b></th> <th><b>DESCRIPTION</b></th> </tr> </thead> <tbody> <tr> <td><code>prepare_batch</code></td> <td class="doc-param-details"> <p>A callable that takes a list of documents and a device and returns a batch of tensors (or anything that can be passed to the <code>forward</code> callable). This will be called on a CPU-bound worker, and may be parallelized.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Callable">Callable</span>[[<span title="typing.List">List</span>, <span title="typing.Union">Union</span>[<span title="str">str</span>, <span title="torch.device">device</span>]], <span title="typing.Any">Any</span>]</code> </span> </p> </td> </tr> <tr> <td><code>forward</code></td> <td class="doc-param-details"> <p>A callable that takes the output of <code>prepare_batch</code> and returns the output of the deep learning operation. This will be called on a GPU-bound worker.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Callable">Callable</span>[[<span title="typing.Any">Any</span>], <span title="typing.Any">Any</span>]</code> </span> </p> </td> </tr> <tr> <td><code>postprocess</code></td> <td class="doc-param-details"> <p>An optional callable that takes the list of documents and the output of the deep learning operation, and returns the final output. This will be called on the same CPU-bound worker that called the <code>prepare_batch</code> function.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>[[<span title="typing.List">List</span>, <span title="typing.Any">Any</span>], <span title="typing.Any">Any</span>]]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>batch_size</code></td> <td class="doc-param-details"> <p>The batch size. Can also be a batching expression like "32 docs", "1024 words", "dataset", "fragment", etc.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="float">float</span>, <span title="str">str</span>]]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>batch_by</code></td> <td class="doc-param-details"> <p>Function to compute the batches. If set, it should take an iterable of documents and return an iterable of batches. You can also set it to "docs", "words" or "padded_words" to use predefined batching functions. Defaults to "docs".</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="edsnlp.utils.batching.BatchBy">BatchBy</span></code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> </tbody> </table> </div> </div><h3 class="sourced-heading" id="edsnlp.core.stream.Stream.loop"><code>loop()</code><span class="sourced-heading-spacer"></span><a href="https///github.com/aphp/edsnlp/blob/65669dc92/edsnlp/core/stream.py#L829" target="_blank">[source]</a></h3> <div class="doc doc-object doc-function"> <div class="doc doc-contents first"> <p>Loops over the stream indefinitely.</p> <p>Note that we cycle over items produced by the reader, not the items produced by the stream operations. This means that the stream operations will be applied to the same items multiple times, and may produce different results if they are non-deterministic. This also mean that calling this function will have the same effect regardless of the operations applied to the stream before calling it, ie:</p> <div class="highlight"><pre><span></span><code>stream.loop().map(...)
# is equivalent to
stream.map(...).loop()
</code></pre></div> </div> </div><h3 class="sourced-heading" id="edsnlp.core.stream.Stream.shuffle"><code>shuffle()</code><span class="sourced-heading-spacer"></span><a href="https///github.com/aphp/edsnlp/blob/65669dc92/edsnlp/core/stream.py#L742" target="_blank">[source]</a></h3> <div class="doc doc-object doc-function"> <div class="doc doc-contents first"> <p>Shuffles the stream by accumulating the documents into batches and shuffling the batches. We try to optimize and avoid the accumulation by shuffling items directly in the reader, but if some upstream operations are not elementwise or if the reader is not compatible with the batching mode, we have to accumulate the documents into batches and shuffle the batches.</p> <p>For instance, imagine a reading from list of 2 very large documents and applying an operation to split the documents into sentences. Shuffling only in the reader, then applying the split operation would not shuffle the sentences across documents and may lead to a lack of randomness when training a model. Think of this as having lumps after mixing your data. In our case, we detect that the split op is not elementwise and trigger the accumulation of sentences into batches after their generation before shuffling the batches.</p> <table> <thead> <tr> <th><b>PARAMETER</b></th> <th><b>DESCRIPTION</b></th> </tr> </thead> <tbody> <tr> <td><code>batch_size</code></td> <td class="doc-param-details"> <p>The batch size. Can also be a batching expression like "32 docs", "1024 words", "dataset", "fragment", etc.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="float">float</span>, <span title="str">str</span>]]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>batch_by</code></td> <td class="doc-param-details"> <p>Function to compute the batches. If set, it should take an iterable of documents and return an iterable of batches. You can also set it to "docs", "words" or "padded_words" to use predefined batching functions. Defaults to "docs".</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Optional">Optional</span>[<span title="str">str</span>, <span title="edsnlp.utils.batching.BatchFn">BatchFn</span>]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>seed</code></td> <td class="doc-param-details"> <p>The seed to use for shuffling.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Optional">Optional</span>[<span title="int">int</span>]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>shuffle_reader</code></td> <td class="doc-param-details"> <p>Whether to shuffle the reader. Defaults to True if the reader is compatible with the batch_by mode, False otherwise.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="bool">bool</span>, <span title="str">str</span>]]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> </tbody> </table> </div> </div><h3 class="sourced-heading" id="edsnlp.core.stream.Stream.set_processing">Configure the execution with <code>set_processing()</code><span class="sourced-heading-spacer"></span><a href="https///github.com/aphp/edsnlp/blob/65669dc92/edsnlp/core/stream.py#L349" target="_blank">[source]</a></h3> <p>You can configure how the operations performed in the stream are executed by calling its <code>set_processing(...)</code> method. The following options are available :</p> <div class="doc doc-object doc-function"> <div class="doc doc-contents first"> <table> <thead> <tr> <th><b>PARAMETER</b></th> <th><b>DESCRIPTION</b></th> </tr> </thead> <tbody> <tr> <td><code>batch_size</code></td> <td class="doc-param-details"> <p>The batch size. Can also be a batching expression like "32 docs", "1024 words", "dataset", "fragment", etc.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="int">int</span>, <span title="float">float</span>, <span title="str">str</span>]]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>batch_by</code></td> <td class="doc-param-details"> <p>Function to compute the batches. If set, it should take an iterable of documents and return an iterable of batches. You can also set it to "docs", "words" or "padded_words" to use predefined batching functions. Defaults to "docs".</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="edsnlp.utils.batching.BatchBy">BatchBy</span></code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>num_cpu_workers</code></td> <td class="doc-param-details"> <p>Number of CPU workers. A CPU worker handles the non deep-learning components and the preprocessing, collating and postprocessing of deep-learning components. If no GPU workers are used, the CPU workers also handle the forward call of the deep-learning components.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Optional">Optional</span>[<span title="int">int</span>]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>num_gpu_workers</code></td> <td class="doc-param-details"> <p>Number of GPU workers. A GPU worker handles the forward call of the deep-learning components. Only used with "multiprocessing" backend.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Optional">Optional</span>[<span title="int">int</span>]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>disable_implicit_parallelism</code></td> <td class="doc-param-details"> <p>Whether to disable OpenMP and Huggingface tokenizers implicit parallelism in multiprocessing mode. Defaults to True.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="bool">bool</span></code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>True</code> </span> </p> </td> </tr> <tr> <td><code>backend</code></td> <td class="doc-param-details"> <p>The backend to use for parallel processing. If not set, the backend is automatically selected based on the input data and the number of workers.</p> <ul> <li>"simple" is the default backend and is used when <code>num_cpu_workers</code> is 1 and <code>num_gpu_workers</code> is 0.</li> <li>"multiprocessing" is used when <code>num_cpu_workers</code> is greater than 1 or <code>num_gpu_workers</code> is greater than 0.</li> <li>"spark" is used when the input data is a Spark dataframe and the output writer is a Spark writer.</li> </ul> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Optional">Optional</span>[<span title="typing_extensions.Literal">Literal</span>['simple', 'multiprocessing', 'mp', 'spark']]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>autocast</code></td> <td class="doc-param-details"> <p>Whether to use <a href="https://pytorch.org/docs/stable/amp.html">automatic mixed precision (AMP)</a> for the forward pass of the deep-learning components. If True (by default), AMP will be used with the default settings. If False, AMP will not be used. If a dtype is provided, it will be passed to the <code>torch.autocast</code> context manager.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Union">Union</span>[<span title="bool">bool</span>, <span title="typing.Any">Any</span>]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>show_progress</code></td> <td class="doc-param-details"> <p>Whether to show progress bars (only applicable with "simple" and "multiprocessing" backends).</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="bool">bool</span></code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>False</code> </span> </p> </td> </tr> <tr> <td><code>gpu_pipe_names</code></td> <td class="doc-param-details"> <p>List of pipe names to accelerate on a GPUWorker, defaults to all pipes that inherit from TorchComponent. Only used with "multiprocessing" backend. Inferred from the pipeline if not set.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<span title="str">str</span>]]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>process_start_method</code></td> <td class="doc-param-details"> <p>Whether to use "fork" or "spawn" as the start method for the multiprocessing backend. The default is "fork" on Unix systems and "spawn" on Windows.</p> <ul> <li>"fork" is the default start method on Unix systems and is the fastest start method, but it is not available on Windows, can cause issues with CUDA and is not safe when using multiple threads.</li> <li>"spawn" is the default start method on Windows and is the safest start method, but it is not available on Unix systems and is slower than "fork".</li> </ul> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Optional">Optional</span>[<span title="typing_extensions.Literal">Literal</span>['fork', 'spawn']]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>gpu_worker_devices</code></td> <td class="doc-param-details"> <p>List of GPU devices to use for the GPU workers. Defaults to all available devices, one worker per device. Only used with "multiprocessing" backend.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<span title="str">str</span>]]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>cpu_worker_devices</code></td> <td class="doc-param-details"> <p>List of GPU devices to use for the CPU workers. Used for debugging purposes.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<span title="str">str</span>]]</code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>None</code> </span> </p> </td> </tr> <tr> <td><code>deterministic</code></td> <td class="doc-param-details"> <p>Whether to try and preserve the order of the documents in "multiprocessing" mode. If set to <code>False</code>, workers will process documents whenever they are available in a dynamic fashion, which may result in out-of-order but usually faster processing. If set to true, tasks will be distributed in a static, round-robin fashion to workers. Defaults to <code>True</code>.</p> <p> <span class="doc-param-annotation"> <b>TYPE:</b> <code><span title="bool">bool</span></code> </span> <span class="doc-param-default"> <b>DEFAULT:</b> <code>True</code> </span> </p> </td> </tr> </tbody> </table> </div> </div><h2 id="backends">Backends</h2> <p>The <code>backend</code> parameter of the <code>set_processing</code> supports the following values:</p> <h3 class="sourced-heading" id="edsnlp.processing.simple.execute_simple_backend"><code>simple</code><span class="sourced-heading-spacer"></span><a href="https///github.com/aphp/edsnlp/blob/65669dc92/edsnlp/processing/simple.py#L15" target="_blank">[source]</a></h3> <div class="doc doc-object doc-function"> <div class="doc doc-contents first"> <p>This is the default execution mode which batches the documents and processes each batch on the current process in a sequential manner.</p> </div> </div><h3 class="sourced-heading" id="edsnlp.processing.multiprocessing.execute_multiprocessing_backend"><code>multiprocessing</code><span class="sourced-heading-spacer"></span><a href="https///github.com/aphp/edsnlp/blob/65669dc92/edsnlp/processing/multiprocessing.py#L1382" target="_blank">[source]</a></h3> <div class="doc doc-object doc-function"> <div class="doc doc-contents first"> <p>If you have multiple CPU cores, and optionally multiple GPUs, we provide the <code>multiprocessing</code> backend that allows to run the inference on multiple processes.</p> <p>This accelerator dispatches the batches between multiple workers (data-parallelism), and distribute the computation of a given batch on one or two workers (model-parallelism):</p> <ul> <li>a <code>CPUWorker</code> which handles the non deep-learning components and the preprocessing, collating and postprocessing of deep-learning components</li> <li>a <code>GPUWorker</code> which handles the forward call of the deep-learning components</li> </ul> <p>If no GPU is available, no <code>GPUWorker</code> is started, and the <code>CPUWorkers</code> handle the forward call of the deep-learning components as well.</p> <p>The advantage of dedicating a worker to the deep-learning components is that it allows to prepare multiple batches in parallel in multiple <code>CPUWorker</code>, and ensure that the <code>GPUWorker</code> never wait for a batch to be ready.</p> <p>The overall architecture described in the following figure, for 3 CPU workers and 2 GPU workers.</p> <div style="text-align:center"> <img src="../../assets/images/multiprocessing.png" style="height:400px"/> </div> <p>Here is how a small pipeline with rule-based components and deep-learning components is distributed between the workers:</p> <div style="text-align:center"> <img src="../../assets/images/model-parallelism.png"/> </div> </div> </div><h3 class="sourced-heading" id="edsnlp.processing.spark.execute_spark_backend"><code>spark</code><span class="sourced-heading-spacer"></span><a href="https///github.com/aphp/edsnlp/blob/65669dc92/edsnlp/processing/spark.py#L32" target="_blank">[source]</a></h3> <div class="doc doc-object doc-function"> <div class="doc doc-contents first"> <p>This execution mode uses Spark to parallelize the processing of the documents. The documents are first stored in a Spark DataFrame (if it was not already the case) and then processed in parallel using Spark.</p> <p>Beware, if the original reader was not a SparkReader (<code>edsnlp.data.from_spark</code>), the <em>local docs</em> → <em>spark dataframe</em> conversion might take some time, and the whole process might be slower than using the <code>multiprocessing</code> backend.</p> </div> </div><h2 id="batching">Batching</h2> <p>Many operations rely on batching, either to be more efficient or because they require a fixed-size input. The <code>batch_size</code> and <code>batch_by</code> argument of the <code>map_batches()</code> method allows you to specify the size of the batches and what function to use to compute the size of the batches.</p> <div class="no-check highlight"><pre><span></span><code><span class="c1"># Accumulate in chunks of 1024 documents</span>
<span class="n">lengths</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span><span class="nb">len</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>

<span class="c1"># Accumulate in chunks of 100 000 words</span>
<span class="n">lengths</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span><span class="nb">len</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">batch_by</span><span class="o">=</span><span class="s2">"words"</span><span class="p">)</span>
<span class="c1"># or</span>
<span class="n">lengths</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span><span class="nb">len</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="s2">"100_000 words"</span><span class="p">)</span>
</code></pre></div> <p>We also support special values for <code>batch_size</code> which use "sentinels" (i.e. markers inserted in the stream) to delimit the batches.</p> <div class="no-check highlight"><pre><span></span><code><span class="c1"># Accumulate every element of the input in a single batch</span>
<span class="c1"># which is useful when looping over the data in training</span>
<span class="n">lengths</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span><span class="nb">len</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="s2">"dataset"</span><span class="p">)</span>

<span class="c1"># Accumulate in chunks of fragments, in the case of parquet datasets</span>
<span class="n">lengths</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map_batches</span><span class="p">(</span><span class="nb">len</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="s2">"fragments"</span><span class="p">)</span>
</code></pre></div> <p>Note that these batch functions are only available under specific conditions:</p> <ul> <li>either <code>backend="simple"</code> or <code>deterministic=True</code> (default) if <code>backend="multiprocessing"</code>, otherwise elements might be processed out of order</li> <li>if every op before was elementwise (e.g. <code>map()</code>, <code>map_gpu()</code>, <code>map_pipeline()</code> and no generator function), or <code>sentinel_mode</code> was explicitly set to <code>"split"</code> in <code>map_batches()</code>, otherwise the sentinel are dropped by default when the user requires batching.</li> </ul> <div class="footnote"><hr/><ol></ol></div> <script async="" crossorigin="anonymous" data-category="Announcements" data-category-id="DIC_kwDOG97JnM4CkS1h" data-emit-metadata="0" data-input-position="bottom" data-lang="en" data-mapping="title" data-reactions-enabled="1" data-repo="aphp/edsnlp" data-repo-id="R_kgDOG97JnA" data-strict="0" data-theme="https://aphp.github.io/edsnlp/master/assets/stylesheets/giscus_light.css" loading="lazy" src="https://giscus.app/client.js">
</script> <script>
    var giscus = document.querySelector("script[src*=giscus]")

    // Set palette on initial load
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
        var theme = palette.color.scheme === "slate"
            ? "https://aphp.github.io/edsnlp/master/assets/stylesheets/giscus_dark.css"
            : "https://aphp.github.io/edsnlp/master/assets/stylesheets/giscus_light.css"

        // Instruct Giscus to set theme
        giscus.setAttribute("data-theme", theme)
    }

    // Register event handlers after documented loaded
    document.addEventListener("DOMContentLoaded", function () {
        var ref = document.querySelector("[data-md-component=palette]")
        ref.addEventListener("change", function () {
            var palette = __md_get("__palette")
            if (palette && typeof palette.color === "object") {
                var theme = palette.color.scheme === "slate"
                    ? "https://aphp.github.io/edsnlp/master/assets/stylesheets/giscus_dark.css"
                    : "https://aphp.github.io/edsnlp/master/assets/stylesheets/giscus_light.css"

                // Instruct Giscus to change theme
                var frame = document.querySelector(".giscus-frame")
                frame.contentWindow.postMessage(
                    {giscus: {setConfig: {theme}}},
                    "https://giscus.app"
                )
            }
        })
    })
</script> </article> </div> </div> <button class="md-top md-icon" data-md-component="top" hidden="" type="button"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"></path></svg> Back to top </button> </main> <footer class="md-footer"> <nav aria-label="Footer" class="md-footer__inner md-grid"> <a aria-label="Previous: Torch Component" class="md-footer__link md-footer__link--prev" href="../torch-component/" rel="prev"> <div class="md-footer__button md-icon"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg> </div> <div class="md-footer__title"> <span class="md-footer__direction"> Previous </span> <div class="md-ellipsis"> Torch Component </div> </div> </a> <a aria-label="Next: Overview" class="md-footer__link md-footer__link--next" href="../../metrics/" rel="next"> <div class="md-footer__title"> <span class="md-footer__direction"> Next </span> <div class="md-ellipsis"> Overview </div> </div> <div class="md-footer__button md-icon"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"></path></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class="md-copyright"> <div class="md-copyright__highlight"> Copyright © 2025 – Assistance Publique - Hôpitaux de Paris </div> Made with <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank"> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class="md-dialog" data-md-component="dialog"> <div class="md-dialog__inner md-typeset"></div> </div> <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tracking", "navigation.instant", "navigation.indexes", "navigation.prune", "navigation.top", "navigation.footer", "content.code.annotate", "content.code.copy", "announce.dismiss"], "search": "../../assets/javascripts/workers/search.dfff1995.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "latest", "provider": "mike"}}</script> <script src="../../assets/javascripts/bundle.dff1b7c8.min.js"></script> <script pret-head-scripts=""></script> </body> </html>